{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac2d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, ccf\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))  # Adjust the path\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))  # Set working directory\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from utils import plot_outages_on_map_us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384578c",
   "metadata": {},
   "source": [
    "## 1.  Environment Setup\n",
    "\n",
    "‚úî Ensure all dependencies are installed using requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07f565d0",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '../dynamic_rhythms/requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r \"../dynamic_rhythms/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e1e58",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration\n",
    "\n",
    "### 2.1 Load Power Outage Data\n",
    "- Read .csv files in ../dynamic_rhythm_env/eaglei_data/\n",
    "- Inspect columns, data types, and missing values\n",
    "- Identify key variables like state, run_start_time, customers_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e106b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileURL = \"https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2\"\n",
    "# dest = \"repdata_data_StormData.csv.bz2\"\n",
    "# if(!file.exists(dest))\n",
    "#     download.file(fileURL,dest)\n",
    "# storm = read.csv(dest)\n",
    "\n",
    "\n",
    "# setwd(\".\")\n",
    "# download.file(\"https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2\", destfile = \"./repdata-data-StormData.csv.bz2\", method = \"curl\")\n",
    "# storm <- read.csv(\"repdata-data-StormData.csv.bz2\", header = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "418df76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest  \n",
    "# path = kagglehub.dataset_download(\"sobhanmoosavi/us-weather-events\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f966f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power Outage Files: ['DQI_processing.R', 'DQI.csv', 'eaglei_outages_2019.csv', 'eaglei_outages_2018.csv', 'eaglei_outages_2023.csv', 'MCC.csv', 'eaglei_outages_2022.csv', 'eaglei_outages_2020.csv', 'eaglei_outages_2021.csv', 'coverage_history.csv', 'eaglei_outages_2016.csv', 'eaglei_outages_2017.csv', 'eaglei_outages_2015.csv', 'eaglei_outages_2014.csv', 'Uri_Map.R']\n",
      "Storm Event Files: ['StormEvents_details-ftp_v1.0_d2014_c20231116.csv', 'StormEvents_details-ftp_v1.0_d2019_c20240117.csv', 'StormEvents_details-ftp_v1.0_d2021_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2018_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2024_c20241216.csv', 'StormEvents_details-ftp_v1.0_d2023_c20241216.csv', 'StormEvents_details-ftp_v1.0_d2016_c20220719.csv', 'StormEvents_details-ftp_v1.0_d2017_c20230317.csv', 'StormEvents_details-ftp_v1.0_d2015_c20240716.csv', 'StormEvents_details-ftp_v1.0_d2022_c20241121.csv', 'StormEvents_details-ftp_v1.0_d2020_c20240620.csv', 'StormEvents_2014_2024.csv']\n"
     ]
    }
   ],
   "source": [
    "# Define dataset directories\n",
    "outages_dir = \"../dynamic_rhythm_train_data/eaglei_data/\"\n",
    "storms_dir = \"../dynamic_rhythm_train_data/NOAA_StormEvents/\"\n",
    "\n",
    "# List files\n",
    "outage_files = os.listdir(outages_dir)\n",
    "storm_files = os.listdir(storms_dir)\n",
    "\n",
    "print(\"Power Outage Files:\", outage_files)\n",
    "print(\"Storm Event Files:\", storm_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1349d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Power Outage Files:\n",
      "   - DQI_processing.R (R)\n",
      "   - DQI.csv (csv)\n",
      "   - eaglei_outages_2019.csv (csv)\n",
      "   - eaglei_outages_2018.csv (csv)\n",
      "   - eaglei_outages_2023.csv (csv)\n",
      "   - MCC.csv (csv)\n",
      "   - eaglei_outages_2022.csv (csv)\n",
      "   - eaglei_outages_2020.csv (csv)\n",
      "   - eaglei_outages_2021.csv (csv)\n",
      "   - coverage_history.csv (csv)\n",
      "   - eaglei_outages_2016.csv (csv)\n",
      "   - eaglei_outages_2017.csv (csv)\n",
      "   - eaglei_outages_2015.csv (csv)\n",
      "   - eaglei_outages_2014.csv (csv)\n",
      "   - Uri_Map.R (R)\n",
      "\n",
      " Storm Event Files:\n",
      "   - StormEvents_details-ftp_v1.0_d2014_c20231116.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2019_c20240117.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2021_c20240716.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2018_c20240716.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2024_c20241216.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2023_c20241216.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2016_c20220719.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2017_c20230317.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2015_c20240716.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2022_c20241121.csv (csv)\n",
      "   - StormEvents_details-ftp_v1.0_d2020_c20240620.csv (csv)\n",
      "   - StormEvents_2014_2024.csv (csv)\n",
      "\n",
      " **Power Outage Data Preview**\n",
      "\n",
      "üîç Preview of DQI.csv:\n",
      "   fema  year  success_rate  percent_enabled  spatial_precision  \\\n",
      "0     1  2018     97.901622        99.314809          75.017989   \n",
      "1     1  2019     99.752504        99.523138          75.080810   \n",
      "2     1  2020     99.722473        68.974422          75.055220   \n",
      "3     1  2021     99.833900        99.524848          75.078565   \n",
      "4     1  2022     99.757155        99.921440          74.522534   \n",
      "\n",
      "   cust_coverage  max_covered  total_customers        DQI  \n",
      "0      92.281197      6703631          7264352  70.318387  \n",
      "1      93.218137      6795247          7289619  90.052990  \n",
      "2      95.654575      7114749          7437960  61.808936  \n",
      "3      98.010174      7092716          7236714  94.041252  \n",
      "4      98.148235      7224192          7360491  93.675949  \n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   fema               5 non-null      int64  \n",
      " 1   year               5 non-null      int64  \n",
      " 2   success_rate       5 non-null      float64\n",
      " 3   percent_enabled    5 non-null      float64\n",
      " 4   spatial_precision  5 non-null      float64\n",
      " 5   cust_coverage      5 non-null      float64\n",
      " 6   max_covered        5 non-null      int64  \n",
      " 7   total_customers    5 non-null      int64  \n",
      " 8   DQI                5 non-null      float64\n",
      "dtypes: float64(5), int64(4)\n",
      "memory usage: 488.0 bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fema                 0\n",
      "year                 0\n",
      "success_rate         0\n",
      "percent_enabled      0\n",
      "spatial_precision    0\n",
      "cust_coverage        0\n",
      "max_covered          0\n",
      "total_customers      0\n",
      "DQI                  0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2019.csv:\n",
      "   fips_code   county    state  customers_out       run_start_time\n",
      "0       1001  Autauga  Alabama             10  2019-01-01 00:00:00\n",
      "1       1003  Baldwin  Alabama             36  2019-01-01 00:00:00\n",
      "2       1007     Bibb  Alabama             93  2019-01-01 00:00:00\n",
      "3       1009   Blount  Alabama              4  2019-01-01 00:00:00\n",
      "4       1015  Calhoun  Alabama            457  2019-01-01 00:00:00\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   fips_code       5 non-null      int64 \n",
      " 1   county          5 non-null      object\n",
      " 2   state           5 non-null      object\n",
      " 3   customers_out   5 non-null      int64 \n",
      " 4   run_start_time  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     0\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2018.csv:\n",
      "   fips_code     county    state  customers_out       run_start_time\n",
      "0       1015    Calhoun  Alabama              1  2018-01-01 00:00:00\n",
      "1       1029   Cleburne  Alabama              5  2018-01-01 00:00:00\n",
      "2       1057    Fayette  Alabama             16  2018-01-01 00:00:00\n",
      "3       1073  Jefferson  Alabama              6  2018-01-01 00:00:00\n",
      "4       1097     Mobile  Alabama              2  2018-01-01 00:00:00\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   fips_code       5 non-null      int64 \n",
      " 1   county          5 non-null      object\n",
      " 2   state           5 non-null      object\n",
      " 3   customers_out   5 non-null      int64 \n",
      " 4   run_start_time  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     0\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2023.csv:\n",
      "   fips_code    county    state  customers_out       run_start_time\n",
      "0       1003   Baldwin  Alabama              1  2023-01-01 00:00:00\n",
      "1       1011   Bullock  Alabama              9  2023-01-01 00:00:00\n",
      "2       1015   Calhoun  Alabama              4  2023-01-01 00:00:00\n",
      "3       1021   Chilton  Alabama              4  2023-01-01 00:00:00\n",
      "4       1029  Cleburne  Alabama            142  2023-01-01 00:00:00\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   fips_code       5 non-null      int64 \n",
      " 1   county          5 non-null      object\n",
      " 2   state           5 non-null      object\n",
      " 3   customers_out   5 non-null      int64 \n",
      " 4   run_start_time  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     0\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of MCC.csv:\n",
      "   County_FIPS  Customers\n",
      "0         1001      24619\n",
      "1         1003     195253\n",
      "2         1005      12400\n",
      "3         1007      11037\n",
      "4         1009      27074\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype\n",
      "---  ------       --------------  -----\n",
      " 0   County_FIPS  5 non-null      int64\n",
      " 1   Customers    5 non-null      int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 208.0 bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "County_FIPS    0\n",
      "Customers      0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2022.csv:\n",
      "   fips_code   county    state  customers_out       run_start_time\n",
      "0       1005  Barbour  Alabama              4  2022-01-01 00:00:00\n",
      "1       1009   Blount  Alabama            160  2022-01-01 00:00:00\n",
      "2       1051   Elmore  Alabama              3  2022-01-01 00:00:00\n",
      "3       1055   Etowah  Alabama              4  2022-01-01 00:00:00\n",
      "4       1057  Fayette  Alabama              4  2022-01-01 00:00:00\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   fips_code       5 non-null      int64 \n",
      " 1   county          5 non-null      object\n",
      " 2   state           5 non-null      object\n",
      " 3   customers_out   5 non-null      int64 \n",
      " 4   run_start_time  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     0\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2020.csv:\n",
      "   fips_code     county    state  customers_out       run_start_time\n",
      "0       1003    Baldwin  Alabama            NaN  2020-01-01 00:00:00\n",
      "1       1047     Dallas  Alabama            4.0  2020-01-01 00:00:00\n",
      "2       1055     Etowah  Alabama            4.0  2020-01-01 00:00:00\n",
      "3       1073  Jefferson  Alabama            4.0  2020-01-01 00:00:00\n",
      "4       1097     Mobile  Alabama            4.0  2020-01-01 00:00:00\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   fips_code       5 non-null      int64  \n",
      " 1   county          5 non-null      object \n",
      " 2   state           5 non-null      object \n",
      " 3   customers_out   4 non-null      float64\n",
      " 4   run_start_time  5 non-null      object \n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     1\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2021.csv:\n",
      "   fips_code    county    state  customers_out       run_start_time\n",
      "0       1003   Baldwin  Alabama              6  2021-01-01 00:00:00\n",
      "1       1009    Blount  Alabama              4  2021-01-01 00:00:00\n",
      "2       1017  Chambers  Alabama              4  2021-01-01 00:00:00\n",
      "3       1031    Coffee  Alabama              1  2021-01-01 00:00:00\n",
      "4       1051    Elmore  Alabama              4  2021-01-01 00:00:00\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   fips_code       5 non-null      int64 \n",
      " 1   county          5 non-null      object\n",
      " 2   state           5 non-null      object\n",
      " 3   customers_out   5 non-null      int64 \n",
      " 4   run_start_time  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     0\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of coverage_history.csv:\n",
      "     year state  total_customers  min_covered  max_covered  min_pct_covered  \\\n",
      "0  1/1/18    AK           340543       229424       229424             0.67   \n",
      "1  1/1/19    AK           340543       217506       229424             0.64   \n",
      "2  1/1/20    AK           328964       158477       224243             0.48   \n",
      "3  1/1/21    AK           331443       224243       226079             0.68   \n",
      "4  1/1/22    AK           364614       226079       258830             0.62   \n",
      "\n",
      "   max_pct_covered  \n",
      "0             0.67  \n",
      "1             0.67  \n",
      "2             0.68  \n",
      "3             0.68  \n",
      "4             0.71  \n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   year             5 non-null      object \n",
      " 1   state            5 non-null      object \n",
      " 2   total_customers  5 non-null      int64  \n",
      " 3   min_covered      5 non-null      int64  \n",
      " 4   max_covered      5 non-null      int64  \n",
      " 5   min_pct_covered  5 non-null      float64\n",
      " 6   max_pct_covered  5 non-null      float64\n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 408.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "year               0\n",
      "state              0\n",
      "total_customers    0\n",
      "min_covered        0\n",
      "max_covered        0\n",
      "min_pct_covered    0\n",
      "max_pct_covered    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2016.csv:\n",
      "   fips_code           county     state  customers_out       run_start_time\n",
      "0       2122  Kenai Peninsula    Alaska            657  2016-01-01 00:00:00\n",
      "1       5003           Ashley  Arkansas              7  2016-01-01 00:00:00\n",
      "2       5029           Conway  Arkansas              2  2016-01-01 00:00:00\n",
      "3       5069        Jefferson  Arkansas             41  2016-01-01 00:00:00\n",
      "4       5081     Little River  Arkansas             61  2016-01-01 00:00:00\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   fips_code       5 non-null      int64 \n",
      " 1   county          5 non-null      object\n",
      " 2   state           5 non-null      object\n",
      " 3   customers_out   5 non-null      int64 \n",
      " 4   run_start_time  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     0\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2017.csv:\n",
      "   fips_code                county     state  customers_out  \\\n",
      "0       1003               Baldwin   Alabama              1   \n",
      "1       2090  Fairbanks North Star    Alaska            245   \n",
      "2       2122       Kenai Peninsula    Alaska              5   \n",
      "3       2290         Yukon-Koyukuk    Alaska              2   \n",
      "4       5035            Crittenden  Arkansas              1   \n",
      "\n",
      "        run_start_time  \n",
      "0  2017-01-01 00:00:00  \n",
      "1  2017-01-01 00:00:00  \n",
      "2  2017-01-01 00:00:00  \n",
      "3  2017-01-01 00:00:00  \n",
      "4  2017-01-01 00:00:00  \n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   fips_code       5 non-null      int64 \n",
      " 1   county          5 non-null      object\n",
      " 2   state           5 non-null      object\n",
      " 3   customers_out   5 non-null      int64 \n",
      " 4   run_start_time  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     0\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2015.csv:\n",
      "   fips_code    county     state  customers_out       run_start_time\n",
      "0       1021   Chilton   Alabama              3  2015-01-01 00:00:00\n",
      "1       5003    Ashley  Arkansas              3  2015-01-01 00:00:00\n",
      "2       5023  Cleburne  Arkansas              1  2015-01-01 00:00:00\n",
      "3       5085    Lonoke  Arkansas              2  2015-01-01 00:00:00\n",
      "4       5107  Phillips  Arkansas              3  2015-01-01 00:00:00\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   fips_code       5 non-null      int64 \n",
      " 1   county          5 non-null      object\n",
      " 2   state           5 non-null      object\n",
      " 3   customers_out   5 non-null      int64 \n",
      " 4   run_start_time  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     0\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of eaglei_outages_2014.csv:\n",
      "   fips_code     county    state  customers_out       run_start_time\n",
      "0       1037      Coosa  Alabama             12  2014-11-01 04:00:00\n",
      "1       1051     Elmore  Alabama              7  2014-11-01 04:00:00\n",
      "2       1109       Pike  Alabama              1  2014-11-01 04:00:00\n",
      "3       1121  Talladega  Alabama             31  2014-11-01 04:00:00\n",
      "4       4017     Navajo  Arizona              1  2014-11-01 04:00:00\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   fips_code       5 non-null      int64 \n",
      " 1   county          5 non-null      object\n",
      " 2   state           5 non-null      object\n",
      " 3   customers_out   5 non-null      int64 \n",
      " 4   run_start_time  5 non-null      object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 328.0+ bytes\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "fips_code         0\n",
      "county            0\n",
      "state             0\n",
      "customers_out     0\n",
      "run_start_time    0\n",
      "dtype: int64\n",
      "\n",
      " **Storm Event Data Preview**\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2014_c20231116.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           201402         18        1000         201402       18      2000   \n",
      "1           201402          5         300         201402        5      2300   \n",
      "2           201401         18        1000         201401       19       700   \n",
      "3           201411         26        1000         201411       27      1000   \n",
      "4           201402         13         630         201402       14       800   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID          STATE  STATE_FIPS  ...  END_RANGE  \\\n",
      "0       83473    503953  NEW HAMPSHIRE          33  ...        NaN   \n",
      "1       83491    504065  NEW HAMPSHIRE          33  ...        NaN   \n",
      "2       82185    494521  NEW HAMPSHIRE          33  ...        NaN   \n",
      "3       91728    549746  NEW HAMPSHIRE          33  ...        NaN   \n",
      "4       83476    503982  NEW HAMPSHIRE          33  ...        NaN   \n",
      "\n",
      "  END_AZIMUTH END_LOCATION BEGIN_LAT  BEGIN_LON END_LAT END_LON  \\\n",
      "0         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "1         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "2         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "3         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "4         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  Low pressure developing south of Long Island a...   \n",
      "1  Low pressure moving off the mid-Atlantic coast...   \n",
      "2  Low pressure brought a brief period of heavy s...   \n",
      "3  A strong coastal storm moved up the east coast...   \n",
      "4  A significant winter storm brought six to twel...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  Eight to twelve inches of snow fell across eas...         CSV  \n",
      "1  Six to twelve inches of snow fell across easte...         CSV  \n",
      "2  Four to eight inches of snow fell across easte...         CSV  \n",
      "3  Six to eight inches of snow fell across easter...         CSV  \n",
      "4  Five to eight inches of snow fell across easte...         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     5 non-null      object \n",
      " 25  DAMAGE_CROPS        5 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           0 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      0 non-null      float64\n",
      " 29  FLOOD_CAUSE         0 non-null      float64\n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         0 non-null      float64\n",
      " 32  TOR_LENGTH          0 non-null      float64\n",
      " 33  TOR_WIDTH           0 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         0 non-null      float64\n",
      " 39  BEGIN_AZIMUTH       0 non-null      float64\n",
      " 40  BEGIN_LOCATION      0 non-null      float64\n",
      " 41  END_RANGE           0 non-null      float64\n",
      " 42  END_AZIMUTH         0 non-null      float64\n",
      " 43  END_LOCATION        0 non-null      float64\n",
      " 44  BEGIN_LAT           0 non-null      float64\n",
      " 45  BEGIN_LON           0 non-null      float64\n",
      " 46  END_LAT             0 non-null      float64\n",
      " 47  END_LON             0 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     5 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(21), int64(15), object(15)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       0\n",
      "DAMAGE_CROPS          0\n",
      "SOURCE                0\n",
      "MAGNITUDE             5\n",
      "MAGNITUDE_TYPE        5\n",
      "FLOOD_CAUSE           5\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           5\n",
      "TOR_LENGTH            5\n",
      "TOR_WIDTH             5\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           5\n",
      "BEGIN_AZIMUTH         5\n",
      "BEGIN_LOCATION        5\n",
      "END_RANGE             5\n",
      "END_AZIMUTH           5\n",
      "END_LOCATION          5\n",
      "BEGIN_LAT             5\n",
      "BEGIN_LON             5\n",
      "END_LAT               5\n",
      "END_LON               5\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       0\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2019_c20240117.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           201905          9        1554         201905        9      1830   \n",
      "1           201908          1           0         201908        7      1400   \n",
      "2           201909         25        1823         201909       25      1825   \n",
      "3           201902         19        2226         201902       19      2350   \n",
      "4           201902         19        2255         201902       19      2355   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID         STATE  STATE_FIPS  ...  END_RANGE END_AZIMUTH  \\\n",
      "0      137295    824116         TEXAS          48  ...        7.0         NNE   \n",
      "1      141502    849617  SOUTH DAKOTA          46  ...        3.0           W   \n",
      "2      141998    852808       ARIZONA           4  ...       24.0           S   \n",
      "3      134941    808922      ARKANSAS           5  ...        NaN         NaN   \n",
      "4      134941    808923      ARKANSAS           5  ...        NaN         NaN   \n",
      "\n",
      "   END_LOCATION BEGIN_LAT  BEGIN_LON  END_LAT   END_LON  \\\n",
      "0  SAN GERONIMO   29.7898   -98.6406  29.7158  -98.7744   \n",
      "1         BRUCE   44.5400   -96.9600  44.4300  -96.9400   \n",
      "2      OCOTILLO   32.8700  -111.8800  32.8788 -111.8750   \n",
      "3           NaN       NaN        NaN      NaN       NaN   \n",
      "4           NaN       NaN        NaN      NaN       NaN   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  Thunderstorms developed along a cold front as ...   \n",
      "1  Minor flooding slowly dwindled during early Au...   \n",
      "2  Scattered thunderstorms developed over the cen...   \n",
      "3  Rain was heavy at times on the 19th, and there...   \n",
      "4  Rain was heavy at times on the 19th, and there...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  Thunderstorms produced heavy rain that led to ...         CSV  \n",
      "1  A continuation of flooding from July, the Big ...         CSV  \n",
      "2  Scattered thunderstorms developed across the c...         CSV  \n",
      "3  One-quarter inch of freezing rain was measured...         CSV  \n",
      "4  One-quarter inch of freezing rain was measured...         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     5 non-null      object \n",
      " 25  DAMAGE_CROPS        5 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           0 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      0 non-null      float64\n",
      " 29  FLOOD_CAUSE         2 non-null      object \n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         1 non-null      object \n",
      " 32  TOR_LENGTH          1 non-null      float64\n",
      " 33  TOR_WIDTH           1 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         3 non-null      float64\n",
      " 39  BEGIN_AZIMUTH       3 non-null      object \n",
      " 40  BEGIN_LOCATION      3 non-null      object \n",
      " 41  END_RANGE           3 non-null      float64\n",
      " 42  END_AZIMUTH         3 non-null      object \n",
      " 43  END_LOCATION        3 non-null      object \n",
      " 44  BEGIN_LAT           3 non-null      float64\n",
      " 45  BEGIN_LON           3 non-null      float64\n",
      " 46  END_LAT             3 non-null      float64\n",
      " 47  END_LON             3 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     5 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(15), int64(15), object(21)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       0\n",
      "DAMAGE_CROPS          0\n",
      "SOURCE                0\n",
      "MAGNITUDE             5\n",
      "MAGNITUDE_TYPE        5\n",
      "FLOOD_CAUSE           3\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           4\n",
      "TOR_LENGTH            4\n",
      "TOR_WIDTH             4\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           2\n",
      "BEGIN_AZIMUTH         2\n",
      "BEGIN_LOCATION        2\n",
      "END_RANGE             2\n",
      "END_AZIMUTH           2\n",
      "END_LOCATION          2\n",
      "BEGIN_LAT             2\n",
      "BEGIN_LON             2\n",
      "END_LAT               2\n",
      "END_LON               2\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       0\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2021_c20240716.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           202112         11         349         202112       11       350   \n",
      "1           202112         11         249         202112       11       254   \n",
      "2           202112         11         325         202112       11       327   \n",
      "3           202112         11         232         202112       11       239   \n",
      "4           202112          6         724         202112        6       724   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID      STATE  STATE_FIPS  ...  END_RANGE END_AZIMUTH  \\\n",
      "0      165322    999750  TENNESSEE          47  ...          3          NW   \n",
      "1      165322    999613  TENNESSEE          47  ...          2         ESE   \n",
      "2      165322    999636  TENNESSEE          47  ...          2          SW   \n",
      "3      165322    999604  TENNESSEE          47  ...          4         NNW   \n",
      "4      165321    999306  TENNESSEE          47  ...          1           W   \n",
      "\n",
      "  END_LOCATION BEGIN_LAT  BEGIN_LON  END_LAT  END_LON  \\\n",
      "0   HUNTERS PT   36.3178   -86.3235  36.3296 -86.2965   \n",
      "1  BAKERSWORKS   36.0255   -87.3054  36.0736 -87.2330   \n",
      "2        AMQUI   36.2372   -86.7286  36.2572 -86.7035   \n",
      "3     PINEWOOD   35.9205   -87.6423  35.9725 -87.5068   \n",
      "4    JAMESTOWN   36.4322   -84.9405  36.4322 -84.9405   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  One of the worst tornado outbreaks ever record...   \n",
      "1  One of the worst tornado outbreaks ever record...   \n",
      "2  One of the worst tornado outbreaks ever record...   \n",
      "3  One of the worst tornado outbreaks ever record...   \n",
      "4  After some isolated thunderstorms moved across...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  This small EF-0 tornado was determined through...         CSV  \n",
      "1  This tornado developed just southeast of the D...         CSV  \n",
      "2  Severe straight-line winds caused significant ...         CSV  \n",
      "3  This tornado touched down in far northwest Hic...         CSV  \n",
      "4  A Facebook report indicated trees and power li...         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     5 non-null      object \n",
      " 25  DAMAGE_CROPS        5 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           2 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      2 non-null      object \n",
      " 29  FLOOD_CAUSE         0 non-null      float64\n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         3 non-null      object \n",
      " 32  TOR_LENGTH          3 non-null      float64\n",
      " 33  TOR_WIDTH           3 non-null      float64\n",
      " 34  TOR_OTHER_WFO       1 non-null      object \n",
      " 35  TOR_OTHER_CZ_STATE  1 non-null      object \n",
      " 36  TOR_OTHER_CZ_FIPS   1 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   1 non-null      object \n",
      " 38  BEGIN_RANGE         5 non-null      int64  \n",
      " 39  BEGIN_AZIMUTH       5 non-null      object \n",
      " 40  BEGIN_LOCATION      5 non-null      object \n",
      " 41  END_RANGE           5 non-null      int64  \n",
      " 42  END_AZIMUTH         5 non-null      object \n",
      " 43  END_LOCATION        5 non-null      object \n",
      " 44  BEGIN_LAT           5 non-null      float64\n",
      " 45  BEGIN_LON           5 non-null      float64\n",
      " 46  END_LAT             5 non-null      float64\n",
      " 47  END_LON             5 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     5 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(10), int64(17), object(24)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       0\n",
      "DAMAGE_CROPS          0\n",
      "SOURCE                0\n",
      "MAGNITUDE             3\n",
      "MAGNITUDE_TYPE        3\n",
      "FLOOD_CAUSE           5\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           2\n",
      "TOR_LENGTH            2\n",
      "TOR_WIDTH             2\n",
      "TOR_OTHER_WFO         4\n",
      "TOR_OTHER_CZ_STATE    4\n",
      "TOR_OTHER_CZ_FIPS     4\n",
      "TOR_OTHER_CZ_NAME     4\n",
      "BEGIN_RANGE           0\n",
      "BEGIN_AZIMUTH         0\n",
      "BEGIN_LOCATION        0\n",
      "END_RANGE             0\n",
      "END_AZIMUTH           0\n",
      "END_LOCATION          0\n",
      "BEGIN_LAT             0\n",
      "BEGIN_LON             0\n",
      "END_LAT               0\n",
      "END_LON               0\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       0\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2018_c20240716.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           201806          6        1810         201806        6      1810   \n",
      "1           201806          6        1940         201806        6      1940   \n",
      "2           201806          6        1741         201806        6      1741   \n",
      "3           201806         30        2330         201806       30      2332   \n",
      "4           201806         30        2345         201806       30      2345   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID     STATE  STATE_FIPS  ...  END_RANGE END_AZIMUTH  \\\n",
      "0      125578    753161  NEBRASKA          31  ...         36           N   \n",
      "1      125578    753162  NEBRASKA          31  ...         15           S   \n",
      "2      125578    753160  NEBRASKA          31  ...          1          NW   \n",
      "3      125988    755273   VERMONT          50  ...          3         WSW   \n",
      "4      125988    755929   VERMONT          50  ...          0          NE   \n",
      "\n",
      "     END_LOCATION BEGIN_LAT  BEGIN_LON  END_LAT   END_LON  \\\n",
      "0         OSHKOSH   41.9300  -102.2100  41.9300 -102.2100   \n",
      "1       RUSHVILLE   42.5100  -102.4700  42.5100 -102.4700   \n",
      "2         BINGHAM   42.0300  -102.1000  42.0300 -102.1000   \n",
      "3  WEST BERKSHIRE   44.9565   -72.8699  44.9565  -72.8699   \n",
      "4   BELVIDERE JCT   44.7316   -72.7474  44.7316  -72.7474   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  Severe storms developed in the Nebraska Panhan...   \n",
      "1  Severe storms developed in the Nebraska Panhan...   \n",
      "2  Severe storms developed in the Nebraska Panhan...   \n",
      "3  Vermont and northern NY influenced by heat rid...   \n",
      "4  Vermont and northern NY influenced by heat rid...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  Hail predominately penny size with some quarte...         CSV  \n",
      "1  Most of the hail was penny to nickle sized wit...         CSV  \n",
      "2  Hail mainly quarter size with some half dollar...         CSV  \n",
      "3       Numerous trees downed by thunderstorm winds.         CSV  \n",
      "4  At least half dozen trees downed or snapped al...         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     5 non-null      object \n",
      " 25  DAMAGE_CROPS        5 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           5 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      2 non-null      object \n",
      " 29  FLOOD_CAUSE         0 non-null      float64\n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         0 non-null      float64\n",
      " 32  TOR_LENGTH          0 non-null      float64\n",
      " 33  TOR_WIDTH           0 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         5 non-null      int64  \n",
      " 39  BEGIN_AZIMUTH       5 non-null      object \n",
      " 40  BEGIN_LOCATION      5 non-null      object \n",
      " 41  END_RANGE           5 non-null      int64  \n",
      " 42  END_AZIMUTH         5 non-null      object \n",
      " 43  END_LOCATION        5 non-null      object \n",
      " 44  BEGIN_LAT           5 non-null      float64\n",
      " 45  BEGIN_LON           5 non-null      float64\n",
      " 46  END_LAT             5 non-null      float64\n",
      " 47  END_LON             5 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     5 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(14), int64(17), object(20)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       0\n",
      "DAMAGE_CROPS          0\n",
      "SOURCE                0\n",
      "MAGNITUDE             0\n",
      "MAGNITUDE_TYPE        3\n",
      "FLOOD_CAUSE           5\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           5\n",
      "TOR_LENGTH            5\n",
      "TOR_WIDTH             5\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           0\n",
      "BEGIN_AZIMUTH         0\n",
      "BEGIN_LOCATION        0\n",
      "END_RANGE             0\n",
      "END_AZIMUTH           0\n",
      "END_LOCATION          0\n",
      "BEGIN_LAT             0\n",
      "BEGIN_LON             0\n",
      "END_LAT               0\n",
      "END_LON               0\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       0\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2024_c20241216.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           202402          7        1728         202402        7      1745   \n",
      "1           202402          8         804         202402        8       804   \n",
      "2           202402          2        1621         202402        2      1621   \n",
      "3           202402         14        1800         202402       16      1800   \n",
      "4           202402         18        2218         202402       19       200   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID    STATE  STATE_FIPS  ...  END_RANGE END_AZIMUTH  \\\n",
      "0      187494   1151178    TEXAS          48  ...        NaN         NaN   \n",
      "1      187655   1152279  WYOMING          56  ...        NaN         NaN   \n",
      "2      187493   1151454    TEXAS          48  ...        8.0          SE   \n",
      "3      187699   1152478  WYOMING          56  ...        NaN         NaN   \n",
      "4      187728   1152830  WYOMING          56  ...        NaN         NaN   \n",
      "\n",
      "  END_LOCATION BEGIN_LAT  BEGIN_LON  END_LAT   END_LON  \\\n",
      "0          NaN       NaN        NaN      NaN       NaN   \n",
      "1          NaN       NaN        NaN      NaN       NaN   \n",
      "2    SETH WARD   34.1385  -101.5707  34.1385 -101.5707   \n",
      "3          NaN       NaN        NaN      NaN       NaN   \n",
      "4          NaN       NaN        NaN      NaN       NaN   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  A powerful short wave trough tracking across t...   \n",
      "1  Strong winds were reported across the wind pro...   \n",
      "2  A negatively tilted short wave trough moving a...   \n",
      "3  Multiple periods of snowfall fell across south...   \n",
      "4  A bora wind event resulted in strong winds for...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0                                                NaN         CSV  \n",
      "1  UPR site EMKAY located 14 miles west southwest...         CSV  \n",
      "2  A Texas Tech University West Texas mesonet sit...         CSV  \n",
      "3  Numerous SNOTEL sites in the Snowy Range estim...         CSV  \n",
      "4  Multiple WYDOT mesonet stations along Intersta...         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     4 non-null      object \n",
      " 25  DAMAGE_CROPS        4 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           4 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      4 non-null      object \n",
      " 29  FLOOD_CAUSE         0 non-null      float64\n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         0 non-null      float64\n",
      " 32  TOR_LENGTH          0 non-null      float64\n",
      " 33  TOR_WIDTH           0 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         1 non-null      float64\n",
      " 39  BEGIN_AZIMUTH       1 non-null      object \n",
      " 40  BEGIN_LOCATION      1 non-null      object \n",
      " 41  END_RANGE           1 non-null      float64\n",
      " 42  END_AZIMUTH         1 non-null      object \n",
      " 43  END_LOCATION        1 non-null      object \n",
      " 44  BEGIN_LAT           1 non-null      float64\n",
      " 45  BEGIN_LON           1 non-null      float64\n",
      " 46  END_LAT             1 non-null      float64\n",
      " 47  END_LON             1 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     4 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(16), int64(15), object(20)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       1\n",
      "DAMAGE_CROPS          1\n",
      "SOURCE                0\n",
      "MAGNITUDE             1\n",
      "MAGNITUDE_TYPE        1\n",
      "FLOOD_CAUSE           5\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           5\n",
      "TOR_LENGTH            5\n",
      "TOR_WIDTH             5\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           4\n",
      "BEGIN_AZIMUTH         4\n",
      "BEGIN_LOCATION        4\n",
      "END_RANGE             4\n",
      "END_AZIMUTH           4\n",
      "END_LOCATION          4\n",
      "BEGIN_LAT             4\n",
      "BEGIN_LON             4\n",
      "END_LAT               4\n",
      "END_LON               4\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       1\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2023_c20241216.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           202310         25         230         202310       27       551   \n",
      "1           202310         25         230         202310       27      1437   \n",
      "2           202310         25         230         202310       27      1126   \n",
      "3           202310         25         230         202310       27      1301   \n",
      "4           202310         25         230         202310       27       600   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID         STATE  STATE_FIPS  ...  END_RANGE END_AZIMUTH  \\\n",
      "0      186682   1145781  NORTH DAKOTA          38  ...        NaN         NaN   \n",
      "1      186682   1145783  NORTH DAKOTA          38  ...        NaN         NaN   \n",
      "2      186682   1145784  NORTH DAKOTA          38  ...        NaN         NaN   \n",
      "3      186682   1145796  NORTH DAKOTA          38  ...        NaN         NaN   \n",
      "4      186682   1145884  NORTH DAKOTA          38  ...        NaN         NaN   \n",
      "\n",
      "  END_LOCATION BEGIN_LAT  BEGIN_LON END_LAT END_LON  \\\n",
      "0          NaN       NaN        NaN     NaN     NaN   \n",
      "1          NaN       NaN        NaN     NaN     NaN   \n",
      "2          NaN       NaN        NaN     NaN     NaN   \n",
      "3          NaN       NaN        NaN     NaN     NaN   \n",
      "4          NaN       NaN        NaN     NaN     NaN   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  In late October, a winter storm dumped heavy s...   \n",
      "1  In late October, a winter storm dumped heavy s...   \n",
      "2  In late October, a winter storm dumped heavy s...   \n",
      "3  In late October, a winter storm dumped heavy s...   \n",
      "4  In late October, a winter storm dumped heavy s...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  Public reports 7.5 inches at Black Tiger Bay C...         CSV  \n",
      "1  Local Police Department relays storm total sno...         CSV  \n",
      "2  Public reports 10 inches of storm total snowfa...         CSV  \n",
      "3  Emergency Manager reports 6 inches of storm to...         CSV  \n",
      "4  CoCoRaHS Station ND-GF-23 reports 8.8 inches o...         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     0 non-null      float64\n",
      " 25  DAMAGE_CROPS        0 non-null      float64\n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           0 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      0 non-null      float64\n",
      " 29  FLOOD_CAUSE         0 non-null      float64\n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         0 non-null      float64\n",
      " 32  TOR_LENGTH          0 non-null      float64\n",
      " 33  TOR_WIDTH           0 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         0 non-null      float64\n",
      " 39  BEGIN_AZIMUTH       0 non-null      float64\n",
      " 40  BEGIN_LOCATION      0 non-null      float64\n",
      " 41  END_RANGE           0 non-null      float64\n",
      " 42  END_AZIMUTH         0 non-null      float64\n",
      " 43  END_LOCATION        0 non-null      float64\n",
      " 44  BEGIN_LAT           0 non-null      float64\n",
      " 45  BEGIN_LON           0 non-null      float64\n",
      " 46  END_LAT             0 non-null      float64\n",
      " 47  END_LON             0 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     5 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(23), int64(15), object(13)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       5\n",
      "DAMAGE_CROPS          5\n",
      "SOURCE                0\n",
      "MAGNITUDE             5\n",
      "MAGNITUDE_TYPE        5\n",
      "FLOOD_CAUSE           5\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           5\n",
      "TOR_LENGTH            5\n",
      "TOR_WIDTH             5\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           5\n",
      "BEGIN_AZIMUTH         5\n",
      "BEGIN_LOCATION        5\n",
      "END_RANGE             5\n",
      "END_AZIMUTH           5\n",
      "END_LOCATION          5\n",
      "BEGIN_LAT             5\n",
      "BEGIN_LON             5\n",
      "END_LAT               5\n",
      "END_LON               5\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       0\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2016_c20220719.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           201607         15        1715         201607       15      1715   \n",
      "1           201607         15        1725         201607       15      1725   \n",
      "2           201607         16        1246         201607       16      1246   \n",
      "3           201607          8        1755         201607        8      1755   \n",
      "4           201607          8        1810         201607        8      1810   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID           STATE  STATE_FIPS  ...  END_RANGE  \\\n",
      "0      108769    651823  SOUTH CAROLINA          45  ...          1   \n",
      "1      108769    651825  SOUTH CAROLINA          45  ...          1   \n",
      "2      108812    651828  NORTH CAROLINA          37  ...          2   \n",
      "3      105872    635287       TENNESSEE          47  ...          1   \n",
      "4      105872    635296       TENNESSEE          47  ...          1   \n",
      "\n",
      "  END_AZIMUTH END_LOCATION BEGIN_LAT  BEGIN_LON END_LAT END_LON  \\\n",
      "0           N    BOYD HILL     34.94     -81.03   34.94  -81.03   \n",
      "1           S      FT MILL     35.01     -80.93   35.01  -80.93   \n",
      "2         ENE       OLD FT     35.64     -82.14   35.64  -82.14   \n",
      "3           W         JENA     35.65     -84.18   35.65  -84.18   \n",
      "4         WSW       PITNER     35.87     -83.77   35.87  -83.77   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  Scattered thunderstorms developed across the U...   \n",
      "1  Scattered thunderstorms developed across the U...   \n",
      "2  Scattered thunderstorms developed near the Blu...   \n",
      "3  The combination of summer heating and high low...   \n",
      "4  The combination of summer heating and high low...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  One to two feet of deep standing water develop...         CSV  \n",
      "1  NWS Columbia relayed a report of trees blown d...         CSV  \n",
      "2  Media reported two trees blown down along I-40...         CSV  \n",
      "3  Numerous trees were reported down in the Green...         CSV  \n",
      "4  Numerous trees were reported down in Seymour. ...         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     3 non-null      object \n",
      " 25  DAMAGE_CROPS        3 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           4 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      4 non-null      object \n",
      " 29  FLOOD_CAUSE         0 non-null      float64\n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         0 non-null      float64\n",
      " 32  TOR_LENGTH          0 non-null      float64\n",
      " 33  TOR_WIDTH           0 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         5 non-null      int64  \n",
      " 39  BEGIN_AZIMUTH       5 non-null      object \n",
      " 40  BEGIN_LOCATION      5 non-null      object \n",
      " 41  END_RANGE           5 non-null      int64  \n",
      " 42  END_AZIMUTH         5 non-null      object \n",
      " 43  END_LOCATION        5 non-null      object \n",
      " 44  BEGIN_LAT           5 non-null      float64\n",
      " 45  BEGIN_LON           5 non-null      float64\n",
      " 46  END_LAT             5 non-null      float64\n",
      " 47  END_LON             5 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     5 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(14), int64(17), object(20)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       2\n",
      "DAMAGE_CROPS          2\n",
      "SOURCE                0\n",
      "MAGNITUDE             1\n",
      "MAGNITUDE_TYPE        1\n",
      "FLOOD_CAUSE           5\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           5\n",
      "TOR_LENGTH            5\n",
      "TOR_WIDTH             5\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           0\n",
      "BEGIN_AZIMUTH         0\n",
      "BEGIN_LOCATION        0\n",
      "END_RANGE             0\n",
      "END_AZIMUTH           0\n",
      "END_LOCATION          0\n",
      "BEGIN_LAT             0\n",
      "BEGIN_LON             0\n",
      "END_LAT               0\n",
      "END_LON               0\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       0\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2017_c20230317.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           201704          6        1509         201704        6      1509   \n",
      "1           201704          6         930         201704        6       940   \n",
      "2           201704          5        1749         201704        5      1753   \n",
      "3           201704         16        1759         201704       16      1900   \n",
      "4           201704         15        1550         201704       15      1550   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID       STATE  STATE_FIPS  ...  END_RANGE END_AZIMUTH  \\\n",
      "0      113355    678791  NEW JERSEY          34  ...          1          NW   \n",
      "1      113459    679228     FLORIDA          12  ...          1          SW   \n",
      "2      113448    679268        OHIO          39  ...          3          NE   \n",
      "3      113697    682042        OHIO          39  ...          1          NW   \n",
      "4      113683    682062    NEBRASKA          31  ...          2         ENE   \n",
      "\n",
      "        END_LOCATION BEGIN_LAT  BEGIN_LON  END_LAT  END_LON  \\\n",
      "0        FRIES MILLS   39.6600   -75.0800  39.6600 -75.0800   \n",
      "1  FORT MYERS VILLAS   26.5010   -81.9980  26.5339 -81.8836   \n",
      "2           FAIRBORN   39.8500   -83.9900  39.8500 -83.9900   \n",
      "3         SUMMERSIDE   39.1065   -84.2875  39.1061 -84.2874   \n",
      "4          COLE ARPT   40.9800   -95.8900  40.9800 -95.8900   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  Low pressure tracked from the Ohio Valley into...   \n",
      "1  A line of thunderstorms developed along a pref...   \n",
      "2  Showers and thunderstorms developed ahead of a...   \n",
      "3  Thunderstorms with very heavy rain developed a...   \n",
      "4  An upper level storm system moved into Nebrask...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  A couple of trees were taken down due to thund...         CSV  \n",
      "1  Emergency management reported and broadcast me...         CSV  \n",
      "2  An entire tree was uprooted in a yard on Dayto...         CSV  \n",
      "3        Garage of a home was flooded by high water.         CSV  \n",
      "4                                                NaN         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     4 non-null      object \n",
      " 25  DAMAGE_CROPS        4 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           3 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      2 non-null      object \n",
      " 29  FLOOD_CAUSE         1 non-null      object \n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         1 non-null      object \n",
      " 32  TOR_LENGTH          1 non-null      float64\n",
      " 33  TOR_WIDTH           1 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         5 non-null      int64  \n",
      " 39  BEGIN_AZIMUTH       5 non-null      object \n",
      " 40  BEGIN_LOCATION      5 non-null      object \n",
      " 41  END_RANGE           5 non-null      int64  \n",
      " 42  END_AZIMUTH         5 non-null      object \n",
      " 43  END_LOCATION        5 non-null      object \n",
      " 44  BEGIN_LAT           5 non-null      float64\n",
      " 45  BEGIN_LON           5 non-null      float64\n",
      " 46  END_LAT             5 non-null      float64\n",
      " 47  END_LON             5 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     4 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(12), int64(17), object(22)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       1\n",
      "DAMAGE_CROPS          1\n",
      "SOURCE                0\n",
      "MAGNITUDE             2\n",
      "MAGNITUDE_TYPE        3\n",
      "FLOOD_CAUSE           4\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           4\n",
      "TOR_LENGTH            4\n",
      "TOR_WIDTH             4\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           0\n",
      "BEGIN_AZIMUTH         0\n",
      "BEGIN_LOCATION        0\n",
      "END_RANGE             0\n",
      "END_AZIMUTH           0\n",
      "END_LOCATION          0\n",
      "BEGIN_LAT             0\n",
      "BEGIN_LON             0\n",
      "END_LAT               0\n",
      "END_LON               0\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       1\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2015_c20240716.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           201501         27        1200         201501       28       400   \n",
      "1           201501         24         700         201501       24      2100   \n",
      "2           201501         27         600         201501       27      1200   \n",
      "3           201502         14         800         201502       15      1700   \n",
      "4           201502          7        2200         201502       10       500   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID          STATE  STATE_FIPS  ...  END_RANGE  \\\n",
      "0       92561    559139  NEW HAMPSHIRE          33  ...        NaN   \n",
      "1       92625    555067  NEW HAMPSHIRE          33  ...        NaN   \n",
      "2       92561    554564  NEW HAMPSHIRE          33  ...        NaN   \n",
      "3       93895    564438  NEW HAMPSHIRE          33  ...        NaN   \n",
      "4       93902    564486  NEW HAMPSHIRE          33  ...        NaN   \n",
      "\n",
      "  END_AZIMUTH END_LOCATION BEGIN_LAT  BEGIN_LON END_LAT END_LON  \\\n",
      "0         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "1         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "2         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "3         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "4         NaN          NaN       NaN        NaN     NaN     NaN   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  An area of low pressure developed off the Delm...   \n",
      "1  An area of low pressure moving rapidly northea...   \n",
      "2  An area of low pressure developed off the Delm...   \n",
      "3  Low pressure dropping southeast from Canada on...   \n",
      "4  A series of low pressure areas moving along a ...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  Once blizzard conditions subsided, snow and bl...         CSV  \n",
      "1                                                NaN         CSV  \n",
      "2                                                NaN         CSV  \n",
      "3                                                NaN         CSV  \n",
      "4                                                NaN         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     5 non-null      object \n",
      " 25  DAMAGE_CROPS        5 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           0 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      0 non-null      float64\n",
      " 29  FLOOD_CAUSE         0 non-null      float64\n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         0 non-null      float64\n",
      " 32  TOR_LENGTH          0 non-null      float64\n",
      " 33  TOR_WIDTH           0 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         0 non-null      float64\n",
      " 39  BEGIN_AZIMUTH       0 non-null      float64\n",
      " 40  BEGIN_LOCATION      0 non-null      float64\n",
      " 41  END_RANGE           0 non-null      float64\n",
      " 42  END_AZIMUTH         0 non-null      float64\n",
      " 43  END_LOCATION        0 non-null      float64\n",
      " 44  BEGIN_LAT           0 non-null      float64\n",
      " 45  BEGIN_LON           0 non-null      float64\n",
      " 46  END_LAT             0 non-null      float64\n",
      " 47  END_LON             0 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     1 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(21), int64(15), object(15)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       0\n",
      "DAMAGE_CROPS          0\n",
      "SOURCE                0\n",
      "MAGNITUDE             5\n",
      "MAGNITUDE_TYPE        5\n",
      "FLOOD_CAUSE           5\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           5\n",
      "TOR_LENGTH            5\n",
      "TOR_WIDTH             5\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           5\n",
      "BEGIN_AZIMUTH         5\n",
      "BEGIN_LOCATION        5\n",
      "END_RANGE             5\n",
      "END_AZIMUTH           5\n",
      "END_LOCATION          5\n",
      "BEGIN_LAT             5\n",
      "BEGIN_LON             5\n",
      "END_LAT               5\n",
      "END_LON               5\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       4\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2022_c20241121.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           202202         20        2118         202202       20      2218   \n",
      "1           202202         21         800         202202       22      1000   \n",
      "2           202202         22         200         202202       22       900   \n",
      "3           202202         18        1609         202202       18      1609   \n",
      "4           202202          2           0         202202        3         0   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID           STATE  STATE_FIPS  ...  END_RANGE  \\\n",
      "0      165464    999902          NEVADA          32  ...        NaN   \n",
      "1      165465    999903          NEVADA          32  ...        NaN   \n",
      "2      165465    999904          NEVADA          32  ...        NaN   \n",
      "3      165611   1001181  ATLANTIC SOUTH          87  ...        7.0   \n",
      "4      165668   1001527  AMERICAN SAMOA          97  ...        5.0   \n",
      "\n",
      "  END_AZIMUTH END_LOCATION BEGIN_LAT  BEGIN_LON  END_LAT   END_LON  \\\n",
      "0         NaN          NaN       NaN        NaN      NaN       NaN   \n",
      "1         NaN          NaN       NaN        NaN      NaN       NaN   \n",
      "2         NaN          NaN       NaN        NaN      NaN       NaN   \n",
      "3          SE  PONTE VEDRA    30.050   -81.1700  30.0500  -81.1700   \n",
      "4         NNW      VAITOGI   -14.333  -170.7157 -14.3393 -170.7268   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  Strong winds increased ahead of an approaching...   \n",
      "1  A low centered over northern and central Nevad...   \n",
      "2  A low centered over northern and central Nevad...   \n",
      "3  Pre-frontal showers and thunderstorms moved so...   \n",
      "4  A surface trough over the Islands held  the po...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  Station (UP994) 3.1 SE West Wendover, Elevatio...         CSV  \n",
      "1  Thirteen inches fell at station (BCSN2) Big Cr...         CSV  \n",
      "2  Fifteen inches fell at station (TJMN2) Toe Jam...         CSV  \n",
      "3  A brief waterspout was observed offshore of So...         CSV  \n",
      "4  Over a 24-hour period, WSO Pago Pago recorded ...         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     5 non-null      object \n",
      " 25  DAMAGE_CROPS        5 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           1 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      1 non-null      object \n",
      " 29  FLOOD_CAUSE         0 non-null      float64\n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         0 non-null      float64\n",
      " 32  TOR_LENGTH          0 non-null      float64\n",
      " 33  TOR_WIDTH           0 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         2 non-null      float64\n",
      " 39  BEGIN_AZIMUTH       2 non-null      object \n",
      " 40  BEGIN_LOCATION      2 non-null      object \n",
      " 41  END_RANGE           2 non-null      float64\n",
      " 42  END_AZIMUTH         2 non-null      object \n",
      " 43  END_LOCATION        2 non-null      object \n",
      " 44  BEGIN_LAT           2 non-null      float64\n",
      " 45  BEGIN_LON           2 non-null      float64\n",
      " 46  END_LAT             2 non-null      float64\n",
      " 47  END_LON             2 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     5 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(16), int64(15), object(20)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       0\n",
      "DAMAGE_CROPS          0\n",
      "SOURCE                0\n",
      "MAGNITUDE             4\n",
      "MAGNITUDE_TYPE        4\n",
      "FLOOD_CAUSE           5\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           5\n",
      "TOR_LENGTH            5\n",
      "TOR_WIDTH             5\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           3\n",
      "BEGIN_AZIMUTH         3\n",
      "BEGIN_LOCATION        3\n",
      "END_RANGE             3\n",
      "END_AZIMUTH           3\n",
      "END_LOCATION          3\n",
      "BEGIN_LAT             3\n",
      "BEGIN_LON             3\n",
      "END_LAT               3\n",
      "END_LON               3\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       0\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_details-ftp_v1.0_d2020_c20240620.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           202006         24        1620         202006       24      1620   \n",
      "1           202006         20        1930         202006       20      1930   \n",
      "2           202006          3        1550         202006        3      1550   \n",
      "3           202006         19        1900         202006       19      1900   \n",
      "4           202006         20        1900         202006       20      1900   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID    STATE  STATE_FIPS  ...  END_RANGE END_AZIMUTH  \\\n",
      "0      149684    902190  GEORGIA          13  ...          1           W   \n",
      "1      149048    898391   KANSAS          20  ...          8          SE   \n",
      "2      149149    899120   KANSAS          20  ...         14          NW   \n",
      "3      149046    898383   KANSAS          20  ...          2         SSE   \n",
      "4      149048    898388   KANSAS          20  ...          1          SE   \n",
      "\n",
      "       END_LOCATION BEGIN_LAT  BEGIN_LON  END_LAT   END_LON  \\\n",
      "0             DOLES   31.7000   -83.8900  31.7000  -83.8900   \n",
      "1           CALVERT   39.7571   -99.6684  39.7571  -99.6684   \n",
      "2        ST FRANCIS   39.9137  -101.9753  39.9137 -101.9753   \n",
      "3          BREWSTER   39.3400  -101.3700  39.3400 -101.3700   \n",
      "4  NORTON MUNI ARPT   39.8400   -99.8900  39.8400  -99.8900   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  As is typical during summer, scattered afterno...   \n",
      "1  Supercells in small clusters formed during the...   \n",
      "2  Thunderstorms formed in eastern Colorado durin...   \n",
      "3  Thunderstorms moved east off the central Rocki...   \n",
      "4  Supercells in small clusters formed during the...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  A power line was blown down on Highway 32W.  H...         CSV  \n",
      "1  Penny to quarter size hail reported and ongoin...         CSV  \n",
      "2  Dime to penny sized hail reported at the locat...         CSV  \n",
      "3  Officer reported an estimated 60 mph wind gust...         CSV  \n",
      "4                                                NaN         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     1 non-null      object \n",
      " 25  DAMAGE_CROPS        1 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           5 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      2 non-null      object \n",
      " 29  FLOOD_CAUSE         0 non-null      float64\n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         0 non-null      float64\n",
      " 32  TOR_LENGTH          0 non-null      float64\n",
      " 33  TOR_WIDTH           0 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         5 non-null      int64  \n",
      " 39  BEGIN_AZIMUTH       5 non-null      object \n",
      " 40  BEGIN_LOCATION      5 non-null      object \n",
      " 41  END_RANGE           5 non-null      int64  \n",
      " 42  END_AZIMUTH         5 non-null      object \n",
      " 43  END_LOCATION        5 non-null      object \n",
      " 44  BEGIN_LAT           5 non-null      float64\n",
      " 45  BEGIN_LON           5 non-null      float64\n",
      " 46  END_LAT             5 non-null      float64\n",
      " 47  END_LON             5 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     4 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(14), int64(17), object(20)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       4\n",
      "DAMAGE_CROPS          4\n",
      "SOURCE                0\n",
      "MAGNITUDE             0\n",
      "MAGNITUDE_TYPE        3\n",
      "FLOOD_CAUSE           5\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           5\n",
      "TOR_LENGTH            5\n",
      "TOR_WIDTH             5\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           0\n",
      "BEGIN_AZIMUTH         0\n",
      "BEGIN_LOCATION        0\n",
      "END_RANGE             0\n",
      "END_AZIMUTH           0\n",
      "END_LOCATION          0\n",
      "BEGIN_LAT             0\n",
      "BEGIN_LON             0\n",
      "END_LAT               0\n",
      "END_LON               0\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       1\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      "üîç Preview of StormEvents_2014_2024.csv:\n",
      "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
      "0           201402         18        1000         201402       18      2000   \n",
      "1           201403         30         831         201403       30       931   \n",
      "2           201404         27        2306         201404       27      2306   \n",
      "3           201404         27        2303         201404       27      2303   \n",
      "4           201402         15        1300         201402       15      2100   \n",
      "\n",
      "   EPISODE_ID  EVENT_ID          STATE  STATE_FIPS  ...  END_RANGE  \\\n",
      "0       83473    503953  NEW HAMPSHIRE          33  ...        NaN   \n",
      "1       83971    507163  MASSACHUSETTS          25  ...        1.0   \n",
      "2       83517    506236       MISSOURI          29  ...        1.0   \n",
      "3       83517    506237       MISSOURI          29  ...        1.0   \n",
      "4       83132    501499     WASHINGTON          53  ...        NaN   \n",
      "\n",
      "  END_AZIMUTH       END_LOCATION BEGIN_LAT  BEGIN_LON  END_LAT  END_LON  \\\n",
      "0         NaN                NaN       NaN        NaN      NaN      NaN   \n",
      "1         WNW  CHELMSFORD CENTER   42.5861   -71.3472  42.5867 -71.3469   \n",
      "2           W                AVA   36.9500   -92.6600  36.9500 -92.6600   \n",
      "3           W                AVA   36.9500   -92.6600  36.9500 -92.6600   \n",
      "4         NaN                NaN       NaN        NaN      NaN      NaN   \n",
      "\n",
      "                                   EPISODE_NARRATIVE  \\\n",
      "0  Low pressure developing south of Long Island a...   \n",
      "1  A stacked low pressure system passed south and...   \n",
      "2  A powerful storm system and a dry line produce...   \n",
      "3  A powerful storm system and a dry line produce...   \n",
      "4  A strong cold front produced strong winds for ...   \n",
      "\n",
      "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
      "0  Eight to twelve inches of snow fell across eas...         CSV  \n",
      "1  Boston Road was closed near Brian Road due to ...         CSV  \n",
      "2                                                NaN         CSV  \n",
      "3  Several power poles snapped and trees blown down.         CSV  \n",
      "4  Two stations measured strong wind gusts in the...         CSV  \n",
      "\n",
      "[5 rows x 51 columns]\n",
      "\n",
      "üìä Column Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     5 non-null      int64  \n",
      " 1   BEGIN_DAY           5 non-null      int64  \n",
      " 2   BEGIN_TIME          5 non-null      int64  \n",
      " 3   END_YEARMONTH       5 non-null      int64  \n",
      " 4   END_DAY             5 non-null      int64  \n",
      " 5   END_TIME            5 non-null      int64  \n",
      " 6   EPISODE_ID          5 non-null      int64  \n",
      " 7   EVENT_ID            5 non-null      int64  \n",
      " 8   STATE               5 non-null      object \n",
      " 9   STATE_FIPS          5 non-null      int64  \n",
      " 10  YEAR                5 non-null      int64  \n",
      " 11  MONTH_NAME          5 non-null      object \n",
      " 12  EVENT_TYPE          5 non-null      object \n",
      " 13  CZ_TYPE             5 non-null      object \n",
      " 14  CZ_FIPS             5 non-null      int64  \n",
      " 15  CZ_NAME             5 non-null      object \n",
      " 16  WFO                 5 non-null      object \n",
      " 17  BEGIN_DATE_TIME     5 non-null      object \n",
      " 18  CZ_TIMEZONE         5 non-null      object \n",
      " 19  END_DATE_TIME       5 non-null      object \n",
      " 20  INJURIES_DIRECT     5 non-null      int64  \n",
      " 21  INJURIES_INDIRECT   5 non-null      int64  \n",
      " 22  DEATHS_DIRECT       5 non-null      int64  \n",
      " 23  DEATHS_INDIRECT     5 non-null      int64  \n",
      " 24  DAMAGE_PROPERTY     5 non-null      object \n",
      " 25  DAMAGE_CROPS        5 non-null      object \n",
      " 26  SOURCE              5 non-null      object \n",
      " 27  MAGNITUDE           3 non-null      float64\n",
      " 28  MAGNITUDE_TYPE      2 non-null      object \n",
      " 29  FLOOD_CAUSE         1 non-null      object \n",
      " 30  CATEGORY            0 non-null      float64\n",
      " 31  TOR_F_SCALE         0 non-null      float64\n",
      " 32  TOR_LENGTH          0 non-null      float64\n",
      " 33  TOR_WIDTH           0 non-null      float64\n",
      " 34  TOR_OTHER_WFO       0 non-null      float64\n",
      " 35  TOR_OTHER_CZ_STATE  0 non-null      float64\n",
      " 36  TOR_OTHER_CZ_FIPS   0 non-null      float64\n",
      " 37  TOR_OTHER_CZ_NAME   0 non-null      float64\n",
      " 38  BEGIN_RANGE         3 non-null      float64\n",
      " 39  BEGIN_AZIMUTH       3 non-null      object \n",
      " 40  BEGIN_LOCATION      3 non-null      object \n",
      " 41  END_RANGE           3 non-null      float64\n",
      " 42  END_AZIMUTH         3 non-null      object \n",
      " 43  END_LOCATION        3 non-null      object \n",
      " 44  BEGIN_LAT           3 non-null      float64\n",
      " 45  BEGIN_LON           3 non-null      float64\n",
      " 46  END_LAT             3 non-null      float64\n",
      " 47  END_LON             3 non-null      float64\n",
      " 48  EPISODE_NARRATIVE   5 non-null      object \n",
      " 49  EVENT_NARRATIVE     4 non-null      object \n",
      " 50  DATA_SOURCE         5 non-null      object \n",
      "dtypes: float64(15), int64(15), object(21)\n",
      "memory usage: 2.1+ KB\n",
      "None\n",
      "\n",
      "‚ùó Missing Values:\n",
      "BEGIN_YEARMONTH       0\n",
      "BEGIN_DAY             0\n",
      "BEGIN_TIME            0\n",
      "END_YEARMONTH         0\n",
      "END_DAY               0\n",
      "END_TIME              0\n",
      "EPISODE_ID            0\n",
      "EVENT_ID              0\n",
      "STATE                 0\n",
      "STATE_FIPS            0\n",
      "YEAR                  0\n",
      "MONTH_NAME            0\n",
      "EVENT_TYPE            0\n",
      "CZ_TYPE               0\n",
      "CZ_FIPS               0\n",
      "CZ_NAME               0\n",
      "WFO                   0\n",
      "BEGIN_DATE_TIME       0\n",
      "CZ_TIMEZONE           0\n",
      "END_DATE_TIME         0\n",
      "INJURIES_DIRECT       0\n",
      "INJURIES_INDIRECT     0\n",
      "DEATHS_DIRECT         0\n",
      "DEATHS_INDIRECT       0\n",
      "DAMAGE_PROPERTY       0\n",
      "DAMAGE_CROPS          0\n",
      "SOURCE                0\n",
      "MAGNITUDE             2\n",
      "MAGNITUDE_TYPE        3\n",
      "FLOOD_CAUSE           4\n",
      "CATEGORY              5\n",
      "TOR_F_SCALE           5\n",
      "TOR_LENGTH            5\n",
      "TOR_WIDTH             5\n",
      "TOR_OTHER_WFO         5\n",
      "TOR_OTHER_CZ_STATE    5\n",
      "TOR_OTHER_CZ_FIPS     5\n",
      "TOR_OTHER_CZ_NAME     5\n",
      "BEGIN_RANGE           2\n",
      "BEGIN_AZIMUTH         2\n",
      "BEGIN_LOCATION        2\n",
      "END_RANGE             2\n",
      "END_AZIMUTH           2\n",
      "END_LOCATION          2\n",
      "BEGIN_LAT             2\n",
      "BEGIN_LON             2\n",
      "END_LAT               2\n",
      "END_LON               2\n",
      "EPISODE_NARRATIVE     0\n",
      "EVENT_NARRATIVE       1\n",
      "DATA_SOURCE           0\n",
      "dtype: int64\n",
      "\n",
      " **Non-CSV Files Found:**\n",
      "   - DQI_processing.R\n",
      "   - Uri_Map.R\n",
      " Please check these manually‚Äîsome may contain useful information.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Power Outage Files:\")\n",
    "for file in outage_files:\n",
    "    print(f\"   - {file} ({file.split('.')[-1]})\")  # Show file extensions\n",
    "\n",
    "print(\"\\n Storm Event Files:\")\n",
    "for file in storm_files:\n",
    "    print(f\"   - {file} ({file.split('.')[-1]})\")\n",
    "\n",
    "# Function to preview CSV files\n",
    "def preview_csv(file_path, num_rows=5):\n",
    "    \"\"\"Load and preview a CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=num_rows)  \n",
    "        print(f\"\\nüîç Preview of {os.path.basename(file_path)}:\")\n",
    "        print(df.head())  \n",
    "        print(\"\\nüìä Column Info:\")\n",
    "        print(df.info())  \n",
    "        print(\"\\n‚ùó Missing Values:\")\n",
    "        print(df.isnull().sum())  \n",
    "    except Exception as e:\n",
    "        print(f\" Error loading {file_path}: {e}\")\n",
    "\n",
    "# Preview power outage CSV datasets\n",
    "print(\"\\n **Power Outage Data Preview**\")\n",
    "for file in outage_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        preview_csv(os.path.join(outages_dir, file))\n",
    "\n",
    "# Preview storm event CSV datasets\n",
    "print(\"\\n **Storm Event Data Preview**\")\n",
    "for file in storm_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        preview_csv(os.path.join(storms_dir, file))\n",
    "\n",
    "# Highlight non-CSV files for manual review\n",
    "non_csv_files = [f for f in outage_files + storm_files if not f.endswith(\".csv\")]\n",
    "if non_csv_files:\n",
    "    print(\"\\n **Non-CSV Files Found:**\")\n",
    "    for file in non_csv_files:\n",
    "        print(f\"   - {file}\")\n",
    "    print(\" Please check these manually‚Äîsome may contain useful information.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd275c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Observations:**\n",
    "1. **No Major Missing Data Issues**  \n",
    "   - Most datasets have complete records.  \n",
    "   - **Exception:** `eaglei_outages_2020.csv` has a missing value in `customers_out`.\n",
    "\n",
    "2. **Date Format Consistency Check**  \n",
    "   - The `run_start_time` column in outage datasets is stored as an **object (string)**.  \n",
    "   - We need to **convert it to datetime** for time-based analysis.\n",
    "\n",
    "3. **County-Level Consistency**  \n",
    "   - Some datasets use `fips_code`, while others (like `MCC.csv`) use `County_FIPS`.  \n",
    "   - We should ensure **consistent naming and datatype alignment** before merging.\n",
    "\n",
    "4. **Power Outage Data Granularity**  \n",
    "   - Outage datasets (`eaglei_outages_YYYY.csv`) contain **county-level** power outage records.  \n",
    "   - `MCC.csv` provides **total customer counts per county**, which can be used for **normalization** (outages per 1000 customers).  \n",
    "   - `DQI.csv` and `coverage_history.csv` contain **data quality indicators and coverage trends**, useful for filtering.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "**Step 1:** Convert `run_start_time` to datetime.  \n",
    "**Step 2:** Standardize column names (e.g., `County_FIPS` ‚Üí `fips_code`).  \n",
    "**Step 3:** Handle missing values (`customers_out` in 2020).  \n",
    "**Step 4:** Merge datasets (Outages + MCC + Coverage + DQI).  \n",
    "**Step 5:** Compute new features, e.g., **outage rate per 1000 customers**.\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d26d330",
   "metadata": {},
   "source": [
    "\n",
    "The **big picture**:\n",
    "\n",
    "---\n",
    "\n",
    "**Goal of the Constellation EAGLE-I Power Outage Challenge**:\n",
    "\n",
    "**Overall Objective:**  \n",
    "> Use historical outage data (EAGLE-I) + extreme weather events (NOAA) to model, analyze, and forecast patterns in *power outages*.\n",
    "\n",
    "---\n",
    "\n",
    "**More specifically, the challenge asks to:**\n",
    "\n",
    "1. **Understand** the relationships between weather events and power outages.  \n",
    "   (e.g., *Does ice storm frequency explain outage spikes?*)\n",
    "\n",
    "2. **Build predictive models** for outages:\n",
    "   - Given a set of upcoming weather conditions (storm types, frequency),\n",
    "   - **Predict** the expected number of customers who might lose power.\n",
    "\n",
    "3. **Create informative visualizations** and **report insights**:\n",
    "   - Show how outages vary across **states**, **time**, **event types**.\n",
    "   - Show correlations between specific **storm types** and **outages**.\n",
    "\n",
    "4. **Innovate** with forecasting:\n",
    "   - Can you forecast future outage risks?\n",
    "   - Can you identify **critical areas** where infrastructure is more vulnerable?\n",
    "\n",
    "---\n",
    "\n",
    "**So practically:**\n",
    "- combine **outage time series** + **storm events time series**.\n",
    "- **model** outages using weather variables.\n",
    "- **analyze**, **forecast**, and **explain** the patterns.\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "\n",
    "- **Not just** merging data.\n",
    "- **Not just** visualizing.\n",
    "- **Ultimately** ‚Üí *forecast* or *predict* how bad outages could get under different weather conditions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380f94b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**\"Official\" Datasets (provided or expected):**\n",
    "| Data | Purpose |\n",
    "|:----|:--------|\n",
    "| EAGLE-I Outages (2014‚Äì2023) | Actual counts of customers out, by time and location. |\n",
    "| NOAA Storm Events Database (2014‚Äì2023) | Detailed records of extreme weather events: type, time, severity. |\n",
    "\n",
    "---\n",
    "\n",
    "**\"Extra\" Datasets We *Can* Use:**\n",
    "\n",
    "| Data | Why it might help |\n",
    "|:----|:-------------------|\n",
    "| **Weather** (temperature, wind speed, snow depth, rainfall, etc.) | Outages often happen because of *severe* versions of normal weather. |\n",
    "| **Population density** | More populated areas might report more customers out even for similar storms. |\n",
    "| **Infrastructure resilience data** (if available) | Areas with underground cables might suffer fewer outages. |\n",
    "| **Previous large blackout datasets** | To find patterns (e.g., cascading outages after storms). |\n",
    "| **Climate trends** (e.g., NOAA climate normals) | To detect if outages are increasing due to more frequent extreme events. |\n",
    "| **Energy infrastructure locations** (substations, grid maps) | To check if outages cluster near vulnerable assets. |\n",
    "\n",
    "---\n",
    "\n",
    "**Why add extra data?:**\n",
    "\n",
    "- Improve **feature engineering** ‚Üí better predictive models.\n",
    "- Explain *why* certain areas have more severe outages (not just storms!).\n",
    "- Build a more **resilient forecasting** system that considers multiple stressors.\n",
    "\n",
    "---\n",
    "\n",
    "- We **don't need extra data** to complete the challenge.\n",
    "- **But** bringing it in **could make your solution much stronger**, smarter, and more impressive to the judges.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372fc67",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Must Do**\n",
    "\n",
    "| Section | Key Actions We Need to Take |\n",
    "|:---|:---|\n",
    "| **1. Objective & Approach** | Predict the **occurrence**, **lead time**, **severity**, and/or **duration** of **power outages** from storms (or rare weather). We must **combine datasets** and justify your design choices. |\n",
    "| **2. Data & Feature Engineering** | Use the provided **storm** and **outage** datasets, plus optionally **external public data** (weather, population, infrastructure). We must **align** them carefully in **time** and **space** (e.g., county and date). |\n",
    "| **3. Modeling & Prediction** | Handle **rare events** carefully (imbalanced data). Predict ahead in **lead time** and **severity** (not just occurrence). **Choose appropriate ML models** (classic ML, time series models, or even deep learning if you want). |\n",
    "| **4. Metrics & Evaluation** | **Design your own metrics**! Suggested types: F1 score for rare events, lead-time error, severity prediction accuracy, location-based error. Explain and justify your evaluation methods. |\n",
    "| **5. Submission** | Submit **code + docs**: preprocessing, feature engineering, modeling, and evaluation all clearly shown. Make results **reproducible**. Include **insights** (which features mattered most). Creativity is encouraged! |\n",
    "\n",
    "---\n",
    "\n",
    "**In simple terms:**\n",
    "Predict **if** an outage will happen,  \n",
    "Predict **when** (lead time),  \n",
    "Predict **how bad** (customers out, duration),  \n",
    "Combine multiple datasets creatively and carefully,  \n",
    "Choose your own models, metrics, and validation strategies,  \n",
    "Show clear analysis, visualizations, and insights.\n",
    "\n",
    "---\n",
    "\n",
    "**Now, regarding your earlier question about **extra data**:  **\n",
    "‚Üí **YES**, you can (and *should*) use additional public datasets ‚Äî weather, population, critical infrastructure, etc.  \n",
    "‚Üí The goal is to create a model that feels as **real-world predictive** as possible.\n",
    "\n",
    "---\n",
    "\n",
    "**Immediate Next Steps I Recommend**\n",
    "\n",
    "| Step | What to Do |\n",
    "|:---|:---|\n",
    "| 1 | Finalize core datasets: Storm Events, Outages, and optionally Population or Weather Data. |\n",
    "| 2 | Build **state/county/time-aligned** tables combining outage counts and storm features. |\n",
    "| 3 | Start **feature engineering**: like event type, magnitude, lagged weather, county population normalization. |\n",
    "| 4 | Split data **time-wise** (e.g., train on 2014‚Äì2020, test on 2021‚Äì2023). |\n",
    "| 5 | Build **first simple model** (XGBoost classifier for outage yes/no, for example). |\n",
    "| 6 | Design **custom metrics**: F1 for rare events, lead time error, severity prediction error. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01641558",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "**Power Outage Forecasting Pipeline**\n",
    "\n",
    "**1. **Data Collection****\n",
    "\n",
    "- **Load Provided Data**:\n",
    "  - Outages: (`outages14_23_texas_df`)\n",
    "  - Storm Events: (`storm_events_df`)\n",
    "- **Optional External Data**:\n",
    "  - Population Data (by county)\n",
    "  - ERA5 Weather Data (temperature, wind, precipitation)\n",
    "\n",
    "---\n",
    "\n",
    "**2. **Preprocessing & Alignment****\n",
    "\n",
    "- **Fix Timestamps**: Parse all dates correctly.\n",
    "- **Fix Locations**: Standardize counties (maybe use FIPS codes).\n",
    "- **Filter**: Focus on Texas or selected states.\n",
    "- **Create Event Windows**: Match storms to outage periods.\n",
    "- **Aggregate**: \n",
    "  - Outages ‚Üí county-day level (`total_customers_out`)\n",
    "  - Storms ‚Üí county-day level (summarized storm features)\n",
    "\n",
    "---\n",
    "\n",
    "**3. **Feature Engineering****\n",
    "\n",
    "- **Storm Features**:\n",
    "  - Event type (hurricane, thunderstorm, etc.)\n",
    "  - Event severity (wind speed, property damage, etc.)\n",
    "- **Outage Features**:\n",
    "  - Rolling customers out (lags, moving averages)\n",
    "- **Time Features**:\n",
    "  - Day of week, seasonality (e.g., hurricane season)\n",
    "- **Population or Infra Features** (if external data):\n",
    "  - Normalize outages per capita\n",
    "  - Critical infrastructure density\n",
    "- **Lagged/Lead Features**:\n",
    "  - Weather 1‚Äì3 days before outage\n",
    "\n",
    "---\n",
    "\n",
    "**4. **Label Creation****\n",
    "\n",
    "- **Classification Target**:\n",
    "  - 1 = Major outage (e.g., >500 customers affected)\n",
    "  - 0 = No major outage\n",
    "- **Severity Target** (optional regression):\n",
    "  - Predict number of customers affected\n",
    "- **Lead Time Target** (optional):\n",
    "  - How many hours/days before the outage?\n",
    "\n",
    "---\n",
    "\n",
    "**5. **Modeling****\n",
    "\n",
    "- **Baseline**: Logistic Regression, Random Forest, XGBoost\n",
    "- **Advanced**: \n",
    "  - Time series models (Prophet, LSTM)\n",
    "  - Transformer-based models if ambitious\n",
    "- **Model Outputs**:\n",
    "  - Outage occurrence (yes/no)\n",
    "  - Outage severity (customers_out, duration)\n",
    "\n",
    "---\n",
    "\n",
    "**6. **Evaluation****\n",
    "\n",
    "- **Classification Metrics**:\n",
    "  - F1 Score\n",
    "  - Precision-Recall AUC\n",
    "- **Lead Time Metrics**:\n",
    "  - Mean absolute lead time error\n",
    "- **Severity Prediction Metrics**:\n",
    "  - MAE / RMSE on customers affected\n",
    "- **Spatial Metrics**:\n",
    "  - County-level location accuracy\n",
    "\n",
    "---\n",
    "\n",
    "**7. **Insights & Visualization****\n",
    "\n",
    "- **Feature Importances** (from tree models)\n",
    "- **Partial Dependence Plots** (PDP)\n",
    "- **Error Analysis**:\n",
    "  - Which counties or seasons are hardest to predict?\n",
    "- **Maps**:\n",
    "  - Outage heatmaps vs prediction maps\n",
    "\n",
    "---\n",
    "\n",
    "**8. **Submission****\n",
    "\n",
    "- Organized, reproducible notebooks/scripts\n",
    "- Requirements.txt or environment.yml\n",
    "- Summary of external datasets used\n",
    "- Documentation for rerunning everything\n",
    "\n",
    "---\n",
    "\n",
    "**Visual Sketch of the Pipeline**\n",
    "\n",
    "```plaintext\n",
    "üìÇ Data Loading\n",
    "    ‚Üì\n",
    "üîß Preprocessing & Alignment\n",
    "    ‚Üì\n",
    "üéõÔ∏è Feature Engineering\n",
    "    ‚Üì\n",
    "üè∑Ô∏è Labeling\n",
    "    ‚Üì\n",
    "üß† Modeling\n",
    "    ‚Üì\n",
    "üìà Evaluation\n",
    "    ‚Üì\n",
    "üìä Insights & Visualization\n",
    "    ‚Üì\n",
    "üìù Submission Package\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Minimal MVP version:**\n",
    "- Use only outages + storm events.\n",
    "- Make a binary classifier (outage yes/no next day).\n",
    "- Use Random Forest or XGBoost baseline.\n",
    "- Design a simple F1 + lead-time MAE metric.\n",
    "- Then later add external data if time permits.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff8195",
   "metadata": {},
   "source": [
    "The challenge is about **predicting outages** caused by **extreme weather** ‚Äî *where*, *when*, *how severe*.  \n",
    "So, the most important features will be those that capture:\n",
    "\n",
    "---\n",
    "\n",
    "1. **Weather severity and type** (‚Üí *What happened?*)\n",
    "\n",
    "- `event_type` (storm, tornado, flood, etc.)\n",
    "- `event_magnitude` (e.g., wind speed, rainfall amount, hail size)\n",
    "- `event_duration` (length of the storm)\n",
    "- `event_start_time`, `event_end_time`\n",
    "- `storm category` (for hurricanes, if available)\n",
    "\n",
    "---\n",
    "2. **Location and exposure** (‚Üí *Where and how vulnerable?*)\n",
    "\n",
    "- `county`, `state`, `fips_code`\n",
    "- **Population density** (external data)  \n",
    "  ‚Üí More people = more customers affected\n",
    "- **Urban vs. Rural indicator** (external or derived)  \n",
    "  ‚Üí Rural areas might recover differently than urban\n",
    "- **Infrastructure vulnerability** (optional external data)  \n",
    "  ‚Üí e.g., number of substations, grid strength if available\n",
    "\n",
    "---\n",
    "3. **Temporal context** (‚Üí *When did it happen?*)\n",
    "\n",
    "- **Seasonality features**:\n",
    "  - `month`, `day of week`, `hour of day`\n",
    "  - (`storm_season` indicator: e.g., hurricane season June-Nov)\n",
    "- **Lead time / time gaps** between storms and outages\n",
    "\n",
    "---\n",
    "4. **Historical storm and outage frequency** (‚Üí *Is this place usually hit?*)\n",
    "\n",
    "- **Storm frequency** by county over past X months\n",
    "- **Outage frequency** by county over past X months\n",
    "- (rolling averages, lagged counts)\n",
    "\n",
    "---\n",
    "5. **Magnitude of previous outages** (‚Üí *How bad was it before?*)\n",
    "\n",
    "- `avg_customers_out_last_month`\n",
    "- `max_customers_out_last_year`\n",
    "- `average outage duration in county`\n",
    "\n",
    "---\n",
    "6. **Alignment features** (critical for linking storms to outages)\n",
    "\n",
    "- `storm_to_outage_time_gap`\n",
    "- `storm_event_matches_outage` (binary: 1 if storm caused an outage, 0 otherwise ‚Äî for training)\n",
    "\n",
    "---\n",
    "\n",
    "**Most important categories**:\n",
    "- **Storm strength/severity**\n",
    "- **Location risk (population, infrastructure)**\n",
    "- **Timing (seasonality, lead time)**\n",
    "- **Historical vulnerability (previous storms, outages)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00416c0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "üìã Draft Feature Table:  \n",
    "*(includes what we already have from the provided datasets)*\n",
    "\n",
    "| Feature Name | Source Columns (or Derived) | Notes | Do we already have it? |\n",
    "|:-------------|:----------------------------|:------|:-----------------------|\n",
    "| `event_type` | storm events `event_type` | What kind of storm (e.g., thunderstorm, hurricane) | ‚úÖ |\n",
    "| `event_magnitude` | storm events (depends: wind speed, hail size, etc.) | Severity; may need parsing different columns | ‚ö†Ô∏è *depends on subtype* |\n",
    "| `event_start_time` | storm events `event_begin_time` | When the storm started | ‚úÖ |\n",
    "| `event_end_time` | storm events `event_end_time` | When the storm ended | ‚úÖ |\n",
    "| `county` | storm events `cz_name` and outages `county` | Match both datasets spatially | ‚úÖ |\n",
    "| `state` | storm events `state` and outages `state` | Should match too | ‚úÖ |\n",
    "| `fips_code` | outages `fips_code` | County-level ID for matching | ‚úÖ |\n",
    "| `customers_out` | outages `customers_out` | Target for severity prediction | ‚úÖ |\n",
    "| `outage_time` | outages `run_start_time` | When outages were recorded | ‚úÖ |\n",
    "| `event_duration` | Derived (`event_end_time` - `event_start_time`) | Duration of storm | ‚û°Ô∏è Easy to compute |\n",
    "| `outage_lead_time` | Derived (`outage_time` - `event_end_time`) | How soon after storm the outage happened | ‚û°Ô∏è Easy to compute |\n",
    "| `storm_season` | Derived from `event_start_time` | E.g., flag June‚ÄìNov as hurricane season | ‚û°Ô∏è Easy to compute |\n",
    "| `month` | Derived from `event_start_time` | Month of year (seasonality) | ‚û°Ô∏è Easy to compute |\n",
    "| `day_of_week` | Derived from `event_start_time` | Day of week (temporal patterns) | ‚û°Ô∏è Easy to compute |\n",
    "| `storm_count_past_X_days` | Rolling count from storm data | How stormy was it recently? | ‚ùå *(needs aggregation)* |\n",
    "| `outage_count_past_X_days` | Rolling count from outage data | Outage-prone counties | ‚ùå *(needs aggregation)* |\n",
    "| `population_density` | External (e.g., census) | More people = bigger impact | ‚ùå *(external lookup)* |\n",
    "| `urban_rural_indicator` | External (e.g., USDA) | City vs rural areas | ‚ùå *(external lookup)* |\n",
    "| `infrastructure_risk` | External (optional) | Power grid vulnerability | ‚ùå *(if available)* |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ = already available directly  \n",
    "‚û°Ô∏è = easy to engineer from available data  \n",
    "‚ùå = would require external data or more work  \n",
    "‚ö†Ô∏è = depends: need to decide which storm subtype columns to use for \"magnitude\"\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41c598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading outage files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:54<00:00,  5.41s/file]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>customers_out</th>\n",
       "      <th>run_start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133063</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133064</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133065</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133066</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191133067</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191133068 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               state  customers_out       run_start_time\n",
       "0            Alabama           12.0  2014-11-01 04:00:00\n",
       "1            Alabama            7.0  2014-11-01 04:00:00\n",
       "2            Alabama            1.0  2014-11-01 04:00:00\n",
       "3            Alabama           31.0  2014-11-01 04:00:00\n",
       "4            Arizona            1.0  2014-11-01 04:00:00\n",
       "...              ...            ...                  ...\n",
       "191133063  Wisconsin            0.0  2023-12-31 23:45:00\n",
       "191133064  Wisconsin            1.0  2023-12-31 23:45:00\n",
       "191133065  Wisconsin            0.0  2023-12-31 23:45:00\n",
       "191133066  Wisconsin            0.0  2023-12-31 23:45:00\n",
       "191133067    Wyoming            2.0  2023-12-31 23:45:00\n",
       "\n",
       "[191133068 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_outage_selected_cols(outages_dir, columns=['state', 'customers_out', 'run_start_time']):\n",
    "    \"\"\"\n",
    "    Efficiently loads selected columns from all outage CSV files in a directory.\n",
    "\n",
    "    Parameters:\n",
    "    - outages_dir (str): Path to directory with outage CSVs.\n",
    "    - columns (list): List of columns to load from each file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Concatenated DataFrame with selected columns across all files.\n",
    "    \"\"\"\n",
    "\n",
    "    outage_files = sorted([\n",
    "        f for f in os.listdir(outages_dir)\n",
    "        if f.startswith(\"eaglei_outages_\") and f.endswith(\".csv\")\n",
    "    ])\n",
    "\n",
    "    df_list = []\n",
    "    for f in tqdm(outage_files, desc=\"Loading outage files\", unit=\"file\"):\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(outages_dir, f), usecols=columns)\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {f}: {e}\")\n",
    "\n",
    "    if df_list:\n",
    "        outages_df = pd.concat(df_list, ignore_index=True)\n",
    "        return outages_df\n",
    "    else:\n",
    "        print(\"No outage data loaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "outages_selected_df = load_outage_selected_cols(outages_dir)\n",
    "\n",
    "outages_selected_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa55734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outages_selected_df.info()\n",
    "\n",
    "# Normalize inconsistent state names\n",
    "normalize_state_names = {\n",
    "    'US Virgin Islands': 'United States Virgin Islands',\n",
    "    'U.S. Virgin Islands': 'United States Virgin Islands',\n",
    "    'Virgin Islands': 'United States Virgin Islands',\n",
    "    'District Of Columbia': 'District of Columbia',\n",
    "}\n",
    "\n",
    "outages_selected_df['state'] = outages_selected_df['state'].replace(normalize_state_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "479467b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Top 5 states by total customers out: ['Florida', 'California', 'Texas', 'Louisiana', 'Michigan']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_outages</th>\n",
       "      <th>total_customers_out</th>\n",
       "      <th>avg_customers_out</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Florida</th>\n",
       "      <td>9696235</td>\n",
       "      <td>4.269705e+09</td>\n",
       "      <td>452.182221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>7310527</td>\n",
       "      <td>2.880102e+09</td>\n",
       "      <td>405.826175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Texas</th>\n",
       "      <td>16413490</td>\n",
       "      <td>2.745459e+09</td>\n",
       "      <td>172.549290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Louisiana</th>\n",
       "      <td>6619865</td>\n",
       "      <td>2.198361e+09</td>\n",
       "      <td>338.634795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michigan</th>\n",
       "      <td>6894865</td>\n",
       "      <td>2.160075e+09</td>\n",
       "      <td>332.079810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total_outages  total_customers_out  avg_customers_out\n",
       "state                                                            \n",
       "Florida           9696235         4.269705e+09         452.182221\n",
       "California        7310527         2.880102e+09         405.826175\n",
       "Texas            16413490         2.745459e+09         172.549290\n",
       "Louisiana         6619865         2.198361e+09         338.634795\n",
       "Michigan          6894865         2.160075e+09         332.079810"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Least 5 states by total customers out: ['Montana', 'South Dakota', 'District of Columbia', 'North Dakota', 'Wyoming']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_outages</th>\n",
       "      <th>total_customers_out</th>\n",
       "      <th>avg_customers_out</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>749199</td>\n",
       "      <td>45331082.0</td>\n",
       "      <td>68.904579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>1586739</td>\n",
       "      <td>42664671.0</td>\n",
       "      <td>31.050350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>District of Columbia</th>\n",
       "      <td>202886</td>\n",
       "      <td>27338954.0</td>\n",
       "      <td>135.385591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>874415</td>\n",
       "      <td>23266926.0</td>\n",
       "      <td>39.501348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>650641</td>\n",
       "      <td>17728228.0</td>\n",
       "      <td>29.647552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      total_outages  total_customers_out  avg_customers_out\n",
       "state                                                                      \n",
       "Montana                      749199           45331082.0          68.904579\n",
       "South Dakota                1586739           42664671.0          31.050350\n",
       "District of Columbia         202886           27338954.0         135.385591\n",
       "North Dakota                 874415           23266926.0          39.501348\n",
       "Wyoming                      650641           17728228.0          29.647552"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now query top states\n",
    "state_outages_agg = (\n",
    "    outages_selected_df.groupby('state')\n",
    "    .agg(\n",
    "        total_outages=('state', 'count'),\n",
    "        total_customers_out=('customers_out', 'sum'),\n",
    "        avg_customers_out=('customers_out', 'mean')\n",
    "    )\n",
    "    .sort_values(by='total_customers_out', ascending=False)\n",
    ")\n",
    "\n",
    "top_outages_states_df = state_outages_agg.head(5)\n",
    "least_outages_states_df = state_outages_agg.tail(5)\n",
    "\n",
    "print(\"List of Top 5 states by total customers out:\", top_outages_states_df.index.tolist())\n",
    "display(top_outages_states_df)\n",
    "\n",
    "print(\"List Least 5 states by total customers out:\", least_outages_states_df.index.tolist())\n",
    "display(least_outages_states_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812f9bb",
   "metadata": {},
   "source": [
    "- The entire outages dataset was too large for analysis and modelling (7GB), so it's best to take portions\n",
    "capturing top and least impacted which are more insightful\n",
    "\n",
    "- we can then go back and filter for these group with all columns (fips, county, etc) for future granularity. These reduces the size of dataset to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa40a1a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Top 5 States by Total Customers Out**\n",
    "These are high-priority candidates for predictive modeling, especially if we're focused on **impact severity**:\n",
    "\n",
    "1. **Florida** ‚Äì Highest overall impact (4.27B customers out), very high average per outage.\n",
    "2. **California** ‚Äì Frequent outages and high total impact.\n",
    "3. **Texas** ‚Äì Most frequent outages overall, but lower average per event.\n",
    "4. **Louisiana** ‚Äì Fewer events than CA/TX but higher average impact per outage.\n",
    "5. **Michigan** ‚Äì Similar to Louisiana in impact profile.\n",
    "\n",
    "These states provide a **diverse mix**: hurricane-prone (FL, LA), wildfire-prone (CA), storm/cold-prone (MI), and grid-stress-prone (TX). This variation is useful for training more generalizable models.\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom 5 States**\n",
    "These are low-impact zones (by total `customers_out`), and generally **not ideal** for early modeling due to sparse or low-severity data:\n",
    "\n",
    "**['Montana', 'South Dakota', 'District of Columbia', 'North Dakota', 'Wyoming']**\n",
    "\n",
    "These states may serve better as controls or for testing model generalizability later.\n",
    "\n",
    "---\n",
    "\n",
    "**Follow up:**\n",
    "Now that you have the top 5/ least 5 states:\n",
    "- Go back and reload full outage data for **top5/least5**, including all columns.\n",
    "- Then, filter corresponding storm event records for just those states.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Why Use Top 10 Instead of Just 5?**\n",
    "- **Model robustness**: Broader geographic and climatic diversity leads to better generalization.\n",
    "- **Extreme events**: Some states may be low on *total* outages but high on *rare/extreme* events ‚Äî these patterns only emerge with a larger sample.\n",
    "- **Comparative evaluation**: We‚Äôll be able to better test whether models trained on top 5 generalize well to states ranked 6‚Äì10.\n",
    "- **Future flexibility**: If some states become unusable due to data issues or require exclusion, we already have backups in the top 10.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a04bbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Montana', 'South Dakota', 'District of Columbia', 'North Dakota', 'Wyoming']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_outages_states_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e609f0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Florida', 'California', 'Texas', 'Louisiana', 'Michigan', 'Montana', 'South Dakota', 'District of Columbia', 'North Dakota', 'Wyoming']\n"
     ]
    }
   ],
   "source": [
    "top_least_10_outages_states = top_outages_states_df.index.tolist() + least_outages_states_df.index.tolist()\n",
    "print(top_least_10_outages_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c338b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages for ['Florida', 'California', 'Texas', 'Louisiana', 'Michigan', 'Montana', 'South Dakota', 'District of Columbia', 'North Dakota', 'Wyoming']: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:24<00:00,  8.42s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing outages for ['Florida', 'California', 'Texas', 'Louisiana', 'Michigan', 'Montana', 'South Dakota', 'District of Columbia', 'North Dakota', 'Wyoming']. Shape: (50998862, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_outages_dataset(states, outages_dir):\n",
    "    \"\"\"\n",
    "    Reads and filters outage CSVs for one or more states.\n",
    "    \n",
    "    Parameters:\n",
    "    - states (str or list of str): State name(s) to filter by.\n",
    "    - outages_dir (str): Path to directory containing outage CSVs.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined outages DataFrame filtered by state(s).\n",
    "    \"\"\"\n",
    "    if isinstance(states, str):\n",
    "        states = [states]\n",
    "        \n",
    "    outage_files = sorted([\n",
    "        f for f in os.listdir(outages_dir) \n",
    "        if f.startswith(\"eaglei_outages_\") and f.endswith(\".csv\")\n",
    "    ])\n",
    "\n",
    "    filtered_outages = []\n",
    "\n",
    "    for f in tqdm(outage_files, desc=f\"Processing outages for {states}\", unit=\"file\"):\n",
    "        file_path = os.path.join(outages_dir, f)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, parse_dates=['run_start_time'])\n",
    "            df_state = df[df['state'].isin(states)]\n",
    "            if not df_state.empty:\n",
    "                filtered_outages.append(df_state)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_path}: {e}\")\n",
    "\n",
    "    if not filtered_outages:\n",
    "        print(f\"No outage records found for states: {states}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    outages_df = pd.concat(filtered_outages, ignore_index=True)\n",
    "    outages_df = outages_df.rename(columns={'run_start_time': 'outages_start_time'})\n",
    "\n",
    "    print(f\"Finished processing outages for {states}. Shape: {outages_df.shape}\")\n",
    "    return outages_df\n",
    "\n",
    "\n",
    "# Multiple states\n",
    "# tx_fl_outages_df = prepare_outages_dataset([\"Texas\", \"Florida\"], outages_dir)\n",
    "top_least_10_states_outages_df = prepare_outages_dataset(top_least_10_outages_states, outages_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf36f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50998862 entries, 0 to 50998861\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   fips_code           int64         \n",
      " 1   county              object        \n",
      " 2   state               object        \n",
      " 3   customers_out       float64       \n",
      " 4   outages_start_time  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(2)\n",
      "memory usage: 1.9+ GB\n"
     ]
    }
   ],
   "source": [
    "top_least_10_states_outages_df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5471f1f",
   "metadata": {},
   "source": [
    "- This is a fairer size, though we will still take it further down during modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e8a7c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips_code</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>customers_out</th>\n",
       "      <th>outages_start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6029</td>\n",
       "      <td>Kern</td>\n",
       "      <td>California</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6037</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6065</td>\n",
       "      <td>Riverside</td>\n",
       "      <td>California</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6113</td>\n",
       "      <td>Yolo</td>\n",
       "      <td>California</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12011</td>\n",
       "      <td>Broward</td>\n",
       "      <td>Florida</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50998857</th>\n",
       "      <td>48473</td>\n",
       "      <td>Waller</td>\n",
       "      <td>Texas</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50998858</th>\n",
       "      <td>48481</td>\n",
       "      <td>Wharton</td>\n",
       "      <td>Texas</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50998859</th>\n",
       "      <td>48489</td>\n",
       "      <td>Willacy</td>\n",
       "      <td>Texas</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50998860</th>\n",
       "      <td>48491</td>\n",
       "      <td>Williamson</td>\n",
       "      <td>Texas</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50998861</th>\n",
       "      <td>56039</td>\n",
       "      <td>Teton</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2023-12-31 23:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50998862 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          fips_code       county       state  customers_out  \\\n",
       "0              6029         Kern  California           30.0   \n",
       "1              6037  Los Angeles  California         1555.0   \n",
       "2              6065    Riverside  California            2.0   \n",
       "3              6113         Yolo  California            1.0   \n",
       "4             12011      Broward     Florida           17.0   \n",
       "...             ...          ...         ...            ...   \n",
       "50998857      48473       Waller       Texas            2.0   \n",
       "50998858      48481      Wharton       Texas           33.0   \n",
       "50998859      48489      Willacy       Texas            4.0   \n",
       "50998860      48491   Williamson       Texas            1.0   \n",
       "50998861      56039        Teton     Wyoming            2.0   \n",
       "\n",
       "          outages_start_time  \n",
       "0        2014-11-01 04:00:00  \n",
       "1        2014-11-01 04:00:00  \n",
       "2        2014-11-01 04:00:00  \n",
       "3        2014-11-01 04:00:00  \n",
       "4        2014-11-01 04:00:00  \n",
       "...                      ...  \n",
       "50998857 2023-12-31 23:45:00  \n",
       "50998858 2023-12-31 23:45:00  \n",
       "50998859 2023-12-31 23:45:00  \n",
       "50998860 2023-12-31 23:45:00  \n",
       "50998861 2023-12-31 23:45:00  \n",
       "\n",
       "[50998862 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_least_10_states_outages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36a10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2659e70",
   "metadata": {},
   "source": [
    "\n",
    "---  \n",
    "### 2.2 Load Storm Events Data\n",
    "\n",
    "- Read .csv files in ../dynamic_rhythm_env/NOAA_StormEvents/\n",
    "- Inspect structure, relevant columns like event_type, state, begin_date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7965a2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Top 5 States by Storm Event: ['TEXAS', 'KANSAS', 'VIRGINIA', 'IOWA', 'CALIFORNIA']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_events</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>total_injuries</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TEXAS</th>\n",
       "      <td>52039</td>\n",
       "      <td>625</td>\n",
       "      <td>3297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KANSAS</th>\n",
       "      <td>25616</td>\n",
       "      <td>22</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIRGINIA</th>\n",
       "      <td>22665</td>\n",
       "      <td>54</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IOWA</th>\n",
       "      <td>22610</td>\n",
       "      <td>38</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CALIFORNIA</th>\n",
       "      <td>22467</td>\n",
       "      <td>472</td>\n",
       "      <td>1164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total_events  total_deaths  total_injuries\n",
       "STATE                                                 \n",
       "TEXAS              52039           625            3297\n",
       "KANSAS             25616            22             159\n",
       "VIRGINIA           22665            54             242\n",
       "IOWA               22610            38             280\n",
       "CALIFORNIA         22467           472            1164"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Least 5 States by Storm Event: ['E PACIFIC', 'ST LAWRENCE R', 'HAWAII WATERS', 'GULF OF ALASKA', 'GUAM WATERS']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_events</th>\n",
       "      <th>total_deaths</th>\n",
       "      <th>total_injuries</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E PACIFIC</th>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ST LAWRENCE R</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HAWAII WATERS</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GULF OF ALASKA</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GUAM WATERS</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                total_events  total_deaths  total_injuries\n",
       "STATE                                                     \n",
       "E PACIFIC                 72             9               6\n",
       "ST LAWRENCE R             17             0               0\n",
       "HAWAII WATERS             14             1               1\n",
       "GULF OF ALASKA             6             0               0\n",
       "GUAM WATERS                1             0               0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load storm events file (2014‚Äì2024)\n",
    "storm_events_path = '../dynamic_rhythm_train_data/NOAA_StormEvents/StormEvents_2014_2024.csv'\n",
    "storm_df = pd.read_csv(storm_events_path)\n",
    "\n",
    "# Group by state to get total number of storm events\n",
    "storm_summary = storm_df.groupby('STATE').agg(\n",
    "    total_events=('EVENT_ID', 'count'),\n",
    "    total_deaths=('DEATHS_DIRECT', 'sum'),\n",
    "    total_injuries=('INJURIES_DIRECT', 'sum')\n",
    ").sort_values(by='total_events', ascending=False)\n",
    "\n",
    "# Top 5 states by storm event frequency\n",
    "top_5_storm_states = storm_summary.head(5)\n",
    "\n",
    "# Bottom 5 states (least number of storm events)\n",
    "least_5_storm_states = storm_summary.tail(5)\n",
    "\n",
    "# Display\n",
    "print(\"List of Top 5 States by Storm Event:\", top_5_storm_states.index.tolist())\n",
    "display(top_5_storm_states)\n",
    "\n",
    "print(\"List of Least 5 States by Storm Event:\", least_5_storm_states.index.tolist())\n",
    "display(least_5_storm_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f78813",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**Top 5 States (by total storm events)**\n",
    "These are strong candidates for outage prediction based on weather:\n",
    "\n",
    "| State      | Events | Deaths | Injuries |\n",
    "|------------|--------|--------|----------|\n",
    "| **Texas**      | 52,039 | 625    | 3,297     |\n",
    "| **Kansas**     | 25,616 | 22     | 159       |\n",
    "| **Virginia**   | 22,665 | 54     | 242       |\n",
    "| **Iowa**       | 22,610 | 38     | 280       |\n",
    "| **California** | 22,467 | 472    | 1,164     |\n",
    "\n",
    "These states are **diverse in region and risk type** ‚Äî from hurricanes (TX, VA) to tornadoes and blizzards (KS, IA), and wildfires (CA).\n",
    "\n",
    "---\n",
    "\n",
    "**Least 5 Regions**\n",
    "| Region            | Events | Deaths | Injuries |\n",
    "|-------------------|--------|--------|----------|\n",
    "| E PACIFIC         | 72     | 9      | 6        |\n",
    "| ST LAWRENCE R     | 17     | 0      | 0        |\n",
    "| HAWAII WATERS     | 14     | 1      | 1        |\n",
    "| GULF OF ALASKA    | 6      | 0      | 0        |\n",
    "| GUAM WATERS       | 1      | 0      | 0        |\n",
    "\n",
    "These are **offshore or territorial zones** and may not align well with outage records ‚Äî good candidates for exclusion or generalisation.\n",
    "\n",
    "---\n",
    "\n",
    "Now we've filtered **Top/Least 5 States** by:\n",
    "- `total_outages` and `total_customers_out` (from outage data)\n",
    "- `total_events` and `total_deaths/injuries` (from storm data)\n",
    "\n",
    "We now have a **good shortlist of 10‚Äì15 priority states** for analysis, modeling, and visual exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4837bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E PACIFIC', 'ST LAWRENCE R', 'HAWAII WATERS', 'GULF OF ALASKA', 'GUAM WATERS', 'TEXAS', 'KANSAS', 'VIRGINIA', 'IOWA', 'CALIFORNIA']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['E PACIFIC',\n",
       " 'ST LAWRENCE R',\n",
       " 'HAWAII WATERS',\n",
       " 'GULF OF ALASKA',\n",
       " 'GUAM WATERS',\n",
       " 'TEXAS',\n",
       " 'KANSAS',\n",
       " 'VIRGINIA',\n",
       " 'IOWA',\n",
       " 'CALIFORNIA']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_least_10_storms_states = least_5_storm_states.index.tolist() + top_5_storm_states.index.tolist()\n",
    "print(top_least_10_storms_states)\n",
    "\n",
    "['E PACIFIC', 'ST LAWRENCE R', 'HAWAII WATERS', 'GULF OF ALASKA', 'GUAM WATERS', 'TEXAS', 'KANSAS', 'VIRGINIA', 'IOWA', 'CALIFORNIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08694078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished filtering storm events. Shape: (145507, 51)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 145507 entries, 8 to 691428\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   BEGIN_YEARMONTH     145507 non-null  int64  \n",
      " 1   BEGIN_DAY           145507 non-null  int64  \n",
      " 2   BEGIN_TIME          145507 non-null  int64  \n",
      " 3   END_YEARMONTH       145507 non-null  int64  \n",
      " 4   END_DAY             145507 non-null  int64  \n",
      " 5   END_TIME            145507 non-null  int64  \n",
      " 6   EPISODE_ID          145507 non-null  int64  \n",
      " 7   EVENT_ID            145507 non-null  int64  \n",
      " 8   STATE               145507 non-null  object \n",
      " 9   STATE_FIPS          145507 non-null  int64  \n",
      " 10  YEAR                145507 non-null  int64  \n",
      " 11  MONTH_NAME          145507 non-null  object \n",
      " 12  EVENT_TYPE          145507 non-null  object \n",
      " 13  CZ_TYPE             145507 non-null  object \n",
      " 14  CZ_FIPS             145507 non-null  int64  \n",
      " 15  CZ_NAME             145507 non-null  object \n",
      " 16  WFO                 145507 non-null  object \n",
      " 17  BEGIN_DATE_TIME     145507 non-null  object \n",
      " 18  CZ_TIMEZONE         145507 non-null  object \n",
      " 19  END_DATE_TIME       145507 non-null  object \n",
      " 20  INJURIES_DIRECT     145507 non-null  int64  \n",
      " 21  INJURIES_INDIRECT   145507 non-null  int64  \n",
      " 22  DEATHS_DIRECT       145507 non-null  int64  \n",
      " 23  DEATHS_INDIRECT     145507 non-null  int64  \n",
      " 24  DAMAGE_PROPERTY     117910 non-null  object \n",
      " 25  DAMAGE_CROPS        115484 non-null  object \n",
      " 26  SOURCE              145507 non-null  object \n",
      " 27  MAGNITUDE           70363 non-null   float64\n",
      " 28  MAGNITUDE_TYPE      42250 non-null   object \n",
      " 29  FLOOD_CAUSE         17838 non-null   object \n",
      " 30  CATEGORY            27 non-null      float64\n",
      " 31  TOR_F_SCALE         3319 non-null    object \n",
      " 32  TOR_LENGTH          3319 non-null    float64\n",
      " 33  TOR_WIDTH           3319 non-null    float64\n",
      " 34  TOR_OTHER_WFO       387 non-null     object \n",
      " 35  TOR_OTHER_CZ_STATE  387 non-null     object \n",
      " 36  TOR_OTHER_CZ_FIPS   387 non-null     float64\n",
      " 37  TOR_OTHER_CZ_NAME   387 non-null     object \n",
      " 38  BEGIN_RANGE         89029 non-null   float64\n",
      " 39  BEGIN_AZIMUTH       89029 non-null   object \n",
      " 40  BEGIN_LOCATION      89029 non-null   object \n",
      " 41  END_RANGE           89029 non-null   float64\n",
      " 42  END_AZIMUTH         89029 non-null   object \n",
      " 43  END_LOCATION        89029 non-null   object \n",
      " 44  BEGIN_LAT           89029 non-null   float64\n",
      " 45  BEGIN_LON           89029 non-null   float64\n",
      " 46  END_LAT             89029 non-null   float64\n",
      " 47  END_LON             89029 non-null   float64\n",
      " 48  EPISODE_NARRATIVE   145507 non-null  object \n",
      " 49  EVENT_NARRATIVE     118962 non-null  object \n",
      " 50  DATA_SOURCE         145507 non-null  object \n",
      "dtypes: float64(11), int64(15), object(25)\n",
      "memory usage: 57.7+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>201403</td>\n",
       "      <td>16</td>\n",
       "      <td>1400</td>\n",
       "      <td>201403</td>\n",
       "      <td>17</td>\n",
       "      <td>1200</td>\n",
       "      <td>83806</td>\n",
       "      <td>507869</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low pressure ejecting out of the Southern Plai...</td>\n",
       "      <td>Various sources reported snowfall totals rangi...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>201403</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>201403</td>\n",
       "      <td>13</td>\n",
       "      <td>1100</td>\n",
       "      <td>84127</td>\n",
       "      <td>507867</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Northwest winds behind a departing cold front,...</td>\n",
       "      <td>Several minor accidents occurred in Tazewell c...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>201405</td>\n",
       "      <td>11</td>\n",
       "      <td>2230</td>\n",
       "      <td>201405</td>\n",
       "      <td>11</td>\n",
       "      <td>2230</td>\n",
       "      <td>83868</td>\n",
       "      <td>506362</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>LAKEVIEW</td>\n",
       "      <td>34.6990</td>\n",
       "      <td>-100.7000</td>\n",
       "      <td>34.6990</td>\n",
       "      <td>-100.7000</td>\n",
       "      <td>A line of thunderstorm activity blossomed acro...</td>\n",
       "      <td>Numerous large tree limbs were blown down nort...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>201403</td>\n",
       "      <td>25</td>\n",
       "      <td>400</td>\n",
       "      <td>201403</td>\n",
       "      <td>25</td>\n",
       "      <td>1800</td>\n",
       "      <td>83807</td>\n",
       "      <td>505969</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Despite a surface coastal storm that was well ...</td>\n",
       "      <td>Snowfall reports ranged from 3 to 4 inches acr...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>201403</td>\n",
       "      <td>25</td>\n",
       "      <td>400</td>\n",
       "      <td>201403</td>\n",
       "      <td>25</td>\n",
       "      <td>1800</td>\n",
       "      <td>83807</td>\n",
       "      <td>505970</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Despite a surface coastal storm that was well ...</td>\n",
       "      <td>Snowfall reports ranged from less than 1 up to...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691413</th>\n",
       "      <td>202406</td>\n",
       "      <td>4</td>\n",
       "      <td>1600</td>\n",
       "      <td>202406</td>\n",
       "      <td>4</td>\n",
       "      <td>1601</td>\n",
       "      <td>192700</td>\n",
       "      <td>1189543</td>\n",
       "      <td>KANSAS</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>INMAN</td>\n",
       "      <td>38.2900</td>\n",
       "      <td>-97.7400</td>\n",
       "      <td>38.2900</td>\n",
       "      <td>-97.7400</td>\n",
       "      <td>A weak cold front pushed south across the area...</td>\n",
       "      <td>Broadcast media (KAKE TV) meteorologist report...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691418</th>\n",
       "      <td>202406</td>\n",
       "      <td>7</td>\n",
       "      <td>1745</td>\n",
       "      <td>202406</td>\n",
       "      <td>7</td>\n",
       "      <td>1805</td>\n",
       "      <td>192915</td>\n",
       "      <td>1190968</td>\n",
       "      <td>KANSAS</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>HOXIE</td>\n",
       "      <td>39.3645</td>\n",
       "      <td>-100.4708</td>\n",
       "      <td>39.3645</td>\n",
       "      <td>-100.4708</td>\n",
       "      <td>During the afternoon to early evening hours a ...</td>\n",
       "      <td>Hail ranging in size from dime to golf ball fe...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691419</th>\n",
       "      <td>202406</td>\n",
       "      <td>7</td>\n",
       "      <td>1635</td>\n",
       "      <td>202406</td>\n",
       "      <td>7</td>\n",
       "      <td>1635</td>\n",
       "      <td>192915</td>\n",
       "      <td>1190967</td>\n",
       "      <td>KANSAS</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>KANORADO</td>\n",
       "      <td>39.3330</td>\n",
       "      <td>-102.0335</td>\n",
       "      <td>39.3330</td>\n",
       "      <td>-102.0335</td>\n",
       "      <td>During the afternoon to early evening hours a ...</td>\n",
       "      <td>Pea to quarter sized hail reported.</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691420</th>\n",
       "      <td>202406</td>\n",
       "      <td>7</td>\n",
       "      <td>1730</td>\n",
       "      <td>202406</td>\n",
       "      <td>7</td>\n",
       "      <td>1730</td>\n",
       "      <td>192915</td>\n",
       "      <td>1190966</td>\n",
       "      <td>KANSAS</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>SEGUIN</td>\n",
       "      <td>39.3576</td>\n",
       "      <td>-100.6091</td>\n",
       "      <td>39.3576</td>\n",
       "      <td>-100.6091</td>\n",
       "      <td>During the afternoon to early evening hours a ...</td>\n",
       "      <td>Ping pong ball to golf ball sized hail fell ov...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691428</th>\n",
       "      <td>202405</td>\n",
       "      <td>9</td>\n",
       "      <td>1253</td>\n",
       "      <td>202405</td>\n",
       "      <td>9</td>\n",
       "      <td>1853</td>\n",
       "      <td>189658</td>\n",
       "      <td>1166452</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An unseasonably strong sub-tropical ridge/heat...</td>\n",
       "      <td>McAllen International Airport (KMFE) recorded ...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145507 rows √ó 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  \\\n",
       "8                201403         16        1400         201403       17   \n",
       "9                201403         13           0         201403       13   \n",
       "10               201405         11        2230         201405       11   \n",
       "16               201403         25         400         201403       25   \n",
       "17               201403         25         400         201403       25   \n",
       "...                 ...        ...         ...            ...      ...   \n",
       "691413           202406          4        1600         202406        4   \n",
       "691418           202406          7        1745         202406        7   \n",
       "691419           202406          7        1635         202406        7   \n",
       "691420           202406          7        1730         202406        7   \n",
       "691428           202405          9        1253         202405        9   \n",
       "\n",
       "        END_TIME  EPISODE_ID  EVENT_ID     STATE  STATE_FIPS  ...  END_RANGE  \\\n",
       "8           1200       83806    507869  VIRGINIA          51  ...        NaN   \n",
       "9           1100       84127    507867  VIRGINIA          51  ...        NaN   \n",
       "10          2230       83868    506362     TEXAS          48  ...        2.0   \n",
       "16          1800       83807    505969  VIRGINIA          51  ...        NaN   \n",
       "17          1800       83807    505970  VIRGINIA          51  ...        NaN   \n",
       "...          ...         ...       ...       ...         ...  ...        ...   \n",
       "691413      1601      192700   1189543    KANSAS          20  ...        4.0   \n",
       "691418      1805      192915   1190968    KANSAS          20  ...        2.0   \n",
       "691419      1635      192915   1190967    KANSAS          20  ...        0.0   \n",
       "691420      1730      192915   1190966    KANSAS          20  ...        2.0   \n",
       "691428      1853      189658   1166452     TEXAS          48  ...        NaN   \n",
       "\n",
       "       END_AZIMUTH END_LOCATION BEGIN_LAT  BEGIN_LON  END_LAT   END_LON  \\\n",
       "8              NaN          NaN       NaN        NaN      NaN       NaN   \n",
       "9              NaN          NaN       NaN        NaN      NaN       NaN   \n",
       "10               N     LAKEVIEW   34.6990  -100.7000  34.6990 -100.7000   \n",
       "16             NaN          NaN       NaN        NaN      NaN       NaN   \n",
       "17             NaN          NaN       NaN        NaN      NaN       NaN   \n",
       "...            ...          ...       ...        ...      ...       ...   \n",
       "691413         NNE        INMAN   38.2900   -97.7400  38.2900  -97.7400   \n",
       "691418         WNW        HOXIE   39.3645  -100.4708  39.3645 -100.4708   \n",
       "691419          NW     KANORADO   39.3330  -102.0335  39.3330 -102.0335   \n",
       "691420          NW       SEGUIN   39.3576  -100.6091  39.3576 -100.6091   \n",
       "691428         NaN          NaN       NaN        NaN      NaN       NaN   \n",
       "\n",
       "                                        EPISODE_NARRATIVE  \\\n",
       "8       Low pressure ejecting out of the Southern Plai...   \n",
       "9       Northwest winds behind a departing cold front,...   \n",
       "10      A line of thunderstorm activity blossomed acro...   \n",
       "16      Despite a surface coastal storm that was well ...   \n",
       "17      Despite a surface coastal storm that was well ...   \n",
       "...                                                   ...   \n",
       "691413  A weak cold front pushed south across the area...   \n",
       "691418  During the afternoon to early evening hours a ...   \n",
       "691419  During the afternoon to early evening hours a ...   \n",
       "691420  During the afternoon to early evening hours a ...   \n",
       "691428  An unseasonably strong sub-tropical ridge/heat...   \n",
       "\n",
       "                                          EVENT_NARRATIVE DATA_SOURCE  \n",
       "8       Various sources reported snowfall totals rangi...         CSV  \n",
       "9       Several minor accidents occurred in Tazewell c...         CSV  \n",
       "10      Numerous large tree limbs were blown down nort...         CSV  \n",
       "16      Snowfall reports ranged from 3 to 4 inches acr...         CSV  \n",
       "17      Snowfall reports ranged from less than 1 up to...         CSV  \n",
       "...                                                   ...         ...  \n",
       "691413  Broadcast media (KAKE TV) meteorologist report...         CSV  \n",
       "691418  Hail ranging in size from dime to golf ball fe...         CSV  \n",
       "691419                Pea to quarter sized hail reported.         CSV  \n",
       "691420  Ping pong ball to golf ball sized hail fell ov...         CSV  \n",
       "691428  McAllen International Airport (KMFE) recorded ...         CSV  \n",
       "\n",
       "[145507 rows x 51 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read the storm events CSV\n",
    "storm_events = pd.read_csv(storm_events_path)\n",
    "\n",
    "# Filter for the selected states\n",
    "top_least_10_storms_states_df = storm_events[storm_events['STATE'].isin(top_least_10_storms_states)]\n",
    "\n",
    "# Show a summary of the filtered data\n",
    "print(f\"Finished filtering storm events. Shape: {top_least_10_storms_states_df.shape}\")\n",
    "print(top_least_10_storms_states_df.info())\n",
    "\n",
    "top_least_10_storms_states_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9210a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_state = 'TEXAS'\n",
    "\n",
    "# # Path to the storm events CSV\n",
    "# storm_events_path = '../dynamic_rhythm_train_data/NOAA_StormEvents/StormEvents_2014_2024.csv'\n",
    "\n",
    "# # Read storm events\n",
    "# storm_events = pd.read_csv(storm_events_path)\n",
    "\n",
    "# # Filter for the same state\n",
    "# storm_events_14_24_df = storm_events[storm_events['STATE'] == target_state.upper()]\n",
    "\n",
    "# print(f\"Finished reading storm events. Shape: {storm_events_14_24_df.shape}\")\n",
    "\n",
    "# storm_events_14_24_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de497c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>...</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>201403</td>\n",
       "      <td>16</td>\n",
       "      <td>1400</td>\n",
       "      <td>201403</td>\n",
       "      <td>17</td>\n",
       "      <td>1200</td>\n",
       "      <td>83806</td>\n",
       "      <td>507869</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Low pressure ejecting out of the Southern Plai...</td>\n",
       "      <td>Various sources reported snowfall totals rangi...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>201403</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>201403</td>\n",
       "      <td>13</td>\n",
       "      <td>1100</td>\n",
       "      <td>84127</td>\n",
       "      <td>507867</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Northwest winds behind a departing cold front,...</td>\n",
       "      <td>Several minor accidents occurred in Tazewell c...</td>\n",
       "      <td>CSV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
       "8           201403         16        1400         201403       17      1200   \n",
       "9           201403         13           0         201403       13      1100   \n",
       "\n",
       "   EPISODE_ID  EVENT_ID     STATE  STATE_FIPS  ...  END_RANGE END_AZIMUTH  \\\n",
       "8       83806    507869  VIRGINIA          51  ...        NaN         NaN   \n",
       "9       84127    507867  VIRGINIA          51  ...        NaN         NaN   \n",
       "\n",
       "  END_LOCATION BEGIN_LAT  BEGIN_LON END_LAT END_LON  \\\n",
       "8          NaN       NaN        NaN     NaN     NaN   \n",
       "9          NaN       NaN        NaN     NaN     NaN   \n",
       "\n",
       "                                   EPISODE_NARRATIVE  \\\n",
       "8  Low pressure ejecting out of the Southern Plai...   \n",
       "9  Northwest winds behind a departing cold front,...   \n",
       "\n",
       "                                     EVENT_NARRATIVE DATA_SOURCE  \n",
       "8  Various sources reported snowfall totals rangi...         CSV  \n",
       "9  Several minor accidents occurred in Tazewell c...         CSV  \n",
       "\n",
       "[2 rows x 51 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_least_10_storms_states_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9c72c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips_code</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>customers_out</th>\n",
       "      <th>outages_start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6029</td>\n",
       "      <td>Kern</td>\n",
       "      <td>California</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6037</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>California</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fips_code       county       state  customers_out  outages_start_time\n",
       "0       6029         Kern  California           30.0 2014-11-01 04:00:00\n",
       "1       6037  Los Angeles  California         1555.0 2014-11-01 04:00:00"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_least_10_states_outages_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44521372",
   "metadata": {},
   "source": [
    "- keeping state selection consistent across both datasets by basing it entirely on the power outages data ensures clean joins, aligned time series, and avoids inconsistencies caused by extra regions or naming issues in the storms data.\n",
    "\n",
    "- let's use the outages states to filter the storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8db6429a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Florida', 'California', 'Texas', 'Louisiana', 'Michigan', 'Montana', 'South Dakota', 'District of Columbia', 'North Dakota', 'Wyoming']\n"
     ]
    }
   ],
   "source": [
    "# print(storm_events['STATE'].unique())\n",
    "# print(outages_selected_df['state'].nunique())\n",
    "print(top_least_10_outages_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdb3f36",
   "metadata": {},
   "source": [
    "- 'District Of Columbia' instead of 'District of Columbia' in storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storm_events['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede5c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_events['STATE'] = storm_events['STATE'].str.title()\n",
    "\n",
    "storm_events['STATE'] = storm_events['STATE'].replace({\n",
    "    'District Of Columbia':'District of Columbia'\n",
    "})\n",
    "\n",
    "# storm_events['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40fd6687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'South Dakota', 'Texas', 'Michigan', 'California', 'Louisiana', 'Montana', 'North Dakota', 'Wyoming', 'District of Columbia', 'Florida'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Now filter storm data to match only outages-based states\n",
    "storm_from_outages_states_df = storm_events[storm_events['STATE'].isin(top_least_10_outages_states)]\n",
    "\n",
    "# print(top_least_10_outages_states)\n",
    "# storm_from_outages_states_df['STATE'].unique()\n",
    "\n",
    "intersectings_states = set(storm_from_outages_states_df['STATE'].unique()) & set(top_least_10_outages_states)\n",
    "print(intersectings_states)\n",
    "\n",
    "# storm_from_outages_states_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c10962df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Texas', 'California', 'Wyoming', 'Louisiana', 'Montana',\n",
       "       'South Dakota', 'North Dakota', 'Florida', 'Michigan',\n",
       "       'District of Columbia'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storm_from_outages_states_df['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storm_from_outages_states_df['BEGIN_DATE_TIME'].unique()\n",
    "\n",
    "# top_least_10_outages_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57148832",
   "metadata": {},
   "source": [
    "Why only 7 states were initially fetched instead of the full 10:\n",
    "\n",
    "---\n",
    "\n",
    "Our `top_least_10_outages_states` includes:\n",
    "- `'District of Columbia'`\n",
    "- `'US Virgin Islands'`\n",
    "- `'United States Virgin Islands'`\n",
    "\n",
    "---\n",
    "\n",
    "In `storm_events['STATE'].unique()`:\n",
    "- `'District Of Columbia'` Match (but with title case difference)\n",
    "- `'Virgin Islands'` Exists, but your dataset has inconsistent naming (`US Virgin Islands`, `United States Virgin Islands`)\n",
    "\n",
    "---\n",
    "\n",
    "**Why 3 states didn‚Äôt match:**\n",
    "1. `'District of Columbia'` ‚â† `'District Of Columbia'` ‚Üí fix with consistent title-casing.\n",
    "2. `'United States Virgin Islands'` ‚â† `'Virgin Islands'`\n",
    "3. `'US Virgin Islands'` ‚â† `'Virgin Islands'`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f16099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storm_from_outages_states_df['DEATHS_INDIRECT'].unique()\n",
    "# top_least_10_states_outages_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4c046de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storm_from_outages_states_df['EVENT_TYPE'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02f4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d28c272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/x4myp3c97fx7r5kdg1c9gzmc0000gn/T/ipykernel_1724/1989978007.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  storm_df['BEGIN_DATETIME'] = pd.to_datetime(storm_df['BEGIN_DATETIME'])\n",
      "/var/folders/fz/x4myp3c97fx7r5kdg1c9gzmc0000gn/T/ipykernel_1724/1989978007.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  storm_df['event_day'] = storm_df['BEGIN_DATETIME'].dt.date\n",
      "/var/folders/fz/x4myp3c97fx7r5kdg1c9gzmc0000gn/T/ipykernel_1724/1989978007.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  storm_df['DAMAGE_PROPERTY'] = storm_df['DAMAGE_PROPERTY'].apply(parse_damage)\n",
      "/var/folders/fz/x4myp3c97fx7r5kdg1c9gzmc0000gn/T/ipykernel_1724/1989978007.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  storm_df['DAMAGE_CROPS'] = storm_df['DAMAGE_CROPS'].apply(parse_damage)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>event_day</th>\n",
       "      <th>storm_event_count</th>\n",
       "      <th>deaths_total</th>\n",
       "      <th>deaths_indirect</th>\n",
       "      <th>injuries_indirect</th>\n",
       "      <th>injuries_direct</th>\n",
       "      <th>total_damage_property</th>\n",
       "      <th>total_damage_crops</th>\n",
       "      <th>num_astronomical_low_tide_events</th>\n",
       "      <th>...</th>\n",
       "      <th>dayofweek_sin</th>\n",
       "      <th>dayofweek_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>lag_1_storms</th>\n",
       "      <th>rolling_3d_storms</th>\n",
       "      <th>lag_1_outages</th>\n",
       "      <th>rolling_3d_outages</th>\n",
       "      <th>outage_occurred</th>\n",
       "      <th>outage_severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>California</td>\n",
       "      <td>2014-11-01</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California</td>\n",
       "      <td>2014-11-03</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>738.0</td>\n",
       "      <td>762.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>California</td>\n",
       "      <td>2014-11-04</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>787.0</td>\n",
       "      <td>744.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>2014-11-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>709.0</td>\n",
       "      <td>761.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>2014-11-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>789.0</td>\n",
       "      <td>715.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        STATE  event_day  storm_event_count  deaths_total  deaths_indirect  \\\n",
       "0  California 2014-11-01                 18             0                0   \n",
       "1  California 2014-11-03                  2             0                0   \n",
       "2  California 2014-11-04                  2             0                0   \n",
       "3  California 2014-11-07                  5             0                0   \n",
       "4  California 2014-11-08                  1             0                0   \n",
       "\n",
       "   injuries_indirect  injuries_direct  total_damage_property  \\\n",
       "0                  0                0                    0.0   \n",
       "1                  0                0                    0.0   \n",
       "2                  0                0                    0.0   \n",
       "3                  0                0                    0.0   \n",
       "4                  0                0                    0.0   \n",
       "\n",
       "   total_damage_crops  num_astronomical_low_tide_events  ...  dayofweek_sin  \\\n",
       "0                 0.0                                 0  ...      -0.974928   \n",
       "1                 0.0                                 0  ...       0.000000   \n",
       "2                 0.0                                 0  ...       0.781831   \n",
       "3                 0.0                                 0  ...      -0.433884   \n",
       "4                 0.0                                 0  ...      -0.974928   \n",
       "\n",
       "   dayofweek_cos  month_sin  month_cos  lag_1_storms  rolling_3d_storms  \\\n",
       "0      -0.222521       -0.5   0.866025           NaN          18.000000   \n",
       "1       1.000000       -0.5   0.866025          18.0          10.000000   \n",
       "2       0.623490       -0.5   0.866025           2.0           7.333333   \n",
       "3      -0.900969       -0.5   0.866025           2.0           3.000000   \n",
       "4      -0.222521       -0.5   0.866025           5.0           2.666667   \n",
       "\n",
       "   lag_1_outages  rolling_3d_outages  outage_occurred  outage_severity  \n",
       "0            NaN          738.000000                1             High  \n",
       "1          738.0          762.500000                1             High  \n",
       "2          787.0          744.666667                1             High  \n",
       "3          709.0          761.666667                1             High  \n",
       "4          789.0          715.666667                1             High  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_damage(damage_str):\n",
    "    if pd.isnull(damage_str):\n",
    "        return 0\n",
    "    if isinstance(damage_str, (int, float)):\n",
    "        return damage_str\n",
    "    damage_str = damage_str.strip()\n",
    "    multiplier = 1\n",
    "    if damage_str.endswith('K'):\n",
    "        multiplier = 1_000\n",
    "        damage_str = damage_str[:-1]\n",
    "    elif damage_str.endswith('M'):\n",
    "        multiplier = 1_000_000\n",
    "        damage_str = damage_str[:-1]\n",
    "    elif damage_str.endswith('B'):\n",
    "        multiplier = 1_000_000_000\n",
    "        damage_str = damage_str[:-1]\n",
    "    try:\n",
    "        return float(damage_str) * multiplier\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def prepare_daily_storm_outage_features(storm_df, outage_df):\n",
    "    # Ensure datetime fields are parsed\n",
    "    storm_df['BEGIN_DATETIME'] = pd.to_datetime(storm_df['BEGIN_DATETIME'])\n",
    "    outage_df['outages_start_time'] = pd.to_datetime(outage_df['outages_start_time'])\n",
    "\n",
    "    # Add event_day\n",
    "    storm_df['event_day'] = storm_df['BEGIN_DATETIME'].dt.date\n",
    "    outage_df['event_day'] = outage_df['outages_start_time'].dt.date\n",
    "\n",
    "    storm_df['DAMAGE_PROPERTY'] = storm_df['DAMAGE_PROPERTY'].apply(parse_damage)\n",
    "    storm_df['DAMAGE_CROPS'] = storm_df['DAMAGE_CROPS'].apply(parse_damage)\n",
    "\n",
    "    # Count of each event type per state and day\n",
    "    event_type_counts = (\n",
    "        storm_df.groupby(['STATE', 'event_day', 'EVENT_TYPE'])\n",
    "        .size()\n",
    "        .reset_index(name='event_count')\n",
    "        .pivot(index=['STATE', 'event_day'], columns='EVENT_TYPE', values='event_count')\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    # Rename columns\n",
    "    event_type_counts.columns = [\n",
    "        f\"num_{col.lower().replace(' ', '_').replace('/', '_').replace('(', '').replace(')', '').replace('-', '').replace('__', '_')}_events\"\n",
    "        for col in event_type_counts.columns\n",
    "    ]\n",
    "    event_type_counts = event_type_counts.reset_index()\n",
    "\n",
    "    # Basic storm aggregation\n",
    "    storm_daily_df = (\n",
    "        storm_df\n",
    "        .groupby(['STATE', 'event_day'])\n",
    "        .agg(\n",
    "            storm_event_count=('EVENT_ID', 'count'),\n",
    "            deaths_total=('DEATHS_DIRECT', 'sum'),\n",
    "            deaths_indirect=('DEATHS_INDIRECT', 'sum'),\n",
    "            injuries_indirect=('INJURIES_INDIRECT', 'sum'),\n",
    "            injuries_direct=('DEATHS_DIRECT', 'sum'),\n",
    "            total_damage_property=('DAMAGE_PROPERTY','sum'),\n",
    "            total_damage_crops=('DAMAGE_CROPS','sum')\n",
    "            # total_damage_property=('DAMAGE_PROPERTY', lambda x: x.dropna().count())\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Merge event type features\n",
    "    storm_daily_df = storm_daily_df.merge(event_type_counts, on=['STATE', 'event_day'], how='left')\n",
    "\n",
    "    # Daily outage aggregation\n",
    "    outage_daily_df = (\n",
    "        outage_df\n",
    "        .groupby(['state', 'event_day'])\n",
    "        .agg(\n",
    "            total_outages=('fips_code', 'count'),\n",
    "            total_customers_out=('customers_out', 'sum'),\n",
    "            avg_customers_out=('customers_out', 'mean')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    outage_daily_df['state'] = outage_daily_df['state'].str.title()\n",
    "\n",
    "    # Merge on state and day\n",
    "    merged_df = pd.merge(\n",
    "        storm_daily_df,\n",
    "        outage_daily_df,\n",
    "        left_on=['STATE', 'event_day'],\n",
    "        right_on=['state', 'event_day'],\n",
    "        how='inner'\n",
    "    )\n",
    "\n",
    "    # Convert event_day to datetime\n",
    "    merged_df['event_day'] = pd.to_datetime(merged_df['event_day'])\n",
    "\n",
    "    # Add time features\n",
    "    merged_df['year'] = merged_df['event_day'].dt.year\n",
    "    merged_df['month'] = merged_df['event_day'].dt.month\n",
    "    merged_df['day'] = merged_df['event_day'].dt.day\n",
    "    merged_df['dayofweek'] = merged_df['event_day'].dt.dayofweek\n",
    "    merged_df['week'] = merged_df['event_day'].dt.isocalendar().week\n",
    "    merged_df['quarter'] = merged_df['event_day'].dt.quarter\n",
    "\n",
    "    # Cyclical encodings\n",
    "    merged_df['dayofweek_sin'] = np.sin(2 * np.pi * merged_df['dayofweek'] / 7)\n",
    "    merged_df['dayofweek_cos'] = np.cos(2 * np.pi * merged_df['dayofweek'] / 7)\n",
    "    merged_df['month_sin'] = np.sin(2 * np.pi * merged_df['month'] / 12)\n",
    "    merged_df['month_cos'] = np.cos(2 * np.pi * merged_df['month'] / 12)\n",
    "\n",
    "    # Sort for lag features\n",
    "    merged_df = merged_df.sort_values(['STATE', 'event_day'])\n",
    "\n",
    "    # Lag and rolling features\n",
    "    merged_df['lag_1_storms'] = merged_df.groupby('STATE')['storm_event_count'].shift(1)\n",
    "    merged_df['rolling_3d_storms'] = (\n",
    "        merged_df.groupby('STATE')['storm_event_count']\n",
    "        .rolling(3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    merged_df['lag_1_outages'] = merged_df.groupby('STATE')['total_outages'].shift(1)\n",
    "    merged_df['rolling_3d_outages'] = (\n",
    "        merged_df.groupby('STATE')['total_outages']\n",
    "        .rolling(3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Binary target\n",
    "    merged_df['outage_occurred'] = (merged_df['total_outages'] > 0).astype(int)\n",
    "\n",
    "    # Outage severity categorization\n",
    "    def categorize_severity(x):\n",
    "        if x == 0:\n",
    "            return 'None'\n",
    "        elif x < 1000:\n",
    "            return 'Low'\n",
    "        elif x < 10000:\n",
    "            return 'Medium'\n",
    "        else:\n",
    "            return 'High'\n",
    "\n",
    "    merged_df['outage_severity'] = merged_df['total_customers_out'].apply(categorize_severity)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Call the function on your datasets\n",
    "merged_daily_df = prepare_daily_storm_outage_features(\n",
    "    storm_df=storm_from_outages_states_df,\n",
    "    outage_df=top_least_10_states_outages_df\n",
    ")\n",
    "\n",
    "merged_daily_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68141b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['California', 'Florida', 'Louisiana', 'Michigan', 'Montana',\n",
       "       'North Dakota', 'South Dakota', 'Texas', 'Wyoming'], dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merged_daily_df['total_damage_property'].unique()\n",
    "merged_daily_df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1451892b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52505c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0913e374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99edfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_daily_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034ab3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fefe6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20c5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b94d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_ts_states_outages(states,\n",
    "                    start_year,\n",
    "                    start_month,\n",
    "                    start_day,\n",
    "                    end_year,\n",
    "                    end_month,\n",
    "                    end_day,\n",
    "                    df_power):\n",
    "    \"\"\"\n",
    "    Create a time series DataFrame for power outages for multiple states within a date range.\n",
    "\n",
    "    Parameters:\n",
    "    - states (list of str): List of state names to filter (case-sensitive).\n",
    "    - start_year, start_month, start_day, end_year, end_month, end_day (int): Date range.\n",
    "    - df_power (DataFrame): Pre-loaded and pre-filtered outage data.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame indexed by time with 'customers_out' and 'state' fields.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure datetime format\n",
    "    df_power['outages_start_time'] = pd.to_datetime(df_power['outages_start_time'])\n",
    "\n",
    "    # Filter by date range\n",
    "    start_date = pd.Timestamp(year=start_year, month=start_month, day=start_day)\n",
    "    end_date = pd.Timestamp(year=end_year, month=end_month, day=end_day)\n",
    "\n",
    "    df_power_filtered = df_power[\n",
    "        (df_power['outages_start_time'] >= start_date) & \n",
    "        (df_power['outages_start_time'] <= end_date) &\n",
    "        (df_power['state'].isin(states))\n",
    "    ].copy()\n",
    "\n",
    "    # Create time series DataFrame\n",
    "    df_power_filtered['time'] = df_power_filtered['outages_start_time']\n",
    "    df_power_ts = df_power_filtered[['time', 'state', 'customers_out']]\n",
    "\n",
    "    return df_power_ts\n",
    "\n",
    "\n",
    "def make_ts_states_storms(states, event_types, start_year, start_month, start_day, end_year, end_month, end_day, df):\n",
    "    \"\"\"\n",
    "    Construct a DataFrame with 15-minute intervals indicating event occurrence for multiple states.\n",
    "\n",
    "    Parameters:\n",
    "    - states (list of str): List of states to filter (e.g., [\"Texas\", \"California\"]).\n",
    "    - event_types (list): The event types to filter (e.g., [\"Winter Storm\", \"Hurricane\"]).\n",
    "    - start_year, start_month, start_day, end_year, end_month, end_day (int): Date range.\n",
    "    - df (pd.DataFrame): The NOAA StormEvent database.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with 15-minute intervals, event counts, and state label.\n",
    "    \"\"\"\n",
    "\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Generate the time range for the new DataFrame\n",
    "    start_date = datetime(start_year, start_month, start_day)\n",
    "    end_date = datetime(end_year, end_month, end_day, 23, 45)\n",
    "    time_index = pd.date_range(start=start_date, end=end_date, freq='15min')\n",
    "\n",
    "    # Convert BEGIN and END times into datetime objects\n",
    "    df['BEGIN_DATETIME'] = pd.to_datetime(\n",
    "        df['BEGIN_YEARMONTH'].astype(str) + df['BEGIN_DAY'].astype(str).str.zfill(2) +\n",
    "        df['BEGIN_TIME'].astype(str).str.zfill(4), format='%Y%m%d%H%M'\n",
    "    )\n",
    "    df['END_DATETIME'] = pd.to_datetime(\n",
    "        df['END_YEARMONTH'].astype(str) + df['END_DAY'].astype(str).str.zfill(2) +\n",
    "        df['END_TIME'].astype(str).str.zfill(4), format='%Y%m%d%H%M'\n",
    "    )\n",
    "\n",
    "    all_states_df = []\n",
    "\n",
    "    for state in states:\n",
    "        # Create state-specific frame\n",
    "        new_df = pd.DataFrame({'time': time_index})\n",
    "        for event_type in event_types:\n",
    "            new_df[f'event_count {event_type}'] = 0\n",
    "\n",
    "        # Filter for this state and event types\n",
    "        filtered_df = df[\n",
    "            (df['STATE'] == state) &\n",
    "            (df['EVENT_TYPE'].isin(event_types)) &\n",
    "            (df['END_DATETIME'] >= start_date) &\n",
    "            (df['BEGIN_DATETIME'] <= end_date)\n",
    "        ].copy()\n",
    "\n",
    "        # Iterate through events to populate counts\n",
    "        for event_type in event_types:\n",
    "            event_subset = filtered_df[filtered_df['EVENT_TYPE'] == event_type]\n",
    "\n",
    "            for _, row in event_subset.iterrows():\n",
    "                event_start = row['BEGIN_DATETIME'].round('15min')\n",
    "                event_end = row['END_DATETIME'].round('15min')\n",
    "                start_idx = new_df['time'].searchsorted(event_start)\n",
    "                end_idx = new_df['time'].searchsorted(event_end)\n",
    "\n",
    "                if start_idx < len(new_df) and end_idx <= len(new_df):\n",
    "                    new_df.loc[start_idx:end_idx, f'event_count {event_type}'] += 1\n",
    "\n",
    "        new_df['STATE'] = state\n",
    "        all_states_df.append(new_df)\n",
    "\n",
    "    # Combine all states\n",
    "    combined_df = pd.concat(all_states_df, ignore_index=True)\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4791b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2014-11-01 04:00:00'), Timestamp('2023-12-31 23:45:00'))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_least_10_states_outages_df['outages_start_time'].min(), top_least_10_states_outages_df['outages_start_time'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68fdd902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of storms types in the top 5 and least 5 states: 49 \n",
      "\n",
      "['Winter Storm' 'Winter Weather' 'Thunderstorm Wind' 'Heavy Snow'\n",
      " 'High Wind' 'Hail' 'Drought' 'Tornado' 'Dust Storm' 'Flash Flood' 'Flood'\n",
      " 'Blizzard' 'Cold/Wind Chill' 'Freezing Fog' 'Rip Current' 'Strong Wind'\n",
      " 'Dense Fog' 'Sleet' 'Debris Flow' 'High Surf' 'Frost/Freeze' 'Lightning'\n",
      " 'Coastal Flood' 'Dust Devil' 'Wildfire' 'Heavy Rain' 'Funnel Cloud'\n",
      " 'Waterspout' 'Heat' 'Extreme Cold/Wind Chill' 'Ice Storm'\n",
      " 'Tropical Storm' 'Astronomical Low Tide' 'Marine Strong Wind'\n",
      " 'Sneakerwave' 'Seiche' 'Excessive Heat' 'Marine High Wind' 'Tsunami'\n",
      " 'Tropical Depression' 'Storm Surge/Tide' 'Avalanche' 'Marine Dense Fog'\n",
      " 'Marine Thunderstorm Wind' 'Hurricane' 'Marine Hail' 'Dense Smoke'\n",
      " 'Lake-Effect Snow' 'Marine Tropical Depression']\n"
     ]
    }
   ],
   "source": [
    "top_least_10_storm_types = top_least_10_storms_states_df['EVENT_TYPE'].unique()\n",
    "print(\"number of storms types in the top 5 and least 5 states:\", top_least_10_storms_states_df['EVENT_TYPE'].nunique(), \"\\n\")\n",
    "print(top_least_10_storm_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6023cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outages_ts_df = make_ts_states_outages(\n",
    "    states=top_least_10_outages_states,\n",
    "    start_year=2014, start_month=1, start_day=1,\n",
    "    end_year=2023, end_month=12, end_day=31,\n",
    "    df_power=top_least_10_states_outages_df\n",
    ")\n",
    "\n",
    "# ts_outages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ccf1cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50986284 entries, 0 to 50986283\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   time           datetime64[ns]\n",
      " 1   state          object        \n",
      " 2   customers_out  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(1)\n",
      "memory usage: 1.5+ GB\n"
     ]
    }
   ],
   "source": [
    "# outages_ts_df['state'].unique()\n",
    "outages_ts_df.info()\n",
    "\n",
    "# top_least_10_storms_states\n",
    "# len(top_least_10_storm_types)\n",
    "# top_least_10_storms_states_df['EVENT_TYPE'].nunique()\n",
    "\n",
    "# print(storm_from_outages_states_df['EVENT_TYPE'].unique())\n",
    "# print(top_least_10_storm_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55d99342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/x4myp3c97fx7r5kdg1c9gzmc0000gn/T/ipykernel_1724/2948589550.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['BEGIN_DATETIME'] = pd.to_datetime(\n",
      "/var/folders/fz/x4myp3c97fx7r5kdg1c9gzmc0000gn/T/ipykernel_1724/2948589550.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['END_DATETIME'] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "storm_ts_df = make_ts_states_storms(\n",
    "    states=storm_from_outages_states_df['STATE'].unique(),\n",
    "    event_types=storm_from_outages_states_df['EVENT_TYPE'].unique(),\n",
    "    start_year=2014, start_month=1, start_day=1,\n",
    "    end_year=2023, end_month=12, end_day=31,\n",
    "    df=storm_from_outages_states_df\n",
    ")\n",
    "\n",
    "# storm_ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2dd1523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3505920 entries, 0 to 3505919\n",
      "Data columns (total 46 columns):\n",
      " #   Column                               Dtype         \n",
      "---  ------                               -----         \n",
      " 0   time                                 datetime64[ns]\n",
      " 1   event_count Thunderstorm Wind        int64         \n",
      " 2   event_count Hail                     int64         \n",
      " 3   event_count Drought                  int64         \n",
      " 4   event_count Winter Weather           int64         \n",
      " 5   event_count Heavy Snow               int64         \n",
      " 6   event_count Winter Storm             int64         \n",
      " 7   event_count Ice Storm                int64         \n",
      " 8   event_count Flood                    int64         \n",
      " 9   event_count Avalanche                int64         \n",
      " 10  event_count Tornado                  int64         \n",
      " 11  event_count Blizzard                 int64         \n",
      " 12  event_count High Wind                int64         \n",
      " 13  event_count Extreme Cold/Wind Chill  int64         \n",
      " 14  event_count Heavy Rain               int64         \n",
      " 15  event_count Sleet                    int64         \n",
      " 16  event_count Dust Storm               int64         \n",
      " 17  event_count Flash Flood              int64         \n",
      " 18  event_count Cold/Wind Chill          int64         \n",
      " 19  event_count Freezing Fog             int64         \n",
      " 20  event_count Strong Wind              int64         \n",
      " 21  event_count Rip Current              int64         \n",
      " 22  event_count Lightning                int64         \n",
      " 23  event_count Dense Fog                int64         \n",
      " 24  event_count Debris Flow              int64         \n",
      " 25  event_count High Surf                int64         \n",
      " 26  event_count Frost/Freeze             int64         \n",
      " 27  event_count Coastal Flood            int64         \n",
      " 28  event_count Dust Devil               int64         \n",
      " 29  event_count Wildfire                 int64         \n",
      " 30  event_count Funnel Cloud             int64         \n",
      " 31  event_count Lakeshore Flood          int64         \n",
      " 32  event_count Heat                     int64         \n",
      " 33  event_count Seiche                   int64         \n",
      " 34  event_count Lake-Effect Snow         int64         \n",
      " 35  event_count Astronomical Low Tide    int64         \n",
      " 36  event_count Sneakerwave              int64         \n",
      " 37  event_count Excessive Heat           int64         \n",
      " 38  event_count Tsunami                  int64         \n",
      " 39  event_count Tropical Depression      int64         \n",
      " 40  event_count Tropical Storm           int64         \n",
      " 41  event_count Storm Surge/Tide         int64         \n",
      " 42  event_count Dense Smoke              int64         \n",
      " 43  event_count Hurricane                int64         \n",
      " 44  event_count Hurricane (Typhoon)      int64         \n",
      " 45  STATE                                object        \n",
      "dtypes: datetime64[ns](1), int64(44), object(1)\n",
      "memory usage: 1.2+ GB\n"
     ]
    }
   ],
   "source": [
    "storm_ts_df.info()\n",
    "# storm_ts_df['STATE'].unique()\n",
    "# top_least_10_storms_states_df['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_state_ts_power.columns\n",
    "# df_state_ts_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e656d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model['num_drought_events'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cdfcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>state</th>\n",
       "      <th>customers_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>California</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>California</td>\n",
       "      <td>1555.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>California</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>California</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>Florida</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50986279</th>\n",
       "      <td>2023-12-31 00:00:00</td>\n",
       "      <td>Texas</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50986280</th>\n",
       "      <td>2023-12-31 00:00:00</td>\n",
       "      <td>Texas</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50986281</th>\n",
       "      <td>2023-12-31 00:00:00</td>\n",
       "      <td>Texas</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50986282</th>\n",
       "      <td>2023-12-31 00:00:00</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50986283</th>\n",
       "      <td>2023-12-31 00:00:00</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50986284 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        time       state  customers_out\n",
       "0        2014-11-01 04:00:00  California           30.0\n",
       "1        2014-11-01 04:00:00  California         1555.0\n",
       "2        2014-11-01 04:00:00  California            2.0\n",
       "3        2014-11-01 04:00:00  California            1.0\n",
       "4        2014-11-01 04:00:00     Florida           17.0\n",
       "...                      ...         ...            ...\n",
       "50986279 2023-12-31 00:00:00       Texas           19.0\n",
       "50986280 2023-12-31 00:00:00       Texas            4.0\n",
       "50986281 2023-12-31 00:00:00       Texas          161.0\n",
       "50986282 2023-12-31 00:00:00     Wyoming            1.0\n",
       "50986283 2023-12-31 00:00:00     Wyoming            2.0\n",
       "\n",
       "[50986284 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outages_ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f95b0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>50986284</td>\n",
       "      <td>2020-03-16 16:13:33.389327616</td>\n",
       "      <td>2014-11-01 04:00:00</td>\n",
       "      <td>2018-07-21 23:30:00</td>\n",
       "      <td>2020-06-15 14:00:00</td>\n",
       "      <td>2022-03-27 08:15:00</td>\n",
       "      <td>2023-12-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customers_out</th>\n",
       "      <td>48855271.0</td>\n",
       "      <td>294.938671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1777800.0</td>\n",
       "      <td>4724.017568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count                           mean                  min  \\\n",
       "time             50986284  2020-03-16 16:13:33.389327616  2014-11-01 04:00:00   \n",
       "customers_out  48855271.0                     294.938671                  0.0   \n",
       "\n",
       "                               25%                  50%                  75%  \\\n",
       "time           2018-07-21 23:30:00  2020-06-15 14:00:00  2022-03-27 08:15:00   \n",
       "customers_out                  2.0                  6.0                 44.0   \n",
       "\n",
       "                               max          std  \n",
       "time           2023-12-31 00:00:00          NaN  \n",
       "customers_out            1777800.0  4724.017568  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outages_ts_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_state_ts_events.columns\n",
    "# storm_ts_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined_hr, df_combined_day = combine_agg_ts(...)\n",
    "\n",
    "# storm_events_14_24_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df3ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Texas' 'California' 'Wyoming' 'Louisiana' 'Montana' 'South Dakota'\n",
      " 'North Dakota' 'Florida' 'Michigan' 'District of Columbia'] \n",
      "\n",
      "['California' 'Florida' 'Louisiana' 'Michigan' 'Texas' 'North Dakota'\n",
      " 'District of Columbia' 'Montana' 'South Dakota' 'Wyoming']\n"
     ]
    }
   ],
   "source": [
    "print(storm_ts_df['STATE'].unique(),\"\\n\")\n",
    "print(outages_ts_df['state'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f48ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34284a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips_code</th>\n",
       "      <th>county</th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>outages_start_time</th>\n",
       "      <th>customers_out</th>\n",
       "      <th>major_outage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6001</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>2017-11-21</td>\n",
       "      <td>California</td>\n",
       "      <td>2017-11-21 00:00:00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6001</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>California</td>\n",
       "      <td>2018-06-01 05:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6001</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>California</td>\n",
       "      <td>2018-06-05 23:30:00</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6001</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>California</td>\n",
       "      <td>2018-06-05 23:45:00</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6001</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>2018-06-06</td>\n",
       "      <td>California</td>\n",
       "      <td>2018-06-06 00:00:00</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fips_code   county       date       state  outages_start_time  \\\n",
       "0      6001  Alameda 2017-11-21  California 2017-11-21 00:00:00   \n",
       "1      6001  Alameda 2018-06-01  California 2018-06-01 05:00:00   \n",
       "2      6001  Alameda 2018-06-05  California 2018-06-05 23:30:00   \n",
       "3      6001  Alameda 2018-06-05  California 2018-06-05 23:45:00   \n",
       "4      6001  Alameda 2018-06-06  California 2018-06-06 00:00:00   \n",
       "\n",
       "   customers_out  major_outage  \n",
       "0           28.0             0  \n",
       "1            0.0             0  \n",
       "2          107.0             0  \n",
       "3          108.0             0  \n",
       "4           66.0             0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Copy the data\n",
    "df_labels = top_least_10_states_outages_df.copy()\n",
    "\n",
    "# Step 2: Parse run_start_time into date\n",
    "df_labels['date'] = pd.to_datetime(df_labels['outages_start_time']).dt.date\n",
    "\n",
    "# Step 3: Group by fips_code, county, and date\n",
    "df_outages_grouped = (\n",
    "    df_labels\n",
    "    .groupby(['fips_code', 'county', 'date','state','outages_start_time'])\n",
    "    .agg({'customers_out': 'sum'})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 4: Create the binary major outage label\n",
    "# THRESHOLD = 1000  # customers\n",
    "# df_outages_grouped['major_outage'] = (df_outages_grouped['customers_out'] >= THRESHOLD).astype(int)\n",
    "\n",
    "# df_outages_grouped['fips_code'] = df_outages_grouped['fips_code'].astype(str)\n",
    "# df_outages_grouped['date'] = pd.to_datetime(df_outages_grouped['date'])\n",
    "\n",
    "# Step 5: View a few rows\n",
    "df_outages_grouped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4011db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50998862 entries, 0 to 50998861\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   fips_code           object        \n",
      " 1   county              object        \n",
      " 2   date                datetime64[ns]\n",
      " 3   state               object        \n",
      " 4   outages_start_time  datetime64[ns]\n",
      " 5   customers_out       float64       \n",
      " 6   major_outage        int64         \n",
      "dtypes: datetime64[ns](2), float64(1), int64(1), object(3)\n",
      "memory usage: 2.7+ GB\n"
     ]
    }
   ],
   "source": [
    "        #    Total_Injuries=('INJURIES_DIRECT', 'sum'),\n",
    "        #     Total_Deaths=('DEATHS_DIRECT', 'sum'),\n",
    "        #     Total_Property_Damage=('DAMAGE_PROPERTY', 'sum'),\n",
    "df_outages_grouped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b82d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_least_10_storms_states_df['CZ_NAME'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_least_10_storms_states_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e271e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/x4myp3c97fx7r5kdg1c9gzmc0000gn/T/ipykernel_69472/2266566031.py:26: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_storms['date'] = pd.to_datetime(df_storms['BEGIN_DATE_TIME']).dt.date\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_storms = storm_from_outages_states_df.copy()\n",
    "\n",
    "# Fix the date\n",
    "df_storms['date'] = pd.to_datetime(df_storms['BEGIN_DATE_TIME']).dt.date\n",
    "\n",
    "# Create fips_code\n",
    "df_storms['STATE_FIPS'] = df_storms['STATE_FIPS'].astype(str).str.zfill(2)\n",
    "df_storms['CZ_FIPS'] = df_storms['CZ_FIPS'].astype(str).str.zfill(3)\n",
    "df_storms['fips_code'] = df_storms['STATE_FIPS'] + df_storms['CZ_FIPS']\n",
    "\n",
    "# --- Clean the damage columns here ---\n",
    "df_storms['DAMAGE_PROPERTY'] = df_storms['DAMAGE_PROPERTY'].apply(parse_damage)\n",
    "df_storms['DAMAGE_CROPS'] = df_storms['DAMAGE_CROPS'].apply(parse_damage)\n",
    "\n",
    "df_storms = df_storms.rename(columns={\n",
    "    'STATE': 'state',\n",
    "})\n",
    "\n",
    "storm_agg_dict = {\n",
    "    'EVENT_TYPE': 'count',\n",
    "    'DAMAGE_PROPERTY': 'sum',\n",
    "    'DAMAGE_CROPS': 'sum',\n",
    "    'INJURIES_DIRECT': 'sum',\n",
    "    'INJURIES_INDIRECT': 'sum',\n",
    "    'DEATHS_DIRECT': 'sum',\n",
    "    'TOR_F_SCALE': pd.Series.nunique,\n",
    "}\n",
    "\n",
    "storm_agg_renames = {\n",
    "    'EVENT_TYPE': 'num_events',\n",
    "    'DAMAGE_PROPERTY': 'total_property_damage',\n",
    "    'DAMAGE_CROPS': 'total_crop_damage',\n",
    "    'TOR_F_SCALE': 'num_tornado_scales',\n",
    "    'INJURIES_DIRECT': 'num_direct_injuries',\n",
    "    'INJURIES_INDIRECT': 'num_indirect_injuries',\n",
    "    'DEATHS_DIRECT': 'num_direct_deaths',\n",
    "}\n",
    "\n",
    "\n",
    "# --- THEN group ---\n",
    "df_storms_grouped = (\n",
    "    df_storms\n",
    "    .groupby(['fips_code', 'date', 'state'])\n",
    "    .agg(storm_agg_dict)\n",
    "    .rename(columns=storm_agg_renames)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    " \n",
    "#  fips_code should be string in both\n",
    "df_storms_grouped['fips_code'] = df_storms_grouped['fips_code'].astype(str)\n",
    "\n",
    "# date should be datetime in both\n",
    "df_storms_grouped['date'] = pd.to_datetime(df_storms_grouped['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1088c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>DAMAGE_CROPS</th>\n",
       "      <th>INJURIES_DIRECT</th>\n",
       "      <th>DEATHS_DIRECT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501864</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562407</th>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197296</th>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447346</th>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475457</th>\n",
       "      <td>0.00K</td>\n",
       "      <td>0.00K</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       DAMAGE_PROPERTY DAMAGE_CROPS  INJURIES_DIRECT  DEATHS_DIRECT\n",
       "501864             NaN          NaN                0              0\n",
       "562407           0.00K        0.00K                0              0\n",
       "197296           0.00K        0.00K                0              0\n",
       "447346           0.00K        0.00K                0              0\n",
       "475457           0.00K        0.00K                0              0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storm_from_outages_states_df[['DAMAGE_PROPERTY','DAMAGE_CROPS','INJURIES_DIRECT','DEATHS_DIRECT']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9680126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>num_events</th>\n",
       "      <th>total_property_damage</th>\n",
       "      <th>total_crop_damage</th>\n",
       "      <th>num_direct_injuries</th>\n",
       "      <th>num_indirect_injuries</th>\n",
       "      <th>num_direct_deaths</th>\n",
       "      <th>num_tornado_scales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>109643</td>\n",
       "      <td>109643.000000</td>\n",
       "      <td>1.096430e+05</td>\n",
       "      <td>1.096430e+05</td>\n",
       "      <td>109643.000000</td>\n",
       "      <td>109643.000000</td>\n",
       "      <td>109643.00000</td>\n",
       "      <td>109643.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019-11-17 20:19:11.942212352</td>\n",
       "      <td>1.509490</td>\n",
       "      <td>1.591601e+06</td>\n",
       "      <td>7.415820e+04</td>\n",
       "      <td>0.052571</td>\n",
       "      <td>0.010580</td>\n",
       "      <td>0.01628</td>\n",
       "      <td>0.026194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2017-03-07 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020-04-20 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2022-08-05 00:00:00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-09-30 00:00:00</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>1.700000e+10</td>\n",
       "      <td>1.500000e+09</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>86.00000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.596529</td>\n",
       "      <td>8.691500e+07</td>\n",
       "      <td>5.138292e+06</td>\n",
       "      <td>3.349964</td>\n",
       "      <td>0.410721</td>\n",
       "      <td>0.42912</td>\n",
       "      <td>0.180060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date     num_events  total_property_damage  \\\n",
       "count                         109643  109643.000000           1.096430e+05   \n",
       "mean   2019-11-17 20:19:11.942212352       1.509490           1.591601e+06   \n",
       "min              2014-01-01 00:00:00       1.000000           0.000000e+00   \n",
       "25%              2017-03-07 00:00:00       1.000000           0.000000e+00   \n",
       "50%              2020-04-20 00:00:00       1.000000           0.000000e+00   \n",
       "75%              2022-08-05 00:00:00       1.000000           0.000000e+00   \n",
       "max              2024-09-30 00:00:00      82.000000           1.700000e+10   \n",
       "std                              NaN       1.596529           8.691500e+07   \n",
       "\n",
       "       total_crop_damage  num_direct_injuries  num_indirect_injuries  \\\n",
       "count       1.096430e+05        109643.000000          109643.000000   \n",
       "mean        7.415820e+04             0.052571               0.010580   \n",
       "min         0.000000e+00             0.000000               0.000000   \n",
       "25%         0.000000e+00             0.000000               0.000000   \n",
       "50%         0.000000e+00             0.000000               0.000000   \n",
       "75%         0.000000e+00             0.000000               0.000000   \n",
       "max         1.500000e+09           806.000000              55.000000   \n",
       "std         5.138292e+06             3.349964               0.410721   \n",
       "\n",
       "       num_direct_deaths  num_tornado_scales  \n",
       "count       109643.00000       109643.000000  \n",
       "mean             0.01628            0.026194  \n",
       "min              0.00000            0.000000  \n",
       "25%              0.00000            0.000000  \n",
       "50%              0.00000            0.000000  \n",
       "75%              0.00000            0.000000  \n",
       "max             86.00000            4.000000  \n",
       "std              0.42912            0.180060  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_storms_grouped.sample(5)\n",
    "df_storms_grouped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a29e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-numerics: 0\n",
      "non-numerics: 0\n",
      "Extreme property damage amount entries (over 1 billion):\n",
      "[1.500000e+09 1.700000e+10 2.000000e+09 2.204500e+09 6.700000e+09\n",
      " 3.000000e+09 7.000000e+09 1.070000e+09 1.680000e+09 6.000000e+09\n",
      " 1.250000e+09 1.340000e+09 1.000750e+09 1.100000e+09 1.360000e+09\n",
      " 1.300200e+09 1.960025e+09 1.351270e+09 8.002830e+09 1.000000e+10\n",
      " 1.000068e+10 1.300000e+09 1.950000e+09]\n",
      "\n",
      "Extreme crop damage amount amount entries (over 1 billion):\n",
      "[1.5e+09]\n"
     ]
    }
   ],
   "source": [
    "# Check for non-numeric values (should be none)\n",
    "print(\"non-numerics:\",df_storms_grouped['total_property_damage'].apply(lambda x: isinstance(x, str)).sum())\n",
    "print(\"non-numerics:\",df_storms_grouped['total_crop_damage'].apply(lambda x: isinstance(x, str)).sum())\n",
    "\n",
    "# Property damage greater than 1 billion\n",
    "gt_onebillion_property = df_storms_grouped[df_storms_grouped['total_property_damage'] > 1e9]['total_property_damage'].unique()\n",
    "\n",
    "# Crop damage greater than 1 billion\n",
    "gt_onebillion_crop = df_storms_grouped[df_storms_grouped['total_crop_damage'] > 1e9]['total_crop_damage'].unique()\n",
    "\n",
    "print(\"Extreme property damage amount entries (over 1 billion):\")\n",
    "print(gt_onebillion_property)\n",
    "\n",
    "print(\"\\nExtreme crop damage amount amount entries (over 1 billion):\")\n",
    "print(gt_onebillion_crop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637968a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_property_damage_usd</th>\n",
       "      <th>total_crop_damage_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100423</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        total_property_damage_usd  total_crop_damage_usd\n",
       "100423                        0.0                    0.0\n",
       "48320                         0.0                    0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_storms_grouped = df_storms_grouped.rename(columns={\n",
    "    'total_property_damage': 'total_property_damage_usd',\n",
    "    'total_crop_damage': 'total_crop_damage_usd'\n",
    "})\n",
    "\t\n",
    "\n",
    "df_storms_grouped[['total_property_damage_usd','total_crop_damage_usd']].sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95002fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_storms_grouped['total_crop_damage_usd'].unique()\n",
    "# df_storms_grouped[['total_property_damage',\n",
    "#        'total_crop_damage', 'num_tornado_scales']].sample(10)\n",
    "# df_storms_grouped.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef26dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['California', 'District of Columbia', 'Florida', 'Louisiana',\n",
       "       'Michigan', 'Montana', 'North Dakota', 'South Dakota', 'Texas',\n",
       "       'Wyoming'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_storms_grouped['state'].unique()\n",
    "df_outages_grouped['state'].unique()\n",
    "# .info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_outages_grouped.info()\n",
    "# df_storms_grouped.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model = pd.merge(\n",
    "#     df_outages_grouped,\n",
    "#     df_storms_grouped,\n",
    "#     on=['fips_code', 'date', 'state'],\n",
    "#     how='inner'\n",
    "# )\n",
    "\n",
    "# df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8f3bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3dc02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3a57b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_storms_grouped['fips_code'] = df_storms_grouped['fips_code'].astype(str)\n",
    "df_storms_grouped['fips_code'] = df_storms_grouped['fips_code'].astype(str)\n",
    "df_storms_grouped['date'] = pd.to_datetime(df_storms_grouped['date'])\n",
    "df_storms_grouped['date'] = pd.to_datetime(df_storms_grouped['date'])\n",
    "\n",
    "\n",
    "# --- Step 5A: Add per-event-type counts ---\n",
    "\n",
    "# 1. Group storms by fips_code, date, and event_type\n",
    "df_storms_event_counts = (\n",
    "    df_storms\n",
    "    .groupby(['fips_code', 'date', 'EVENT_TYPE','state'])\n",
    "    .size()\n",
    "    .reset_index(name='event_count')\n",
    ")\n",
    "\n",
    "# 2. Pivot so each event type becomes a separate column\n",
    "df_storms_event_pivot = (\n",
    "    df_storms_event_counts\n",
    "    .pivot_table(index=['fips_code', 'date','state'], \n",
    "                 columns='EVENT_TYPE', \n",
    "                 values='event_count', \n",
    "                 fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_storms_event_pivot['fips_code'] = df_storms_event_pivot['fips_code'].astype(str)\n",
    "df_storms_event_pivot['fips_code'] = df_storms_event_pivot['fips_code'].astype(str)\n",
    "df_storms_event_pivot['date'] = pd.to_datetime(df_storms_event_pivot['date'])\n",
    "df_storms_event_pivot['date'] = pd.to_datetime(df_storms_event_pivot['date'])\n",
    "\n",
    "# 3. Rename event columns to num_<event>_events for consistency\n",
    "df_storms_event_pivot.columns = ['fips_code', 'date','state'] + [\n",
    "     f\"num_{col.lower().replace(' ', '_').replace('/', '_')}_events\" for col in df_storms_event_pivot.columns[3:]\n",
    "]\n",
    "\n",
    "\n",
    "# 4. Merge with df_storms_grouped (the previous storm summary)\n",
    "df_storms_full = pd.merge(\n",
    "    df_storms_grouped,\n",
    "    df_storms_event_pivot,\n",
    "    on=['fips_code', 'date','state'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill NA event counts with 0 (if no such event that day)\n",
    "event_cols = [col for col in df_storms_full.columns if col.startswith('num_') and col.endswith('_events')]\n",
    "# df_storms_full[event_cols] = df_storms_full[event_cols].fillna(0)\n",
    "\n",
    "# --- Final merge to outage labels ---\n",
    "\n",
    "df_outages_grouped['fips_code'] = df_outages_grouped['fips_code'].astype(str)\n",
    "df_outages_grouped['fips_code'] = df_outages_grouped['fips_code'].astype(str)\n",
    "df_outages_grouped['date'] = pd.to_datetime(df_outages_grouped['date'])\n",
    "df_outages_grouped['date'] = pd.to_datetime(df_outages_grouped['date'])\n",
    "\n",
    "df_storms_full['fips_code'] = df_storms_full['fips_code'].astype(str)\n",
    "df_storms_full['fips_code'] = df_storms_full['fips_code'].astype(str)\n",
    "df_storms_full['date'] = pd.to_datetime(df_storms_full['date'])\n",
    "df_storms_full['date'] = pd.to_datetime(df_storms_full['date'])\n",
    "\n",
    "# Standardize fips_code\n",
    "df_outages_grouped['fips_code'] = df_outages_grouped['fips_code'].astype(str).str.zfill(5).str.strip()\n",
    "df_storms_full['fips_code'] = df_storms_full['fips_code'].astype(str).str.zfill(5).str.strip()\n",
    "\n",
    "# Standardize date\n",
    "df_outages_grouped['date'] = pd.to_datetime(df_outages_grouped['date']).dt.tz_localize(None)\n",
    "df_storms_full['date'] = pd.to_datetime(df_storms_full['date']).dt.tz_localize(None)\n",
    "\n",
    "# Standardize state\n",
    "df_outages_grouped['state'] = df_outages_grouped['state'].str.strip().str.upper()\n",
    "df_storms_full['state'] = df_storms_full['state'].str.strip().str.upper()\n",
    "\n",
    "# Get the intersection of shared states\n",
    "shared_states = set(df_outages_grouped['state'].unique()) & set(df_storms_full['state'].unique())\n",
    "\n",
    "# Filter both DataFrames\n",
    "df_outages_grouped = df_outages_grouped[df_outages_grouped['state'].isin(shared_states)]\n",
    "df_storms_full = df_storms_full[df_storms_full['state'].isin(shared_states)]\n",
    "\n",
    "df_model_inner = pd.merge(df_outages_grouped, df_storms_full, on=['fips_code', 'date', 'state'], how='inner')\n",
    "df_model_fallback = pd.merge(df_outages_grouped, df_storms_full, on=['fips_code', 'date'], how='inner')\n",
    "\n",
    "print(f\"Inner merge rows: {len(df_model_inner)}\")\n",
    "print(f\"Fallback merge rows (no state): {len(df_model_fallback)}\")\n",
    "\n",
    "\n",
    "# Merge storm data with outage labels\n",
    "df_model = pd.merge(\n",
    "    df_outages_grouped,\n",
    "    df_storms_full,\n",
    "    on=['fips_code', 'date','state'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Optional safety fill again\n",
    "df_model.fillna(0, inplace=True)\n",
    "\n",
    "df_model.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa81891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                            30338\n",
       "mean     2019-08-14 21:17:20.200408832\n",
       "min                2014-11-01 00:00:00\n",
       "25%                2018-03-06 00:00:00\n",
       "50%                2019-12-07 12:00:00\n",
       "75%                2021-06-21 00:00:00\n",
       "max                2022-12-31 00:00:00\n",
       "Name: date, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_model['state_x'].unique()\n",
    "df_model['date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9089ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap in fips_code: 724\n",
      "Overlap in state: 10\n",
      "Overlap in date: 2915\n",
      "Exact matches (fips_code, date, state): 30338\n"
     ]
    }
   ],
   "source": [
    "# Confirm overlap in fips_code\n",
    "print(\"Overlap in fips_code:\", len(set(df_outages_grouped['fips_code']) & set(df_storms_full['fips_code'])))\n",
    "\n",
    "# Confirm overlap in state\n",
    "print(\"Overlap in state:\", len(set(df_outages_grouped['state']) & set(df_storms_full['state'])))\n",
    "\n",
    "# Confirm overlap in date\n",
    "print(\"Overlap in date:\", len(set(df_outages_grouped['date']) & set(df_storms_full['date'])))\n",
    "\n",
    "# Confirm exact match triplets\n",
    "merge_keys_outages = set(zip(df_outages_grouped['fips_code'], df_outages_grouped['date'], df_outages_grouped['state']))\n",
    "merge_keys_storms = set(zip(df_storms_full['fips_code'], df_storms_full['date'], df_storms_full['state']))\n",
    "print(\"Exact matches (fips_code, date, state):\", len(merge_keys_outages & merge_keys_storms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec10db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_outages_grouped states: ['CALIFORNIA' 'DISTRICT OF COLUMBIA' 'FLORIDA' 'LOUISIANA' 'MICHIGAN'\n",
      " 'NORTH DAKOTA' 'TEXAS' 'WYOMING' 'US VIRGIN ISLANDS'\n",
      " 'UNITED STATES VIRGIN ISLANDS']\n",
      "df_storms_full states: ['CALIFORNIA' 'IOWA' 'KANSAS' 'TEXAS' 'VIRGINIA' 'GUAM WATERS'\n",
      " 'HAWAII WATERS' 'E PACIFIC' 'GULF OF ALASKA' 'ST LAWRENCE R']\n"
     ]
    }
   ],
   "source": [
    "print(\"df_outages_grouped states:\",df_outages_grouped['state'].unique())\n",
    "print(\"df_storms_full states:\",df_storms_full['state'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c3bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_outages_grouped['date'].dt.tz)\n",
    "print(df_storms_full['date'].dt.tz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234915c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da551093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b1b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5000e+01, 2.7300e+02, 3.8700e+02, ..., 1.4490e+03, 4.9624e+04,\n",
       "       5.4530e+03])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['customers_out'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cf18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f24492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customers_out</th>\n",
       "      <th>major_outage</th>\n",
       "      <th>num_events</th>\n",
       "      <th>total_property_damage_usd</th>\n",
       "      <th>total_crop_damage_usd</th>\n",
       "      <th>num_tornado_scales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.033800e+04</td>\n",
       "      <td>30338.000000</td>\n",
       "      <td>30338.000000</td>\n",
       "      <td>3.033800e+04</td>\n",
       "      <td>3.033800e+04</td>\n",
       "      <td>30338.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.347287e+04</td>\n",
       "      <td>0.474619</td>\n",
       "      <td>1.775727</td>\n",
       "      <td>2.469232e+06</td>\n",
       "      <td>7.512939e+04</td>\n",
       "      <td>0.055112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.486636e+05</td>\n",
       "      <td>0.499364</td>\n",
       "      <td>1.906335</td>\n",
       "      <td>8.849966e+07</td>\n",
       "      <td>3.280123e+06</td>\n",
       "      <td>0.258542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.200000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.670000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.156750e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.235904e+07</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000e+10</td>\n",
       "      <td>3.080000e+08</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       customers_out  major_outage    num_events  total_property_damage_usd  \\\n",
       "count   3.033800e+04  30338.000000  30338.000000               3.033800e+04   \n",
       "mean    3.347287e+04      0.474619      1.775727               2.469232e+06   \n",
       "std     4.486636e+05      0.499364      1.906335               8.849966e+07   \n",
       "min     0.000000e+00      0.000000      1.000000               0.000000e+00   \n",
       "25%     4.200000e+01      0.000000      1.000000               0.000000e+00   \n",
       "50%     7.670000e+02      0.000000      1.000000               0.000000e+00   \n",
       "75%     8.156750e+03      1.000000      2.000000               0.000000e+00   \n",
       "max     3.235904e+07      1.000000     50.000000               1.000000e+10   \n",
       "\n",
       "       total_crop_damage_usd  num_tornado_scales  \n",
       "count           3.033800e+04        30338.000000  \n",
       "mean            7.512939e+04            0.055112  \n",
       "std             3.280123e+06            0.258542  \n",
       "min             0.000000e+00            0.000000  \n",
       "25%             0.000000e+00            0.000000  \n",
       "50%             0.000000e+00            0.000000  \n",
       "75%             0.000000e+00            0.000000  \n",
       "max             3.080000e+08            4.000000  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_model.isnull().mean() * 100\n",
    "# df_model.info()\n",
    "# df.describe(include=['number'])\n",
    "# df_model.columns\n",
    "df_model[['customers_out', 'major_outage',\n",
    "       'num_events', 'total_property_damage_usd', 'total_crop_damage_usd',\n",
    "       'num_tornado_scales']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5186e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 4., 3., 2., 5.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['num_dense_fog_events'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d43c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add extra storm features (like event types: tornado, hail, flood separately\n",
    "# tracks maximum wind speed and largest hail size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888b7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_merged_storm_outage_data(\n",
    "#     outages_raw_df,\n",
    "#     storm_raw_df,\n",
    "#     state,\n",
    "#     event_types,\n",
    "#     start_date,\n",
    "#     end_date\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Prepare and merge outages and storm event datasets.\n",
    "\n",
    "#     Args:\n",
    "#         outages_raw_df (pd.DataFrame): Outages dataset (e.g., outages14_23_texas_df).\n",
    "#         storm_raw_df (pd.DataFrame): Storm events dataset (e.g., storm_events_14_24_df).\n",
    "#         state (str): State name to filter storm events (e.g., 'TEXAS').\n",
    "#         event_types (list): List of event types to count (e.g., ['Hurricane', 'Flood', ...]).\n",
    "#         start_date (datetime): Start date of analysis window.\n",
    "#         end_date (datetime): End date of analysis window.\n",
    "\n",
    "#     Returns:\n",
    "#         merged_df (pd.DataFrame): Final merged DataFrame with features.\n",
    "#         event_15min_df (pd.DataFrame): 15-min storm event time series (optional).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # --- Prepare Outages Dataset ---\n",
    "#     outages_df = outages_raw_df.copy()\n",
    "#     outages_df = outages_df.rename(columns={'run_start_time': 'Start_time'})\n",
    "\n",
    "#     # Create Date column for grouping\n",
    "#     outages_df['Date'] = outages_df['Start_time'].dt.date\n",
    "\n",
    "#     # Group outages by fips_code and Date\n",
    "#     outages_grouped_df = (\n",
    "#         outages_df.groupby(['fips_code', 'Date'])\n",
    "#         .agg(\n",
    "#             Total_Customers_Out=('customers_out', 'sum'),\n",
    "#             Number_of_Outages=('customers_out', 'count'),\n",
    "#             Max_Customers_Single_Outage=('customers_out', 'max'),\n",
    "#             Min_Start_Hour=('Start_time', lambda x: x.dt.hour.min()),\n",
    "#             Max_Start_Hour=('Start_time', lambda x: x.dt.hour.max()),\n",
    "#         )\n",
    "#         .reset_index()\n",
    "#     )\n",
    "\n",
    "#     print(f\"Outages grouped shape: {outages_grouped_df.shape}\")\n",
    "\n",
    "#     # --- Prepare Storm Events Dataset ---\n",
    "#     storm_df = storm_raw_df.copy()\n",
    "\n",
    "#     # Parse storm datetime columns\n",
    "#     storm_df['BEGIN_DATE_TIME'] = pd.to_datetime(storm_df['BEGIN_DATE_TIME'])\n",
    "#     storm_df['END_DATE_TIME'] = pd.to_datetime(storm_df['END_DATE_TIME'])\n",
    "\n",
    "#     # Create Date columns for filtering\n",
    "#     storm_df['BEGIN_DATE'] = storm_df['BEGIN_DATE_TIME'].dt.date\n",
    "#     storm_df['END_DATE'] = storm_df['END_DATE_TIME'].dt.date\n",
    "#     storm_df['Date'] = storm_df['BEGIN_DATE_TIME'].dt.date  # Primary Date\n",
    "\n",
    "#     # Correct filtering logic (important!)\n",
    "#     start_dt = pd.to_datetime(start_date).date()\n",
    "#     end_dt = pd.to_datetime(end_date).date()\n",
    "\n",
    "#     filtered_storm = storm_df[\n",
    "#         (storm_df['STATE'] == state) &\n",
    "#         (storm_df['BEGIN_DATE'] <= end_dt) &\n",
    "#         (storm_df['END_DATE'] >= start_dt)\n",
    "#     ].copy()\n",
    "\n",
    "#     if filtered_storm.empty:\n",
    "#         print(\"‚ö†Ô∏è Warning: No storm events after filtering. Check your date range or state name.\")\n",
    "\n",
    "#     # Additional storm features\n",
    "#     filtered_storm['Event_Duration_hours'] = (filtered_storm['END_DATE_TIME'] - filtered_storm['BEGIN_DATE_TIME']).dt.total_seconds() / 3600\n",
    "#     filtered_storm['Is_Overnight'] = (filtered_storm['BEGIN_DATE'] != filtered_storm['END_DATE'])\n",
    "#     filtered_storm['Event_Count'] = 1\n",
    "\n",
    "#     # Group by STATE, CZ_NAME (county), Date\n",
    "#     storm_grouped_df = (\n",
    "#         filtered_storm.groupby(['STATE', 'CZ_NAME', 'Date'])\n",
    "#         .agg(\n",
    "#             Total_Injuries=('INJURIES_DIRECT', 'sum'),\n",
    "#             Total_Deaths=('DEATHS_DIRECT', 'sum'),\n",
    "#             Total_Property_Damage=('DAMAGE_PROPERTY', 'sum'),\n",
    "#             Mean_Event_Duration=('Event_Duration_hours', 'mean'),\n",
    "#             Max_Event_Duration=('Event_Duration_hours', 'max'),\n",
    "#             Proportion_Overnight_Events=('Is_Overnight', 'mean')\n",
    "#         )\n",
    "#         .reset_index()\n",
    "#     )\n",
    "\n",
    "#     # --- Create Event Type Daily Counts ---\n",
    "#     event_type_counts = (\n",
    "#         filtered_storm.pivot_table(\n",
    "#             index=['STATE', 'CZ_NAME', 'Date'],\n",
    "#             columns='EVENT_TYPE',\n",
    "#             values='Event_Count',\n",
    "#             aggfunc='sum',\n",
    "#             fill_value=0\n",
    "#         )\n",
    "#         .reset_index()\n",
    "#     )\n",
    "\n",
    "#     # Rename event columns\n",
    "#     non_event_cols = ['STATE', 'CZ_NAME', 'Date']\n",
    "#     event_cols = [col for col in event_type_counts.columns if col not in non_event_cols]\n",
    "#     event_type_counts.rename(columns={col: f'Event_{col}' for col in event_cols}, inplace=True)\n",
    "\n",
    "#     # Merge grouped storm features with event type counts\n",
    "#     storm_final_df = storm_grouped_df.merge(event_type_counts, on=['STATE', 'CZ_NAME', 'Date'], how='left')\n",
    "#     print(f\"Storm events grouped shape: {storm_final_df.shape}\")\n",
    "\n",
    "#     # Fill missing event counts with 0\n",
    "#     event_count_cols = [col for col in storm_final_df.columns if col.startswith('Event_')]\n",
    "#     storm_final_df[event_count_cols] = storm_final_df[event_count_cols].fillna(0)\n",
    "    \n",
    "\n",
    "#     # --- Merge Outages and Storm Events ---\n",
    "#     # Merge on Date + location\n",
    "#     merged_df = outages_grouped_df.merge(\n",
    "#         storm_final_df,\n",
    "#         left_on=['fips_code', 'Date'], \n",
    "#         right_on=['CZ_NAME', 'Date'], \n",
    "#         how='left'\n",
    "#     )\n",
    "\n",
    "#     # --- Time-Based Features (AFTER merging) ---\n",
    "#     # merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "#     # merged_df['Year'] = merged_df['Date'].dt.year\n",
    "#     # merged_df['Month'] = merged_df['Date'].dt.month\n",
    "#     # merged_df['Quarter'] = merged_df['Date'].dt.quarter\n",
    "#     # merged_df['Week'] = merged_df['Date'].dt.isocalendar().week\n",
    "#     # merged_df['DayOfWeek'] = merged_df['Date'].dt.dayofweek\n",
    "\n",
    "#     # --- Optional: Create 15-min storm event time series ---\n",
    "#     time_index = pd.date_range(start=start_date, end=end_date, freq='15min')\n",
    "#     event_15min_df = pd.DataFrame({'time': time_index})\n",
    "\n",
    "#     # Initialize event count columns\n",
    "#     for event_type in event_types:\n",
    "#         event_15min_df[f'event_count_{event_type}'] = 0\n",
    "\n",
    "#     # Round storm events to 15-min and increment counts\n",
    "#     for event_type in event_types:\n",
    "#         event_subset = filtered_storm[filtered_storm['EVENT_TYPE'] == event_type]\n",
    "\n",
    "#         for _, row in event_subset.iterrows():\n",
    "#             event_start = row['BEGIN_DATE_TIME'].round('15min')\n",
    "#             event_end = row['END_DATE_TIME'].round('15min')\n",
    "\n",
    "#             start_idx = event_15min_df['time'].searchsorted(event_start)\n",
    "#             end_idx = event_15min_df['time'].searchsorted(event_end)\n",
    "\n",
    "#             if start_idx < len(event_15min_df) and end_idx <= len(event_15min_df):\n",
    "#                 event_15min_df.loc[start_idx:end_idx, f'event_count_{event_type}'] += 1\n",
    "\n",
    "#     return merged_df, event_15min_df\n",
    "\n",
    "\n",
    "# # Define inputs\n",
    "# state = 'TEXAS'\n",
    "# event_types = ['Hurricane', 'Flood', 'Thunderstorm Wind', 'Tornado', 'Winter Storm']\n",
    "# start_date = datetime(2014, 1, 1)\n",
    "# end_date = datetime(2023, 12, 31)\n",
    "\n",
    "# # Call the function\n",
    "# merged_df, event_15min_df = prepare_merged_storm_outage_data(\n",
    "#     outages_raw_df=outages14_23_texas_df,\n",
    "#     storm_raw_df=storm_events_14_24_df,\n",
    "#     state=state,\n",
    "#     event_types=event_types,\n",
    "#     start_date=start_date,\n",
    "#     end_date=end_date\n",
    "# )\n",
    "\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a63d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading outages for Florida:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:48<00:42, 10.52s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def prepare_storm_and_outages_data(\n",
    "    storm_df_raw,\n",
    "    outage_files_csv,\n",
    "    outages_dir,\n",
    "    outages_columns,\n",
    "    state_filter='TEXAS',\n",
    "    event_types=None,\n",
    "    start_date='2014-01-01',\n",
    "    end_date='2023-12-31'\n",
    "):\n",
    "    if event_types is None:\n",
    "        event_types = ['Tropical Storm', 'Hurricane', 'Flood', 'Thunderstorm Wind', 'High Wind', 'Heavy Rain']\n",
    "\n",
    "    # Convert date strings to datetime\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date) + pd.Timedelta(hours=23, minutes=45)\n",
    "\n",
    "    # --- OUTAGES PREPARATION ---\n",
    "    filtered_outages = []\n",
    "    for f in tqdm(outage_files_csv, desc=f\"Reading outages for {state_filter}\"):\n",
    "        file_path = os.path.join(outages_dir, f)\n",
    "        df = pd.read_csv(file_path, usecols=lambda c: c in outages_columns)\n",
    "        df_state = df[df['state'].str.upper() == state_filter.upper()]\n",
    "        filtered_outages.append(df_state)\n",
    "\n",
    "    outages_df = pd.concat(filtered_outages, ignore_index=True)\n",
    "    outages_df = outages_df.rename(columns={'run_start_time': 'Start_time'})\n",
    "    outages_df['Start_time'] = pd.to_datetime(outages_df['Start_time'])\n",
    "\n",
    "    outages_df['Start_Hour'] = outages_df['Start_time'].dt.hour\n",
    "    outages_df['Start_DayofWeek'] = outages_df['Start_time'].dt.dayofweek\n",
    "    outages_df['Start_Month'] = outages_df['Start_time'].dt.month\n",
    "    outages_df['Start_Quarter'] = outages_df['Start_time'].dt.quarter\n",
    "    outages_df['Date'] = outages_df['Start_time'].dt.date\n",
    "\n",
    "    outages_grouped_df = (\n",
    "        outages_df.groupby(['fips_code', 'Date'])\n",
    "        .agg(\n",
    "            Total_Customers_Out=('customers_out', 'sum'),\n",
    "            Number_of_Outages=('customers_out', 'count'),\n",
    "            Max_Customers_Single_Outage=('customers_out', 'max'),\n",
    "            Min_Start_Hour=('Start_Hour', 'min'),\n",
    "            Max_Start_Hour=('Start_Hour', 'max'),\n",
    "            Start_DayofWeek=('Start_DayofWeek', 'first'),\n",
    "            Start_Month=('Start_Month', 'first'),\n",
    "            Start_Quarter=('Start_Quarter', 'first')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- STORM PREPARATION ---\n",
    "    storm_df = storm_df_raw.copy()\n",
    "    storm_df['BEGIN_DATE_TIME'] = pd.to_datetime(storm_df['BEGIN_DATE_TIME'])\n",
    "    storm_df['END_DATE_TIME'] = pd.to_datetime(storm_df['END_DATE_TIME'])\n",
    "\n",
    "    storm_df = storm_df[\n",
    "        (storm_df['STATE'].str.upper() == state_filter.upper()) &\n",
    "        (storm_df['END_DATE_TIME'] >= start_dt) &\n",
    "        (storm_df['BEGIN_DATE_TIME'] <= end_dt) &\n",
    "        (storm_df['EVENT_TYPE'].isin(event_types))\n",
    "    ]\n",
    "\n",
    "    storm_df['Start_Hour'] = storm_df['BEGIN_DATE_TIME'].dt.hour\n",
    "    storm_df['End_Hour'] = storm_df['END_DATE_TIME'].dt.hour\n",
    "    storm_df['Start_DayofWeek'] = storm_df['BEGIN_DATE_TIME'].dt.dayofweek\n",
    "    storm_df['Start_Month'] = storm_df['BEGIN_DATE_TIME'].dt.month\n",
    "    storm_df['Start_Quarter'] = storm_df['BEGIN_DATE_TIME'].dt.quarter\n",
    "    storm_df['Event_Duration_hours'] = (storm_df['END_DATE_TIME'] - storm_df['BEGIN_DATE_TIME']).dt.total_seconds() / 3600\n",
    "    storm_df['Is_Overnight'] = (storm_df['BEGIN_DATE_TIME'].dt.date != storm_df['END_DATE_TIME'].dt.date)\n",
    "    storm_df['Date'] = storm_df['BEGIN_DATE_TIME'].dt.date\n",
    "    storm_df['Event_Count'] = 1\n",
    "    storm_df['DAMAGE_PROPERTY'] = storm_df['DAMAGE_PROPERTY'].apply(parse_damage)\n",
    "\n",
    "    # Event type pivot\n",
    "    event_type_counts = (\n",
    "        storm_df.pivot_table(\n",
    "            index=['STATE', 'CZ_NAME', 'Date'],\n",
    "            columns='EVENT_TYPE',\n",
    "            values='Event_Count',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Rename event columns\n",
    "    non_event_cols = ['STATE', 'CZ_NAME', 'Date']\n",
    "    event_cols = [col for col in event_type_counts.columns if col not in non_event_cols]\n",
    "    event_type_counts.rename(columns={col: f'Event_{col}' for col in event_cols}, inplace=True)\n",
    "\n",
    "    # Group storm features\n",
    "    storm_grouped_df = (\n",
    "        storm_df.groupby(['STATE', 'CZ_NAME', 'Date'])\n",
    "        .agg(\n",
    "            Total_Injuries=('INJURIES_DIRECT', 'sum'),\n",
    "            Total_Deaths=('DEATHS_DIRECT', 'sum'),\n",
    "            Total_Property_Damage=('DAMAGE_PROPERTY', 'sum'),\n",
    "            Mean_Event_Duration=('Event_Duration_hours', 'mean'),\n",
    "            Max_Event_Duration=('Event_Duration_hours', 'max'),\n",
    "            Proportion_Overnight_Events=('Is_Overnight', 'mean'),\n",
    "            Start_DayofWeek=('Start_DayofWeek', 'first'),\n",
    "            Start_Month=('Start_Month', 'first'),\n",
    "            Start_Quarter=('Start_Quarter', 'first')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    storm_final_df = storm_grouped_df.merge(event_type_counts, on=['STATE', 'CZ_NAME', 'Date'], how='left')\n",
    "    for col in [c for c in storm_final_df.columns if c.startswith('Event_')]:\n",
    "        storm_final_df[col] = storm_final_df[col].fillna(0)\n",
    "\n",
    "    return outages_grouped_df, storm_final_df\n",
    "\n",
    "\n",
    "outages_df, storm_df = prepare_storm_and_outages_data(\n",
    "    storm_df_raw=storm_events_14_24_df,\n",
    "    outage_files_csv=outage_files_csv,\n",
    "    outages_dir=outages_dir,\n",
    "    outages_columns=outages.columns,\n",
    "    state_filter='Florida',\n",
    "    start_date='2014-01-01',\n",
    "    end_date='2023-12-31'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1c90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72be50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def prepare_storm_and_outage_data(\n",
    "    storm_events_df,\n",
    "    outages_df,\n",
    "    state_filter=\"FLORIDA\",\n",
    "    event_types=None,\n",
    "    start_date=datetime(2014, 1, 1),\n",
    "    end_date=datetime(2023, 12, 31, 23, 45),\n",
    "    parse_damage_func=None\n",
    "):\n",
    "    if event_types is None:\n",
    "        event_types = ['Tropical Storm', 'Hurricane', 'Flood', 'Thunderstorm Wind', 'High Wind', 'Heavy Rain']\n",
    "\n",
    "    # --- Prepare Outages Dataset ---\n",
    "    outages_df = outages_df.copy()\n",
    "    outages_df = outages_df.rename(columns={'run_start_time': 'Start_time'})\n",
    "\n",
    "    outages_df['Start_Hour'] = outages_df['Start_time'].dt.hour\n",
    "    outages_df['Start_DayofWeek'] = outages_df['Start_time'].dt.dayofweek\n",
    "    outages_df['Start_Month'] = outages_df['Start_time'].dt.month\n",
    "    outages_df['Start_Quarter'] = outages_df['Start_time'].dt.quarter\n",
    "    outages_df['Date'] = outages_df['Start_time'].dt.date\n",
    "\n",
    "    outages_grouped_df = (\n",
    "        outages_df.groupby(['fips_code', 'Date'])\n",
    "        .agg(\n",
    "            Total_Customers_Out=('customers_out', 'sum'),\n",
    "            Number_of_Outages=('customers_out', 'count'),\n",
    "            Max_Customers_Single_Outage=('customers_out', 'max'),\n",
    "            Min_Start_Hour=('Start_Hour', 'min'),\n",
    "            Max_Start_Hour=('Start_Hour', 'max'),\n",
    "            Start_DayofWeek=('Start_DayofWeek', 'first'),\n",
    "            Start_Month=('Start_Month', 'first'),\n",
    "            Start_Quarter=('Start_Quarter', 'first')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- Prepare Storm Events Dataset ---\n",
    "    storm_df = storm_events_df.copy()\n",
    "    storm_df['BEGIN_DATE_TIME'] = pd.to_datetime(storm_df['BEGIN_DATE_TIME'])\n",
    "    storm_df['END_DATE_TIME'] = pd.to_datetime(storm_df['END_DATE_TIME'])\n",
    "\n",
    "    storm_df['Start_Hour'] = storm_df['BEGIN_DATE_TIME'].dt.hour\n",
    "    storm_df['End_Hour'] = storm_df['END_DATE_TIME'].dt.hour\n",
    "    storm_df['Start_DayofWeek'] = storm_df['BEGIN_DATE_TIME'].dt.dayofweek\n",
    "    storm_df['Start_Month'] = storm_df['BEGIN_DATE_TIME'].dt.month\n",
    "    storm_df['Start_Quarter'] = storm_df['BEGIN_DATE_TIME'].dt.quarter\n",
    "    storm_df['Event_Duration_hours'] = (storm_df['END_DATE_TIME'] - storm_df['BEGIN_DATE_TIME']).dt.total_seconds() / 3600\n",
    "    storm_df['Is_Overnight'] = (storm_df['BEGIN_DATE_TIME'].dt.date != storm_df['END_DATE_TIME'].dt.date)\n",
    "    storm_df['Date'] = storm_df['BEGIN_DATE_TIME'].dt.date\n",
    "    storm_df['Event_Count'] = 1\n",
    "\n",
    "    # Apply damage parser if provided\n",
    "    if parse_damage_func:\n",
    "        storm_df['DAMAGE_PROPERTY'] = storm_df['DAMAGE_PROPERTY'].apply(parse_damage_func)\n",
    "\n",
    "    # Pivot: Daily event type counts\n",
    "    event_type_counts = (\n",
    "        storm_df.pivot_table(\n",
    "            index=['STATE', 'CZ_NAME', 'Date'],\n",
    "            columns='EVENT_TYPE',\n",
    "            values='Event_Count',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    non_event_cols = ['STATE', 'CZ_NAME', 'Date']\n",
    "    event_cols = [col for col in event_type_counts.columns if col not in non_event_cols]\n",
    "    event_type_counts.rename(columns={col: f'Event_{col}' for col in event_cols}, inplace=True)\n",
    "\n",
    "    # Group: Daily storm stats\n",
    "    storm_grouped_df = (\n",
    "        storm_df.groupby(['STATE', 'CZ_NAME', 'Date'])\n",
    "        .agg(\n",
    "            Total_Injuries=('INJURIES_DIRECT', 'sum'),\n",
    "            Total_Deaths=('DEATHS_DIRECT', 'sum'),\n",
    "            Total_Property_Damage=('DAMAGE_PROPERTY', 'sum'),\n",
    "            Mean_Event_Duration=('Event_Duration_hours', 'mean'),\n",
    "            Max_Event_Duration=('Event_Duration_hours', 'max'),\n",
    "            Proportion_Overnight_Events=('Is_Overnight', 'mean'),\n",
    "            Start_DayofWeek=('Start_DayofWeek', 'first'),\n",
    "            Start_Month=('Start_Month', 'first'),\n",
    "            Start_Quarter=('Start_Quarter', 'first')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    storm_final_df = storm_grouped_df.merge(event_type_counts, on=['STATE', 'CZ_NAME', 'Date'], how='left')\n",
    "\n",
    "    # Fill missing event counts with 0\n",
    "    event_count_cols = [col for col in storm_final_df.columns if col.startswith('Event_')]\n",
    "    storm_final_df[event_count_cols] = storm_final_df[event_count_cols].fillna(0)\n",
    "\n",
    "    return storm_final_df, outages_grouped_df\n",
    "\n",
    "\n",
    "storm_final_df, outages_grouped_df = prepare_storm_and_outage_data(\n",
    "    storm_events_df=storm_events_14_24_df,\n",
    "    outages_df=outages14_23_texas_df,\n",
    "    state_filter=\"FLORIDA\",\n",
    "    parse_damage_func=parse_damage\n",
    ")\n",
    "storm_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0351e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/x4myp3c97fx7r5kdg1c9gzmc0000gn/T/ipykernel_87278/3164795041.py:53: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  storm_df['END_DATE_TIME'] = pd.to_datetime(storm_df['END_DATE_TIME'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outages grouped shape: (522485, 10)\n",
      "Storm events grouped shape: (0, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "def prepare_merged_storm_outage_data(storm_df, outages_df, state='FLORIDA', event_types=None,\n",
    "                                     start_date='2014-01-01', end_date='2023-12-31'):\n",
    "    # --- Prepare outages ---\n",
    "    outages_df = outages_df.copy()\n",
    "    outages_df = outages_df.rename(columns={'run_start_time': 'Start_time'})\n",
    "    outages_df['Date'] = outages_df['Start_time'].dt.date\n",
    "    outages_df['Start_Hour'] = outages_df['Start_time'].dt.hour\n",
    "    outages_df['Start_DayofWeek'] = outages_df['Start_time'].dt.dayofweek\n",
    "    outages_df['Start_Month'] = outages_df['Start_time'].dt.month\n",
    "    outages_df['Start_Quarter'] = outages_df['Start_time'].dt.quarter\n",
    "\n",
    "    outages_grouped = (\n",
    "        outages_df.groupby(['fips_code', 'Date'])\n",
    "        .agg(\n",
    "            Total_Customers_Out=('customers_out', 'sum'),\n",
    "            Number_of_Outages=('customers_out', 'count'),\n",
    "            Max_Customers_Single_Outage=('customers_out', 'max'),\n",
    "            Min_Start_Hour=('Start_Hour', 'min'),\n",
    "            Max_Start_Hour=('Start_Hour', 'max'),\n",
    "            Start_DayofWeek=('Start_DayofWeek', 'first'),\n",
    "            Start_Month=('Start_Month', 'first'),\n",
    "            Start_Quarter=('Start_Quarter', 'first')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- Prepare storm events ---\n",
    "    storm_df = storm_df.copy()\n",
    "    storm_df['BEGIN_DATE_TIME'] = pd.to_datetime(storm_df['BEGIN_DATE_TIME'])\n",
    "    storm_df['END_DATE_TIME'] = pd.to_datetime(storm_df['END_DATE_TIME'])\n",
    "\n",
    "    storm_df['Start_Hour'] = storm_df['BEGIN_DATE_TIME'].dt.hour\n",
    "    storm_df['End_Hour'] = storm_df['END_DATE_TIME'].dt.hour\n",
    "    storm_df['Start_DayofWeek'] = storm_df['BEGIN_DATE_TIME'].dt.dayofweek\n",
    "    storm_df['Start_Month'] = storm_df['BEGIN_DATE_TIME'].dt.month\n",
    "    storm_df['Start_Quarter'] = storm_df['BEGIN_DATE_TIME'].dt.quarter\n",
    "    storm_df['Event_Duration_hours'] = (storm_df['END_DATE_TIME'] - storm_df['BEGIN_DATE_TIME']).dt.total_seconds() / 3600\n",
    "    storm_df['Is_Overnight'] = storm_df['BEGIN_DATE_TIME'].dt.date != storm_df['END_DATE_TIME'].dt.date\n",
    "    storm_df['Date'] = storm_df['BEGIN_DATE_TIME'].dt.date\n",
    "    storm_df['Event_Count'] = 1\n",
    "    storm_df['DAMAGE_PROPERTY'] = storm_df['DAMAGE_PROPERTY'].apply(parse_damage)\n",
    "\n",
    "    # Filter storm data by state and date range\n",
    "    start_dt = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    filtered_storm = storm_df[\n",
    "        (storm_df['STATE'] == state) &\n",
    "        (storm_df['BEGIN_DATE_TIME'] <= end_dt) &\n",
    "        (storm_df['END_DATE_TIME'] >= start_dt)\n",
    "    ]\n",
    "\n",
    "    # Pivot for event type counts\n",
    "    event_counts = (\n",
    "        filtered_storm.pivot_table(\n",
    "            index=['STATE', 'CZ_NAME', 'Date'],\n",
    "            columns='EVENT_TYPE',\n",
    "            values='Event_Count',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    non_event_cols = ['STATE', 'CZ_NAME', 'Date']\n",
    "    event_cols = [col for col in event_counts.columns if col not in non_event_cols]\n",
    "    event_counts.rename(columns={col: f'Event_{col}' for col in event_cols}, inplace=True)\n",
    "\n",
    "    # Aggregate storm stats\n",
    "    storm_grouped = (\n",
    "        filtered_storm.groupby(['STATE', 'CZ_NAME', 'Date'])\n",
    "        .agg(\n",
    "            Total_Injuries=('INJURIES_DIRECT', 'sum'),\n",
    "            Total_Deaths=('DEATHS_DIRECT', 'sum'),\n",
    "            Total_Property_Damage=('DAMAGE_PROPERTY', 'sum'),\n",
    "            Mean_Event_Duration=('Event_Duration_hours', 'mean'),\n",
    "            Max_Event_Duration=('Event_Duration_hours', 'max'),\n",
    "            Proportion_Overnight_Events=('Is_Overnight', 'mean'),\n",
    "            Start_DayofWeek=('Start_DayofWeek', 'first'),\n",
    "            Start_Month=('Start_Month', 'first'),\n",
    "            Start_Quarter=('Start_Quarter', 'first')\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    storm_final = storm_grouped.merge(event_counts, on=['STATE', 'CZ_NAME', 'Date'], how='left')\n",
    "    storm_final.update(storm_final.select_dtypes(include='number').fillna(0))\n",
    "\n",
    "    print(f\"Outages grouped shape: {outages_grouped.shape}\")\n",
    "    print(f\"Storm events grouped shape: {storm_final.shape}\")\n",
    "\n",
    "    return storm_final, outages_grouped\n",
    "\n",
    "\n",
    "storm_final_df, outages_grouped_df = prepare_merged_storm_outage_data(\n",
    "    storm_events_14_24_df,\n",
    "    outages14_23_texas_df,\n",
    "    state='FLORIDA',\n",
    "    event_types=['Tropical Storm', 'Hurricane', 'Flood', 'Thunderstorm Wind', 'High Wind', 'Heavy Rain']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd0fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.50000e+01, 7.70000e+01, 1.37000e+02, ..., 1.17735e+05,\n",
       "       4.23060e+04, 1.53840e+04])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outages_grouped_df['Total_Customers_Out'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7b6291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7681b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8868b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387dc87f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Prepare Outages Dataset ---\n",
    "\n",
    "outages_df = outages14_23_texas_df.copy()\n",
    "\n",
    "outages_df = outages_df.rename(columns={'run_start_time': 'Start_time'})\n",
    "\n",
    "outages_df['Start_Hour'] = outages_df['Start_time'].dt.hour\n",
    "outages_df['Start_DayofWeek'] = outages_df['Start_time'].dt.dayofweek\n",
    "outages_df['Start_Month'] = outages_df['Start_time'].dt.month\n",
    "outages_df['Start_Quarter'] = outages_df['Start_time'].dt.quarter\n",
    "outages_df['Date'] = outages_df['Start_time'].dt.date\n",
    "\n",
    "outages_grouped_df = (\n",
    "    outages_df.groupby(['fips_code', 'Date'])\n",
    "    .agg(\n",
    "        Total_Customers_Out=('customers_out', 'sum'),\n",
    "        Number_of_Outages=('customers_out', 'count'),\n",
    "        Max_Customers_Single_Outage=('customers_out', 'max'),\n",
    "        Min_Start_Hour=('Start_Hour', 'min'),\n",
    "        Max_Start_Hour=('Start_Hour', 'max'),\n",
    "        Start_DayofWeek=('Start_DayofWeek', 'first'),\n",
    "        Start_Month=('Start_Month', 'first'),\n",
    "        Start_Quarter=('Start_Quarter', 'first')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Outages grouped shape: {outages_grouped_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50946501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fz/x4myp3c97fx7r5kdg1c9gzmc0000gn/T/ipykernel_84689/3653128900.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  storm_df['END_DATE_TIME'] = pd.to_datetime(storm_df['END_DATE_TIME'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storm events grouped shape: (33685, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Step 2: Prepare Storm Events Dataset ---\n",
    "\n",
    "storm_df = storm_events_14_24_df.copy()\n",
    "\n",
    "# Convert datetime columns\n",
    "storm_df['BEGIN_DATE_TIME'] = pd.to_datetime(storm_df['BEGIN_DATE_TIME'])\n",
    "storm_df['END_DATE_TIME'] = pd.to_datetime(storm_df['END_DATE_TIME'])\n",
    "\n",
    "# Extract time-based features\n",
    "storm_df['Start_Hour'] = storm_df['BEGIN_DATE_TIME'].dt.hour\n",
    "storm_df['End_Hour'] = storm_df['END_DATE_TIME'].dt.hour\n",
    "storm_df['Start_DayofWeek'] = storm_df['BEGIN_DATE_TIME'].dt.dayofweek\n",
    "storm_df['Start_Month'] = storm_df['BEGIN_DATE_TIME'].dt.month\n",
    "storm_df['Start_Quarter'] = storm_df['BEGIN_DATE_TIME'].dt.quarter\n",
    "storm_df['Event_Duration_hours'] = (storm_df['END_DATE_TIME'] - storm_df['BEGIN_DATE_TIME']).dt.total_seconds() / 3600\n",
    "storm_df['Is_Overnight'] = (storm_df['BEGIN_DATE_TIME'].dt.date != storm_df['END_DATE_TIME'].dt.date)\n",
    "storm_df['Date'] = storm_df['BEGIN_DATE_TIME'].dt.date\n",
    "storm_df['Event_Count'] = 1\n",
    "\n",
    "# --- (Optional) 15-Minute Event Count Time Series ---\n",
    "\n",
    "# Define dynamically based on your analysis window and filtering needs\n",
    "start_year, start_month, start_day = 2014, 1, 1\n",
    "end_year, end_month, end_day = 2023, 12, 31\n",
    "\n",
    "state = \"FLORIDA\"  # or dynamically from site metadata\n",
    "event_types = ['Tropical Storm', 'Hurricane', 'Flood', 'Thunderstorm Wind', 'High Wind', 'Heavy Rain']\n",
    "\n",
    "\n",
    "start_date = datetime(start_year, start_month, start_day)\n",
    "end_date = datetime(end_year, end_month, end_day, 23, 45)\n",
    "time_index = pd.date_range(start=start_date, end=end_date, freq='15min')\n",
    "event_15min_df = pd.DataFrame({'time': time_index})\n",
    "\n",
    "# Initialize event count columns\n",
    "for event_type in event_types:\n",
    "    event_15min_df[f'event_count {event_type}'] = 0\n",
    "\n",
    "# Round storm events to 15-min intervals and increment counts\n",
    "filtered_df = storm_df[\n",
    "    (storm_df['STATE'] == state) &\n",
    "    (storm_df['EVENT_TYPE'].isin(event_types)) &\n",
    "    (storm_df['END_DATE_TIME'] >= start_date) &\n",
    "    (storm_df['BEGIN_DATE_TIME'] <= end_date)\n",
    "].copy()\n",
    "\n",
    "for event_type in event_types:\n",
    "    event_subset = filtered_df[filtered_df['EVENT_TYPE'] == event_type]\n",
    "\n",
    "    for _, row in event_subset.iterrows():\n",
    "        event_start = row['BEGIN_DATE_TIME'].round('15min')\n",
    "        event_end = row['END_DATE_TIME'].round('15min')\n",
    "\n",
    "        start_idx = event_15min_df['time'].searchsorted(event_start)\n",
    "        end_idx = event_15min_df['time'].searchsorted(event_end)\n",
    "\n",
    "        if start_idx < len(event_15min_df) and end_idx <= len(event_15min_df):\n",
    "            event_15min_df.loc[start_idx:end_idx, f'event_count {event_type}'] += 1\n",
    "\n",
    "# --- Event Type Daily Count Pivot (for spatial join) ---\n",
    "\n",
    "event_type_counts = (\n",
    "    storm_df.pivot_table(\n",
    "        index=['STATE', 'CZ_NAME', 'Date'],\n",
    "        columns='EVENT_TYPE',\n",
    "        values='Event_Count',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename event columns\n",
    "non_event_cols = ['STATE', 'CZ_NAME', 'Date']\n",
    "event_cols = [col for col in event_type_counts.columns if col not in non_event_cols]\n",
    "event_type_counts.rename(columns={col: f'Event_{col}' for col in event_cols}, inplace=True)\n",
    "\n",
    "# --- Apply damage parser (must define parse_damage beforehand) ---\n",
    "storm_df['DAMAGE_PROPERTY'] = storm_df['DAMAGE_PROPERTY'].apply(parse_damage)\n",
    "\n",
    "# --- Aggregate storm-level features ---\n",
    "storm_grouped_df = (\n",
    "    storm_df.groupby(['STATE', 'CZ_NAME', 'Date'])\n",
    "    .agg(\n",
    "        Total_Injuries=('INJURIES_DIRECT', 'sum'),\n",
    "        Total_Deaths=('DEATHS_DIRECT', 'sum'),\n",
    "        Total_Property_Damage=('DAMAGE_PROPERTY', 'sum'),\n",
    "        Mean_Event_Duration=('Event_Duration_hours', 'mean'),\n",
    "        Max_Event_Duration=('Event_Duration_hours', 'max'),\n",
    "        Proportion_Overnight_Events=('Is_Overnight', 'mean'),\n",
    "        Start_DayofWeek=('Start_DayofWeek', 'first'),\n",
    "        Start_Month=('Start_Month', 'first'),\n",
    "        Start_Quarter=('Start_Quarter', 'first')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- Final merge with event type counts ---\n",
    "storm_final_df = storm_grouped_df.merge(event_type_counts, on=['STATE', 'CZ_NAME', 'Date'], how='left')\n",
    "\n",
    "# Fill missing event counts with 0\n",
    "event_count_cols = [col for col in storm_final_df.columns if col.startswith('Event_')]\n",
    "storm_final_df[event_count_cols] = storm_final_df[event_count_cols].fillna(0)\n",
    "\n",
    "print(f\"Storm events grouped shape: {storm_final_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e4ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3, 2, 4])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storm_final_df['Event_Winter Weather'].unique()\n",
    "# storm_final_df['Total_Injuries'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda4bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d1c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # First, pivot event types before aggregation\n",
    "# event_type_counts = (\n",
    "#     df.pivot_table(\n",
    "#         index=['StationID', 'Date'],\n",
    "#         columns='Event_Type',\n",
    "#         values='Customers_Affected',  # or any column (use 'size' later)\n",
    "#         aggfunc='count',\n",
    "#         fill_value=0\n",
    "#     )\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# # Rename columns: make them easier to work with\n",
    "# event_type_counts.columns = [f'EventType_{col}' if not isinstance(col, tuple) else f'EventType_{col[1]}' for col in event_type_counts.columns]\n",
    "\n",
    "# # Now merge with the original grouped dataframe\n",
    "# grouped_df = (\n",
    "#     df.groupby(['StationID', 'Date'])\n",
    "#     .agg(\n",
    "#         Total_Customers_Affected=('Customers_Affected', 'sum'),\n",
    "#         Number_of_Events=('Customers_Affected', 'count'),\n",
    "#         Mean_Event_Duration=('Event_Duration_hours', 'mean'),\n",
    "#         Max_Customers_Single_Event=('Customers_Affected', 'max'),\n",
    "#         Min_Start_Hour=('Start_Hour', 'min'),\n",
    "#         Max_End_Hour=('End_Hour', 'max'),\n",
    "#         Proportion_Overnight_Events=('Is_Overnight', 'mean'),\n",
    "#         Start_DayofWeek=('Start_DayofWeek', 'first'),\n",
    "#         Start_Month=('Start_Month', 'first'),\n",
    "#         Start_Quarter=('Start_Quarter', 'first')\n",
    "#     )\n",
    "#     .reset_index()\n",
    "# )\n",
    "\n",
    "# # Final merge: add the event type counts\n",
    "# grouped_df = grouped_df.merge(event_type_counts, on=['StationID', 'Date'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31207ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TEXAS'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storm_final_df['STATE'].unique()\n",
    "# storm_grouped_df['Total_Property_Damage'].describe()\n",
    "# event_type_counts['Event_Drought'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Get combined daily data\n",
    "# df_hourly, df_daily = combine_agg_ts(state=\"Florida\", \n",
    "#                                      start_year=2014, start_month=1, start_day=1, \n",
    "#                                      end_year=2023, end_month=12, end_day=31)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e32ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ggggg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b25998",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose df_daily for now\n",
    "df_final = df_daily.copy()\n",
    "\n",
    "# Step 2: Add time-based features\n",
    "df_final['day_of_week'] = df_final.index.dayofweek  # 0 = Monday\n",
    "df_final['is_weekend'] = df_final['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Step 3: Add lag features\n",
    "df_final['customers_out_lag1'] = df_final['customers_out'].shift(1)\n",
    "df_final['customers_out_lag7'] = df_final['customers_out'].shift(7)\n",
    "\n",
    "# Step 4: Add rolling window features\n",
    "df_final['customers_out_roll3'] = df_final['customers_out'].rolling(window=3, min_periods=1).mean()\n",
    "df_final['customers_out_roll7'] = df_final['customers_out'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "# Step 5: Fill any remaining NaNs if needed\n",
    "df_final.fillna(0, inplace=True)\n",
    "\n",
    "# Step 6 (Optional): Scale customers_out if needed (e.g., MinMaxScaler, StandardScaler)\n",
    "# Only necessary if you plan to use linear models or neural nets\n",
    "\n",
    "# Now df_final is model-ready!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03483e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamic_rhythm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
